---
title: Retour d’expérience post-livraison - bonnes pratiques et améliorations
date: 2026-01-01 20:00:00 -0400
categories: []
tags: []
---
## Bonnes pratiques post-livraison pour les équipes de développement


### Prioriser les actions juste après une livraison majeure

Après une **livraison majeure**, il est crucial de prendre du recul avant de foncer vers de nouvelles fonctionnalités. Dans l’immédiat, l’équipe doit **célébrer le travail accompli et analyser le projet** : une réunion de retour d’expérience (post-mortem) permet d’identifier ce qui a bien fonctionné et ce qui pourrait être amélioré pour les prochains cycles. C’est l’occasion de faire le bilan calmement *avant* de passer à autre chose. Concrètement, il faut d’abord **recueillir les retours** : feedback des utilisateurs, métriques de production, rapports de bugs. Ces informations orientent la suite en **priorisant les corrections critiques et les améliorations à apporter**. Par exemple, la phase post-livraison doit se concentrer sur le support et l’amélioration continue : analyser les bugs ou problèmes détectés en production, les corriger en priorité, et utiliser ces retours pour ajuster le backlog des prochaines itérations. En parallèle, on veille à **mettre à jour la documentation utilisateur et technique**, et à adresser les demandes de support client liées à la nouvelle version.

Après une grosse sortie, certaines équipes planifient même un *sprint de stabilisation* ou de **maintenance proactive**. L’idée est de **consacrer du temps aux tâches qualitatives** souvent mises de côté pendant la course à la livraison : amélioration du code, refactoring léger, réduction de la dette technique, **renforcement des tests** et de la supervision. Il est recommandé d’organiser une **rétrospective d’équipe** dédiée à la livraison qui vient de s’achever. Cette rétro permet d’aligner tout le monde sur les enseignements du projet, de pointer ce qui a manqué ou pris du retard, et de proposer des ajustements de processus. Par exemple, l’équipe peut décider de **raffiner sa Definition of Done** si des critères de qualité ont été négligés. En Agile, la Definition of Done est une description formelle de l’état attendu d’une fonctionnalité lorsqu’elle satisfait à tous les critères de qualité requis. L’analyser en rétro aide à voir pourquoi certains items n’étaient pas vraiment *« done »* et comment éviter cela. **Améliorer la Definition of Done réduit les retouches et les défauts** en exigeant dès le départ un niveau de qualité plus élevé. En somme, une fois les urgences traitées, l’équipe doit faire le point sur ses processus internes pour **gagner en maturité** : qu’est-ce qui a freiné ou généré des bugs et comment y remédier ? Cette démarche d’amélioration continue est vitale pour aborder sereinement les développements suivants.

### Renforcer les pipelines DevOps et l’automatisation après coup

Les grands déploiements mettent souvent à l’épreuve le **pipeline CI/CD** et les pratiques DevOps de l’équipe. Juste après la livraison, c’est le moment d’identifier les points faibles et d’apporter des améliorations aux pipelines, aux scripts de build/déploiement, et aux validations automatiques. L’objectif est de **fiabiliser et accélérer** la prochaine livraison. Par exemple, si des tâches manuelles ont été nécessaires durant le déploiement, il convient de les **automatiser**. Adopter ou améliorer une chaîne CI/CD solide permet d’éliminer les opérations répétitives (compilation, packaging, déploiement manuel, etc.) en les exécutant de façon automatique – ce qui libère du temps aux développeurs pour se concentrer sur le code et l’innovation. En pratique, on pourra ajouter des étapes de validation supplémentaires dans le pipeline (analyses de code statique, tests de sécurité, tests de performance automatisés) afin de détecter les problèmes plus tôt. Raccourcir et fiabiliser les builds fait aussi partie des axes d’amélioration courants : par exemple en parallélisant certains tests ou en optimisant l’infrastructure CI. *« Shift-left »* est le maître-mot – intégrer les contrôles qualité le plus tôt possible dans le cycle de développement.

Investir dans l’outillage DevOps après une livraison majeure apporte un **retour sur investissement direct** pour la suite. Des pratiques comme l’**intégration continue** renforcée garantissent que chaque modification de code soumise est automatiquement testée et validée, détectant plus vite les bugs de régression et les conflits d’intégration. Cela améliore la qualité du code et la stabilité de la branche principale avant même qu’on n’approche la prochaine release. De même, si le déploiement en production a été difficile, c’est peut-être le signe qu’il faut améliorer le processus de **déploiement continu** : scripts d’infrastructure (Infrastructure as Code), stratégies de *feature toggles* pour déployer sans activer immédiatement, etc. Par ailleurs, une bonne pratique post-livraison est de **mesurer les indicateurs du pipeline** (durée des builds, taux de succès des tests, temps de déploiement) et de fixer des objectifs d’amélioration. L’équipe DevOps peut profiter de ce moment pour revoir la configuration des environnements de test et de préproduction, afin qu’ils collent davantage à la prod et éviter les « surprises » en déploiement. Enfin, il est recommandé de **documenter les étapes du pipeline et d’inclure cette documentation au wiki interne** pour faciliter l’onboarding des nouveaux et partager les connaissances sur la livraison continue.

### Consolider la qualité logicielle et l’observabilité

Après la frénésie d’une sortie en production, revenir aux fondamentaux de la **qualité logicielle** est une étape importante. Cela passe par un **renforcement de la batterie de tests** : on peut écrire des tests additionnels pour les zones du code qui ont présenté des bugs ou qui sont insuffisamment couvertes. Augmenter la couverture de tests (unitaires, intégration, end-to-end) fiabilisera les prochaines évolutions du logiciel. Il est aussi utile d’analyser les rapports de couverture de code afin de cibler les parties critiques non testées et d’y remédier. La phase post-release offre l’occasion de réaliser des **tests de performance** approfondis sur la version livrée, pour s’assurer qu’elle tient la charge et qu’aucune régression de performance n’a été introduite. Si des problèmes de performance en production ont été constatés, ils doivent être reproduits en environnement de test et analysés. On pourra ensuite ajouter des tests de performance automatisés au pipeline pour prévenir le retour de telles régressions à l’avenir.

L’**observabilité** du logiciel en production est un autre pilier à consolider. Après déploiement, il est crucial de mettre en place (ou améliorer) le **monitoring en continu** : métriques système, surveillance des erreurs applicatives, logs centralisés, etc. Une bonne pratique DevOps est d’intégrer étroitement tests et monitoring, de sorte qu’on puisse détecter et diagnostiquer rapidement tout écart de comportement une fois l’appli en ligne. Par exemple, ajouter des **alertes** sur les taux d’erreur ou les temps de réponse permet de réagir avant que les utilisateurs ne soient impactés. L’équipe devrait revoir ses tableaux de bord (dashboards) de suivi post-livraison : sont-ils suffisamment détaillés ? manquent-ils de certains indicateurs métiers ou techniques pertinents ? Il est également temps de vérifier les **outils de traçabilité** et d’observabilité (par exemple, l’ajout de **traces distribuées** si l’architecture est microservices, pour suivre le parcours des requêtes). L’amélioration de l’observabilité facilite les prochaines phases de support et de diagnostic en cas d’incident. En somme, consolider la qualité logicielle après une release majeure signifie **élever le niveau d’exigence** : plus de tests automatisés et plus de visibilité sur le fonctionnement interne du produit. Ces efforts proactifs réduisent les risques de défauts futurs et améliorent la **fiabilité globale** de l’application sur le long terme.

### Ajuster l’organisation d’équipe et la communication interne

Un déploiement important est aussi un **test pour l’organisation de l’équipe** – c’est le moment d’identifier les améliorations possibles dans la façon de collaborer. D’abord, il convient de **clarifier la “Definition of Done” (DoD) de l’équipe** si des divergences ont été constatées. Une DoD bien définie (incluant par exemple *« code revu et approuvé, tests passés à 100%, déployé sur un environnement de staging, documentation mise à jour »*) aligne tout le monde sur les mêmes critères de qualité. Si pendant le rush de livraison certains éléments sont passés à la trappe (tests bâclés, doc non écrite…), la rétro permet de le souligner et de mettre à jour la DoD en conséquence. Cette clarification évite le **rétravail inutile** et réduit les défauts en s’assurant que toute fonctionnalité livrée respecte un socle qualité défini à l’avance. Ensuite, le **leadership technique** doit s’exercer pour guider l’équipe dans la période post-release : c’est le moment de féliciter les efforts fournis (*« celebrate success »*), mais aussi de **diffuser les apprentissages** à l’ensemble de l’organisation. Par exemple, si un membre de l’équipe a résolu un problème épineux durant la livraison, il pourrait faire un petit retour d’expérience en réunion pour partager la leçon apprise.

Du point de vue de la **communication interne**, il est primordial de maintenir un climat de confiance et de transparence, surtout après les éventuelles tensions d’une grosse livraison. Les managers et Scrum Masters doivent encourager l’équipe à **exprimer les difficultés rencontrées** et les suggestions d’amélioration. Il peut être utile d’organiser un débriefing informel où chacun partage son ressenti sur la release (points positifs et irritants). Pour renforcer la **cohésion d’équipe**, on valorise l’entraide qui a eu lieu et on identifie où des **silos d’information** ont pu freiner le projet. Par exemple, si un développeur était le seul à maîtriser un module critique, on planifiera du *pair programming* ou une session de transfert de connaissances pour éviter ce **point de vulnérabilité** à l’avenir. La communication agile repose sur l’idée qu’**aucune question n’est bête** : chaque membre doit se sentir à l’aise de demander de l’aide ou des éclaircissements. Encourager systématiquement les questions améliore la clarté et peut mener à des découvertes ou à des améliorations de processus insoupçonnées. Enfin, repensez les rituels d’équipe si nécessaire : par exemple, instaurer un point quotidien un peu plus long pendant les phases de stabilisation, ou au contraire réduire la fréquence de certaines réunions pour laisser du temps à l’apprentissage de nouvelles technologies (beaucoup d’équipes accordent du temps de **veille technologique** après une release, pour monter en compétences sur des outils repérés mais pas eu le temps d’approfondir). En somme, la période post-livraison est propice à ajuster l’organisation du travail, que ce soit via une DoD enrichie, une meilleure communication ou un leadership servant à la fois la performance et le bien-être de l’équipe.

## Documentation technique dans un contexte multi-produit

Lorsque plusieurs **produits ou projets** cohabitent au sein de l’entreprise, la gestion de la documentation technique devient plus complexe. Il faut à la fois maintenir la **spécificité de chaque produit** et capitaliser sur des ressources communes (guides, normes, etc.). Voici quelques bonnes pratiques pour structurer et pérenniser une documentation technique multi-produits.

### Structurer un wiki efficace par produit

Il est conseillé d’**organiser le wiki interne par produit ou par domaine** afin de compartimenter l’information et éviter la confusion. Concrètement, on peut créer des *espaces* ou sections dédiés pour chaque produit (ou chaque équipe) dans l’outil de documentation. Cela évite que les pages techniques d’un produit se mélangent avec celles d’un autre, et permet à chaque équipe de retrouver facilement **son périmètre de documentation**. Comme le souligne un guide de bonnes pratiques, si votre entreprise développe **plusieurs produits ou services**, il peut être judicieux de prévoir des espaces de documentation séparés pour **chaque produit**, de façon à prévenir l’encombrement et la navigation confuse dans le wiki. En segmentant ainsi par contexte, on améliore la pertinence de la documentation visible par chaque collaborateur. Par exemple, un nouveau développeur sur le produit A consultera le wiki du produit A sans tomber sur des pages du produit B qui ne le concernent pas.

Bien entendu, ces espaces séparés doivent s’accompagner d’une **navigation transversale** soignée et d’un moteur de **recherche performant**. Il faut que le wiki reste **facile à parcourir** malgré la quantité d’informations. Un bon moteur de recherche interne est crucial pour que chacun trouve rapidement les infos nécessaires, sans devoir connaître l’emplacement exact dans l’arborescence. Pensez également à définir des **gabarits de pages** communs (par exemple un modèle pour les documents d’architecture, un modèle pour les README de service, etc.) afin d’harmoniser la présentation entre les différents produits. Chaque espace produit peut suivre une structure similaire : par exemple *« Présentation générale – Architecture – Guide de démarrage – FAQ – Runbook »*. Cette cohérence aide les lecteurs à s’y retrouver d’un produit à l’autre. Enfin, une fois l’ossature du wiki multi-produit en place, **communiquez auprès des équipes** sur comment l’utiliser (quelle info va où). Clarifiez aussi les droits d’édition : dans un wiki, tout le monde peut contribuer, mais on peut restreindre certaines sections sensibles (*ex.* pages de normes globales) à la relecture par des référents.

### Centraliser les normes de développement et guides transverses

Dans un contexte multi-produit, il est essentiel de **partager les bonnes pratiques communes** et de ne pas dupliquer l’information générique. Pour cela, on crée une **section centrale** dans le wiki (ou un espace dédié) pour regrouper tous les contenus **transverses** utiles à l’ensemble des équipes. Par exemple, on y retrouvera les **normes de développement** (guidelines de code, conventions de nommage, formatage du code, processus de revue de code), les guides d’architecture (patterns recommandés, exigences de sécurité, politiques de gestion des données), ainsi que les **procédures communes** (comment déployer en prod, comment ouvrir une demande de changement, etc.). Ce *hub documentaire* sert de **source unique de vérité** pour les sujets globaux. Ainsi, chaque équipe produit sait qu’elle doit se référer à cet endroit pour tout ce qui dépasse le cadre spécifique de son application.

Un aspect particulièrement important est la **documentation pour les nouveaux arrivants** (*onboarding*). Un nouveau développeur ou architecte qui rejoint l’organisation doit pouvoir trouver rapidement un **guide d’onboarding** qui rassemble : la présentation des produits de l’entreprise, l’architecture globale du système d’information, les outils et accès à configurer, le glossaire interne, etc. Le wiki est l’endroit idéal pour héberger ces guides d’accueil. Il peut être utile de créer un **parcours de lecture** conseillé aux nouveaux (exemple : “Guide du nouvel arrivant – Lire d’abord la section ‘Architecture globale’, puis ‘Standards de code’, puis se pencher sur le wiki du produit affecté…”). Un wiki bien structuré peut ainsi faire office de **base de connaissances** pour accélérer l’intégration. Comme l’explique un article, si le wiki est pensé pour l’onboarding, il contiendra typiquement les documents RH, le matériel de formation, et des documents de **bonnes pratiques** à destination des nouveaux venus. Mieux encore, ce référentiel commun de normes et pratiques doit être **vivant** : il faut encourager les experts de chaque domaine à le tenir à jour. Par exemple, l’expert sécurité mettra à jour la section sécurité dès qu’une nouvelle règle apparaît, le référent qualité logicielle enrichira les standards de tests au fil du temps, etc.

Pour faciliter la centralisation, certaines organisations optent pour des **outils dédiés** ou des *portails développeurs*. Mais un simple Confluence, SharePoint ou GitHub Wiki bien organisé peut suffire. L’important est de garantir que *« tout le monde se réfère aux mêmes données »* et que la **mise à jour régulière** de ce référentiel soit inscrite dans le processus. On peut par exemple instituer une *revue semestrielle* des normes : l’architecte principal et les tech leads parcourent le wiki central et actualisent ce qui doit l’être (nouvelles versions de framework, nouvelles conventions suite à une rétro, etc.). En somme, **centraliser la documentation commune** évite la dispersion de l’information et assure une **cohérence** entre les différents produits de l’écosystème.

### Maintenir la documentation à jour de manière collaborative

Un défi majeur de la documentation technique est sa **mise à jour** dans le temps, surtout dans un environnement multi-projet où les changements sont nombreux. Pour éviter que le wiki devienne obsolète, plusieurs approches et outils peuvent être mis en place. D’une part, il est recommandé de **désigner clairement des responsables ou référents** pour la documentation (*“wiki gardeners”*). Leur rôle est de “jardiner” le wiki, c’est-à-dire de **supprimer les pages dépassées**, corriger les liens morts, consolider les doublons et en général garder la base de connaissances en bon état. Cette responsabilité peut tourner entre membres de l’équipe (par exemple, chaque sprint, une personne différente relit et actualise une partie du wiki) ou être formalisée dans un rôle (certains nomment un *documentation owner* par produit). L’important est que la tâche de maintenance documentaire soit reconnue et planifiée, et non laissée au hasard. Un bon practice est d’**insérer la mise à jour de la doc dans le workflow de développement** : par exemple, inclure dans la Definition of Done qu’une fonctionnalité majeure implique la mise à jour de la page wiki correspondante. De même, lors des revues de code, les reviewers peuvent ajouter un rappel : *“As-tu mis à jour la doc ?”* si nécessaire.

Au niveau des outils, une astuce pour faciliter la collaboration est d’utiliser des plateformes de wiki offrant des **fonctions de suivi des modifications** et de commentaires. Un wiki interne doit idéalement permettre à chacun de proposer des changements (éventuellement via des “suggestions” ou *pull requests* sur la doc si on la stocke en Markdown dans un repo). Le versioning des pages et la notification des modifications aux parties prenantes aident à impliquer tout le monde dans la vie du wiki. Par exemple, si la page “Guide API Produit X” est modifiée, les développeurs du produit X reçoivent une notification (via email ou Slack) et peuvent réagir en cas d’erreur. Pour maintenir la doc **fiable**, il faut également encourager une culture où **consulter et améliorer la documentation fait partie du quotidien**. Lorsqu’un développeur cherche une info et ne la trouve pas ou la trouve périmée, il devrait se sentir responsable de la créer ou la corriger pour les suivants. Un moyen efficace est de traiter le **wiki comme du code** : utiliser un contrôle de source, faire relire les changements significatifs, etc. Certaines organisations intègrent même la doc technique dans le même repo que le code (Docs as Code), ce qui permet de versionner doc et code simultanément et de valider les Mises à jour de doc via revue de code.

Enfin, dans un contexte multi-projets, la **gouvernance de la documentation** peut être un sujet en soi. Il peut être utile de tenir un **comité documentation** (informel) réunissant des représentants de chaque équipe produit, qui se retrouvent ponctuellement pour partager les bonnes pratiques de documentation, les outils, et s’aligner sur la structure. Cela permet de conserver une certaine homogénéité et de propager les améliorations d’une équipe à l’autre. En résumé, garder une documentation à jour requiert **organisation et vigilance collective** : en désignant des “jardiniers” du wiki, en intégrant la doc aux processus de développement et en tirant profit des outils collaboratifs, on pérennise un wiki utile et vivant pour tous.

## Alternatives aux outils ayant changé de licence ou de modèle économique

Ces derniers temps, plusieurs outils populaires de l’écosystème .NET ont annoncé un **changement de licence ou de modèle commercial**. Cela a des impacts sur les projets qui les utilisent, et la communauté s’est mobilisée pour proposer des alternatives open-source ou gratuites. Nous examinons ci-dessous trois cas notables – **FluentAssertions**, **MediatR** et **Swagger/OpenAPI** – en résumant les changements et les solutions de remplacement recommandées par la communauté.

### FluentAssertions : remplacer la librairie d’assertions dans les tests

**FluentAssertions** est une librairie très répandue pour écrire des tests unitaires plus expressifs en .NET. Historiquement distribuée sous licence Apache 2.0, elle a récemment connu un **changement de licence** dans ses versions les plus récentes. En effet, à partir de la version 8 (2024), FluentAssertions est passé sur un modèle qui pourrait imposer des frais aux usages commerciaux, rompant avec la gratuité totale des versions précédentes. En pratique, cela signifie que les entreprises utilisant les dernières versions doivent soit souscrire une licence commerciale, soit rester sur une ancienne version (la 7.x restant sous Apache 2.0). Cette évolution a provoqué des discussions animées et beaucoup de développeurs .NET envisagent de **remplacer FluentAssertions par une alternative** open-source.

La communauté recommande principalement la librairie **Shouldly** pour remplir le même rôle. Shouldly offre également une syntaxe fluide pour les assertions et reste entièrement gratuite et maintenue activement. Comme l’indiquent plusieurs développeurs, *“Shouldly est très proche”* de FluentAssertions dans son usage et permet une transition relativement aisée. Un article de blog détaille d’ailleurs comment migrer son code de FluentAssertions vers Shouldly presque *1:1* en adaptant simplement la syntaxe (par ex. `result.Should().Be(x)` devient `result.ShouldBe(x)`). Les bénéfices de Shouldly incluent une syntaxe encore plus lisible (proche du langage naturel) et une communauté engagée, ce qui est un gage de pérennité. Outre Shouldly, d’autres alternatives existent ou émergent : on peut citer **AwesomeAssertions**, un fork communautaire de FluentAssertions reprenant la version 7 en la faisant évoluer sous licence libre. Certains développeurs font même le choix de revenir aux **assertions fournies nativement** par le framework de test (xUnit, NUnit ou MSTest), estimant que FluentAssertions n’apportait qu’un *sucre syntaxique* payant pouvant être évité.

En somme, face aux changements de licence de FluentAssertions, la recommandation actuelle est de se tourner vers des alternatives comme Shouldly pour conserver des tests lisibles sans contrainte commerciale. Shouldly étant sous licence permissive (MIT) et soutenu par la communauté, il constitue une solution sereine sur le long terme. Pour ceux qui souhaitent rester sur FluentAssertions malgré tout, il est envisageable de **geler la version** à 7.x (dernière version libre) – mais cela prive des nouveautés et corrections futures. La situation de FluentAssertions illustre en tout cas une tendance où certains mainteneurs de projets open-source .NET cherchent des modèles de monétisation, poussant la communauté à réagir en proposant des forks ou en mettant en lumière d’autres outils libres.

### MediatR : alternatives à la médiation CQRS suite à la commercialisation

**MediatR** est une librairie largement adoptée pour implémenter le pattern Médiateur (Mediator Pattern) et faciliter le **CQRS** (séparation Commandes/Queries) dans les applications .NET. Son auteur, Jimmy Bogard, a annoncé en 2023-2024 un changement majeur : MediatR va adopter un **modèle dual de licence commerciale** pour les usages en entreprise, tout en restant gratuit pour les projets open-source ou personnels. En clair, les futures versions exigeront l’achat d’une licence pour une utilisation commerciale (typiquement par des organisations à but lucratif), tandis que les particuliers, étudiants, projets open-source, petites startups, etc., pourront continuer à l’utiliser gratuitement. Ce mouvement, combiné à la commercialisation simultanée d’AutoMapper (autre bibliothèque du même auteur), a suscité de vives réactions dans la communauté .NET.

Beaucoup de développeurs cherchent donc des **alternatives à MediatR** pour ne pas dépendre d’une librairie qui devient payante. La bonne nouvelle est qu’il existe déjà des options matures. L’une des plus citées est **Wolverine** : un framework open-source qui peut remplacer MediatR tout en apportant des capacités avancées. Wolverine, créé par d’autres membres de la communauté, est une librairie haute performance pour la médiation, le messaging interne et le *background processing*. Entièrement gratuit, il utilise des *source generators* pour optimiser les performances et ajoute des fonctionnalités intégrées comme la gestion des *retries*, la planification différée de traitements, le support des *sagas* et d’un **outbox** pour fiabiliser l’envoi de messages. En somme, Wolverine va plus loin que MediatR en combinant un bus de messages léger et un orchestrateur de workflows, tout en restant simple à utiliser pour faire du CQRS pur. La communauté le met en avant depuis l’annonce de Jimmy Bogard : *« MediatR va exiger une licence commerciale... une alternative se distingue : Wolverine »*.

Une autre alternative notable est **Cortex.Mediator**. Apparue en 2025, cette librairie se veut un **équivalent open-source (MIT)** de MediatR, avec les mêmes concepts de base (Command, Query, Notification, Pipeline behaviors) et quelques ajouts intéressants. Par exemple, Cortex.Mediator intègre nativement la notion d’*Unit of Work* pour la gestion des transactions, ce que MediatR ne propose pas en standard. De plus, elle s’inscrit dans un écosystème plus large (Cortex) orienté données et workflows distribués, ce qui peut séduire des architectures modernes. À l’instar de Wolverine, Cortex.Mediator est gratuit pour **tout usage** (licence MIT) et évite tout verrouillage propriétaire. D’autres alternatives existent également, chacune avec son approche : par exemple **LiteBus** (projet sur GitHub/dev.to) ou la possibilité de **développer son propre médiateur interne** relativement simplement si les besoins sont basiques (quelques développeurs préfèrent coder une solution maison adaptée plutôt que d’ajouter une dépendance externe). Enfin, certains projets font le choix d’utiliser directement un **vrai bus de messages** (comme MassTransit, NServiceBus dans sa version open-source limitée, ou Azure Service Bus, etc.) si leur architecture nécessite d’ors et déjà de la communication distribuée – bien que ce soit un changement d’ampleur différente par rapport à MediatR qui est purement in-process.

En conclusion, le changement de cap de MediatR vers un modèle commercial a poussé la communauté .NET à se mobiliser pour proposer des solutions de remplacement viables et libres. **Wolverine** se démarque comme une alternative robuste, offrant tout MediatR et au-delà en restant open-source. **Cortex.Mediator** fournit également une option crédible, très proche conceptuellement de MediatR, sans coût et avec quelques améliorations intégrées. Le choix dépendra des besoins de chaque projet (performance, fonctionnalités étendues ou au contraire simplicité). L’important est que les développeurs .NET ont désormais des chemins tout tracés pour **éviter la dépendance à une librairie commerciale** si cela ne correspond pas à leurs valeurs ou à leur budget.

### Swagger / OpenAPI : outils de documentation d’API dans l’écosystème .NET

Le terme **Swagger** fait généralement référence à l’écosystème d’outils autour de la spécification OpenAPI (ex-Swagger Specification) pour documenter les APIs REST. Dans le monde .NET, *Swagger* est souvent synonyme de la librairie **Swashbuckle.AspNetCore** qui génère une UI de documentation interactive pour les contrôleurs Web API. À strictement parler, Swagger (au sens de Swagger UI, Swagger Codegen, etc.) n’a pas subi un changement de licence comparable aux deux précédents – les outils Swagger UI restent open-source sous licence Apache 2.0. Cependant, on a pu observer des **évolutions dans l’écosystème** : maintien variable des librairies, apparition d’alternatives, et services commerciaux autour de Swagger (ex: SwaggerHub de SmartBear). La question de trouver des **alternatives** se pose donc surtout en termes de choix technique ou de préférences, plus que pour des raisons de licence.

Les deux solutions historiques pour la documentation OpenAPI dans .NET sont **Swashbuckle** et **NSwag**. Ce sont des packages NuGet qui, branchés sur une application ASP.NET Core, permettent de **générer automatiquement un document OpenAPI (JSON/YAML)** décrivant les endpoints, puis de servir une **interface utilisateur** pour consulter et tester ces endpoints. Pendant des années, Swashbuckle et NSwag ont dominé ce domaine et rendu de fiers services à la communauté .NET. Swashbuckle est souvent perçu comme le choix par défaut (très simple à configurer, il ajoute l’UI Swagger au lancement de l’appli). NSwag, quant à lui, offre à la fois une UI similaire et des outils complémentaires (par ex. génération de clients TypeScript à partir de l’OpenAPI). Jusqu’à récemment, le choix entre les deux était surtout affaire de préférence ou de cas d’usage. Aujourd’hui, on note quelques changements : le **rythme de maintenance** de ces projets a ralenti, causant un léger retard dans le support des dernières versions .NET. Swashbuckle a heureusement connu un regain en 2024 avec de **nouveaux mainteneurs** (dont un employé Microsoft) et une mise à jour pour .NET 8. Néanmoins, Microsoft a également introduit **sa propre solution** intégrée : à partir de .NET 7/8 et surtout .NET 9, le framework propose un package `Microsoft.AspNetCore.OpenApi` pour générer l’OpenAPI des **Minimal APIs** sans dépendre d’une librairie tierce. Cette décision fait suite au constat que les outils existants peinent à suivre le rythme de sortie annuel de .NET, et vise à fournir un support *first-party* plus réactif.

Pour les développeurs et architectes, quelles sont les **alternatives Swagger** intéressantes à considérer ? Tout d’abord, il convient de distinguer deux aspects : la génération du **document OpenAPI** (le fichier JSON/YAML décrivant l’API), et la présentation via une **UI** web de cette documentation. Sur le premier aspect (génération), outre Swashbuckle et NSwag, on peut directement exploiter les **générateurs fournis par .NET** (surtout pour les Minimal APIs comme évoqué). Certaines entreprises qui ont des exigences spécifiques utilisent aussi des outils *maison* ou d’autres générateurs OpenAPI disponibles sur le marché, mais Swashbuckle/NSwag couvrent la majorité des besoins standard. Sur le second aspect (UI de doc), il existe des alternatives à Swagger UI lui-même. Par exemple, **ReDoc** est une UI open-source offrant une expérience de lecture différente : plus orientée *documentation statique* et structurée, sans la fonctionnalité “Try it out” interactive de Swagger UI. ReDoc met davantage l’accent sur la clarté de la doc, ce qui peut convenir pour exposer une API en lecture seule (par ex. portail public d’une API). On peut configurer NSwag ou Swashbuckle pour servir ReDoc à la place de Swagger UI si on le souhaite.

En termes de **licence** et modèle économique, **NSwag et Swashbuckle** sont tous deux open-source (licence BSD pour Swashbuckle, MIT pour NSwag) et gratuits. Leurs alternatives mentionnées (ReDoc UI par exemple) sont aussi open-source (ReDoc a une version OSS et une version Pro payante avec plus de fonctionnalités). Ainsi, contrairement aux cas de FluentAssertions et MediatR, il n’y a pas eu de passage soudain à un modèle payant pour ces outils – la question est plutôt de savoir quelle solution est la plus pérenne et adaptée à votre contexte. La communauté .NET semble se diriger vers un mix : **continuer à utiliser Swashbuckle** si celui-ci répond aux besoins (étant donné qu’il est de nouveau maintenu activement, y compris par des membres de Microsoft, gage de stabilité), ou **adopter NSwag** si ses fonctionnalités supplémentaires sont un plus (génération de clients, etc.). Pour les projets à long terme, garder un œil sur l’évolution de l’outil **Microsoft.AspNetCore.OpenApi** est prudent – il pourrait devenir le standard de fait dans le futur, en particulier pour les APIs modernes sans contrôleurs.

En résumé, **Swagger** reste un incontournable pour la documentation des APIs .NET, mais vous avez le choix entre plusieurs implémentations. Swashbuckle et NSwag demeurent les piliers actuels pour générer et exposer l’OpenAPI de vos applications. Des **alternatives d’interface** comme ReDoc peuvent améliorer l’expérience de lecture selon vos besoins (documentation orientée développeur interne vs. exposition publique, etc.). Et avec l’arrivée des outils intégrés à .NET lui-même, on peut s’attendre à une plus grande facilité pour documenter nos APIs à l’avenir – sans pour autant verrouiller qui que ce soit dans un modèle propriétaire, puisque tout l’écosystème OpenAPI reste ouvert et soutenu par de multiples acteurs. Chaque équipe pourra donc choisir l’outil Swagger/OpenAPI qui lui convient le mieux, en étant assurée qu’aucun changement de licence imprévu ne viendra remettre en cause son usage de ces outils de documentation.

**Sources :** Les informations et recommandations ci-dessus s’appuient sur des sources officielles et des retours d’expérience de la communauté : guides Atlassian et Asana sur les rétrospectives, articles techniques (Zeet, Medium) concernant l’amélioration continue post-release, bonnes pratiques DevOps et qualité logicielle (intégration continue, monitoring), conseils de structuration de documentation multi-projet (LinkedIn, wp-glogin), ainsi que des discussions et annonces récentes dans l’écosystème .NET sur les changements de licence et alternatives pour FluentAssertions, MediatR et Swagger. Ce retour d’expérience synthétise ces apports pour fournir une base solide à tout développeur ou architecte confronté à ces enjeux en 2025.
