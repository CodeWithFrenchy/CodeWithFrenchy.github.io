[ { "title": "Concevoir et maintenir des applications microservices performantes en .NET", "url": "/posts/concevoir-maintenir-application-performante-dotnet/", "categories": "architecture", "tags": "dotnet", "date": "2025-10-20 19:00:00 -0400", "snippet": "La performance logicielle ne doit pas être une réflexion après coup : elle se conçoit dès le départ et se cultive tout au long de la vie du système. Dans cet article, nous explorons comment s’outil...", "content": "La performance logicielle ne doit pas être une réflexion après coup : elle se conçoit dès le départ et se cultive tout au long de la vie du système. Dans cet article, nous explorons comment s’outiller pour identifier les problèmes de performance et les bonnes pratiques pour bâtir une application scalable, optimisée et résiliente. Nous aborderons la conception axée sur la scalabilité, l’amélioration continue des performances et l’automatisation pour la résilience, puis détaillerons les étapes clés à chaque phase (conception, développement, déploiement) afin d’assurer des applications microservices .NET hautement performantes.Concevoir pour la scalabilité dès le départDès les premières phases de conception, il est nécessaire d’intégrer la performance et la scalabilité dans l’architecture du logiciel : Utiliser des métriques et de l’observabilité dès le début : Prévoir d’emblée l’instrumentation de l’application (logs, métriques, tracing distribué) facilite le suivi des performances. Mettre en place des métriques clés (temps de réponse, taux d’erreur, utilisation CPU/mémoire, etc.) et centraliser les logs sont des pratiques indispensables pour diagnostiquer les problèmes plus tard. Par exemple, des outils comme Prometheus/Grafana ou Azure Monitor peuvent capturer et visualiser ces métriques en temps réel, et des sondages montrent que 73 % des experts IT estiment que le monitoring temps-réel a amélioré leur capacité à résoudre les problèmes de performance. Journaliser les requêtes SQL et les performances des bases de données : En environnement .NET, pensez à activer le logging des requêtes SQL (par exemple via Entity Framework Core en mode Information). Cela permet de voir chaque commande SQL exécutée et sa durée d’exécution. Si une requête prend plus de temps que prévu, vous avez identifié un coupable potentiel et pouvez enquêter sur sa cause (index manquant, requête N+1, etc.). ⚠️ Attention à ne pas laisser ce logging activé en production de façon permanente, car il peut ralentir l’application et générer d’énormes fichiers de log. Utilisez-le ponctuellement pour collecter des données de performance, ou exploitez des outils APM (Application Performance Management) qui capturent ces informations plus finement (par exemple, Azure Application Insights intègre les temps d’exécution des requêtes SQL dans son analyse). Choisir la bonne architecture (et la bonne granularité de services) : Une architecture bien pensée est la base de la performance. Par exemple, une approche microservices permet une scalabilité indépendante de chaque composant : chaque service peut monter en charge séparément selon les besoins, sans devoir dimensionner toute l’infrastructure globalement. Cela offre aussi plus de résilience (si un service tombe, le reste du système continue de fonctionner). Veillez toutefois à définir des frontières de service claires (idéalement alignées sur des contextes métiers via DDD) et à éviter un morcellement excessif qui introduirait une complexité inutile. Chaque microservice doit être faiblement couplé et autonome, communiquant avec les autres via des API légères ou des messages. En effet, des services découplés (échanges HTTP REST, messages asynchrones via une file, etc.) peuvent évoluer ou être modifiés sans impacter les autres, ce qui améliore la flexibilité et la facilité de mise à l’échelle. Éviter les dépendances synchrones entre composants : Les appels synchrones bloquants entre microservices créent un couplage fort et peuvent provoquer des cascades de latence. Il est souvent recommandé d’adopter une communication asynchrone via des événements ou des files de messages. Par exemple, plutôt que d’attendre la réponse d’un autre service en temps réel, émettez un événement que l’autre service traitera à son rythme. L’Event-Driven Architecture réduit les dépendances directes et améliore la scalabilité globale du système. En pratique, cela signifie utiliser des solutions comme RabbitMQ, Azure Service Bus ou Kafka pour propager des événements, avec des mécanismes de réessais et de tolérance (circuit breakers, timeouts) pour gérer les défaillances. Concevoir dès le départ les services pour qu’ils puissent échouer sans tout faire tomber (principe de design for failure) est clé pour la robustesse. Minimiser le couplage et favoriser la cohésion : Dans la même veine, structurez vos composants pour qu’ils soient le plus autonomes possible (chaque microservice gère sa propre base de données, évitant les dépendances entre bases de données). Un faible couplage se traduit aussi par l’utilisation de contrats d’API bien définis et stables, idéalement avec des modèles de communication idempotents et stateless qui facilitent la montée en charge horizontale. Concrètement, assurez-vous qu’aucun module n’ait de connaissance interne sur un autre module en dehors des APIs publiées. Par exemple, un service de commandes ne devrait pas appeler directement la base de données du service Clients ; il utilisera l’API du service Clients si besoin. Ce découpage modulaire permet de modifier ou déployer un service sans impacter le reste, et de faire évoluer l’architecture plus sereinement. Minimiser le couplage et favoriser la cohésion : Dans la même veine, structurez vos composants pour qu’ils soient le plus autonomes possible (chaque microservice gère sa propre base de données, évitant les dépendances entre bases de données). Un faible couplage se traduit aussi par l’utilisation de contrats d’API bien définis et stables, idéalement avec des modèles de communication idempotents et stateless qui facilitent la montée en charge horizontale. Concrètement, assurez-vous qu’aucun module n’ait de connaissance interne sur un autre module en dehors des APIs publiées. Par exemple, un service de commandes ne devrait pas appeler directement la base de données du service Clients ; il utilisera l’API du service Clients si besoin. Ce découpage modulaire permet de modifier ou déployer un service sans impacter le reste, et de faire évoluer l’architecture plus sereinement.En résumé, intégrer la télémétrie, penser architecture scalable (éventuellement microservices ou modules bien découplés) et éliminer les interactions bloquantes superflues dès la conception pose les fondations d’une application performante.S’améliorer en continu pendant la vie du systèmeGarantir la performance n’est pas un effort ponctuel, mais un processus continu tout au long du cycle de vie du logiciel.Voici quelques principes pour instaurer une culture d’amélioration continue des performances : Mesurer systématiquement et surveiller en production : On ne peut améliorer que ce qu’on mesure. Mettez en place un monitoring continu de l’application en production pour détecter les problèmes avant les utilisateurs. Des solutions d’APM comme Azure Application Insights (ou New Relic, Dynatrace, etc.) détectent automatiquement les anomalies de performance sur vos applications web et peuvent alerter l’équipe en cas de dégradation (par exemple, augmentation anormale du taux d’erreurs ou des temps de réponse). Configurez des alertes proactives sur vos métriques clés (latence, taux d’échec, utilisation mémoire…) afin d’être notifié dès qu’un seuil critique est franchi. Cette surveillance proactive vous aide à corriger les dérives avant qu’elles ne se transforment en panne ou en incident majeur. N’hésitez pas à utiliser des tableaux de bord visibles de tous pour suivre l’évolution des performances et des ressources en temps réel. Identifier et lever régulièrement les goulots d’étranglement : Les bottlenecks peuvent survenir à différents niveaux (base de données saturée, appels externes lents, thread CPU bloqué, etc.). Grâce aux métriques et journaux récoltés, analysez régulièrement où se situent les points chauds. Par exemple, des temps de réponse très élevés sur une API donnée peuvent révéler une requête SQL non optimisée ou un appel à un service tiers trop lent. L’objectif est d’agir en amont : idéalement, effectuez des tests de charge ou des profils de performance en continu (ou à chaque release) pour identifier les problèmes de performance potentiels avant qu’ils n’affectent les utilisateurs. Adapter vos tests automatisés pour inclure des tests de performance (même basiques) peut aider à détecter des régressions précoces. En phase de développement, n’hésitez pas à utiliser un profiler ou analyser les traces de vos API pour trouver les sections de code les plus lentes. Une fois les goulots repérés, traitez-les : par exemple, ajouter un index manquant sur la base de données, mettre en cache une donnée souvent lue, optimiser un algorithme inefficace, etc. Cette démarche proactive assure que l’application garde un niveau de performance acceptable au fil des ajouts de fonctionnalités. Éviter l’accumulation de dette technique : La dette technique non résorbée finit par ralentir l’application et compliquer son évolution. Un code mal conçu ou obsolète peut entraîner des exécutions inefficaces et des bugs, impactant directement la performance (par exemple, des algorithmes inadaptés qui dégradent le temps de réponse). Il est donc vital d’allouer du temps régulièrement pour refactorer les portions de code critiques, améliorer la lisibilité et réduire la complexité. Par exemple, si une partie du code est responsable de nombreuses requêtes redondantes ou de calculs répétés, la refonte de ce module peut éliminer ces inefficacités. Intégrez la résolution de dette technique dans votre processus Agile (incluez des tâches de refactoring dans le carnet de produit, fixez-vous un budget de temps par sprint pour la dette). Veillez aussi à prévenir la dette : suivez les bonnes pratiques de codage, revoyez le code (code reviews) pour détecter les antipatterns de performance, et écrivez des tests de performance pour valider que les nouvelles modifications ne régressent pas. Une dette technique maîtrisée se traduit par une application plus maintenable et performante sur le long terme. Mettre en place une culture de performance : Finalement, la performance doit devenir l’affaire de toute l’équipe. Inscrivez des objectifs de performance (SLO/SLI) clairs, par exemple, “95 % des requêtes sous 200 ms”, “gérer 1000 requêtes/sec sans dégrader l’expérience” et suivez ces indicateurs à chaque version. Si possible, automatisez des tests de non-régression de performance dans votre pipeline CI/CD (par exemple via un outil comme k6 ou JMeter en mode headless). Encouragez le partage des connaissances autour des optimisations réalisées et des incidents évités. Une telle culture implique aussi de ne pas attendre la veille de la mise en production pour se soucier des performances : idéalement, on teste et on optimise en continu. Comme le résume bien un guide, “Performance shouldn’t be an afterthought”, ne considérez pas la performance comme un sujet “non-fonctionnel annexe”, mais comme un critère de qualité aussi important que les fonctionnalités. En sensibilisant développeurs, QA et Ops, on crée un cercle vertueux où chacun est attentif aux impacts performance de ses choix et où l’on réagit vite en cas de problème.En améliorant en continu, en mesurant et en payant régulièrement la dette, vous maintiendrez votre système en forme et éviterez les « effets de pourrissement » qui mènent aux applications lentes et instables avec le temps.Automatiser pour assurer la résilience et la performanceAu-delà des efforts humains, l’automatisation est une alliée précieuse pour garantir la performance et la stabilité du système face à la montée en charge ou aux imprévus : Autoscaling (mise à l’échelle automatique) : Tirez parti des capacités du cloud pour ajuster dynamiquement les ressources en fonction de la charge. L’autoscaling horizontal (ajout/retrait d’instances) permet de maintenir les performances lorsque le trafic augmente, puis de réduire les ressources pour économiser les coûts quand la charge diminue. Par exemple, sur Azure App Service ou Kubernetes, vous pouvez définir des règles du type “si l’utilisation CPU dépasse 70 % sur 5 minutes, ajouter une instance”. De même, fixez une règle de scale-in pour réduire le nombre d’instances quand la charge retombe, afin d’éviter de surprovisionner. 💡 Important : ajustez et affinez ces règles selon les métriques pertinentes (CPU, mémoire, longueur de file de messages, etc.) et surveillez le comportement (pour éviter des effets de bascule trop fréquents, définissez des seuils avec hystérésis et un délai minimal entre deux scale actions). Un autoscaling bien configuré réduit le besoin d’intervention manuelle et assure que votre application reste réactive en tout temps, y compris lors de pics soudains de trafic. N’oubliez pas de prévoir une capacité maximale suffisante et un nombre minimum d’instances par défaut pour absorber le trafic de base même si les métriques ne sont pas disponibles (sécurité en cas de panne du monitoring). Tests de performance automatisés : Intégrez des tests de charge réguliers dans votre cycle de développement ou vos pipelines de déploiement. Des outils open-source comme Apache JMeter (très populaire et riche en fonctionnalités) ou k6 (plus récent, orienté développeur, avec des scripts en JavaScript) sont parfaits pour ça. JMeter, par exemple, est conçu spécifiquement pour générer du trafic et mesurer les temps de réponse de vos applications web, API, bases de données, etc.. Il permet de simuler un grand nombre d’utilisateurs et divers protocoles (HTTP, JDBC, etc.) pour voir comment votre système se comporte sous stress. De son côté, k6 s’est imposé comme un outil moderne et puissant : “k6 est un outil de test de charge open-source qui permet de créer des tests en JavaScript, un langage familier pour beaucoup”. Son moteur en Go lui confère de hautes performances pour simuler des milliers d’utilisateurs avec une empreinte légère. Vous pouvez l’exécuter en local, en distribué ou via son service cloud, et l’intégrer à vos CI/CD pour des tests en continu. Quel que soit l’outil, l’idée est de soumettre régulièrement votre application à des scénarios de charge (pic d’utilisateurs, tests d’endurance sur plusieurs heures, tests de spike, etc.) afin de vérifier sa tenue en conditions extrêmes. Ces tests révéleront peut-être des points faibles (saturation CPU, fuite mémoire, seuil à partir duquel les temps explosent) que vous pourrez corriger avant qu’un trafic réel ne provoque un incident. Surveillance proactive et auto-remédiation : En plus du monitoring passif, pensez à mettre en place des mécanismes de supervision proactive. Par exemple, des sondes de synthetic monitoring peuvent effectuer régulièrement des appels simulés à vos API ou pages principales et vérifier qu’elles restent performantes, ce qui permet de détecter une dégradation avant même un utilisateur réel. Azure Application Insights propose des “Availability Tests” de ce genre. Par ailleurs, configurez votre système pour qu’il puisse réagir automatiquement à certains événements : par exemple, un redémarrage automatique d’une instance en cas de fuite mémoire détectée, ou le déclenchement d’une scale-up temporaire si une latence anormale est mesurée sur un composant critique. L’utilisation combinée de métriques, d’alertes et de scripts d’auto-remédiation augmente la résilience globale. Certaines plateformes cloud offrent des actions automatiques basées sur des alertes (webhook déclenché sur alerte, fonctions Azure Functions ou AWS Lambda lancées pour gérer l’incident, etc.). Enfin, envisagez des approches plus avancées comme le chaos engineering en environnement de staging, pour s’assurer que votre système réagit bien aux pannes (par exemple, couper un service au hasard et vérifier que le système reste stable via des mécanismes de circuit breaker).En automatisant la montée en charge et la surveillance, vous obtenez un système auto-adaptatif : capable de croître pour servir la demande, de prévenir les problèmes avant qu’ils n’affectent les clients, et de maintenir une performance constante sans intervention humaine continue. Cela complète les efforts manuels d’optimisation en apportant une filet de sécurité opérationnel.Étapes clés pour une application performanteSynthétisons ces bonnes pratiques sous forme d’un guide étape-par-étape couvrant le cycle de vie du projet, en prenant l’exemple d’une application distribuée en microservices .NET :1. Planification et conception Profilage des besoins et objectifs : Dès le lancement du projet, définissez les exigences de performance et de scalabilité. Quel volume d’utilisateurs ou de requêtes visez-vous (charge prévue à court et moyen terme) ? Quels sont les SLA/SLO attendus (temps de réponse max, throughput minimal) ? Cette analyse initiale aide à dimensionner l’architecture. Profitez-en pour estimer les coûts associés à certaines charges (exemple : coût de l’infrastructure pour 1000 utilisateurs simultanés) afin d’orienter les choix techniques en fonction du budget. Architecture adaptée aux performances : Concevez l’architecture en fonction de ces exigences. Par exemple, pour un très fort trafic en lecture, peut-être opter pour une base de données NoSQL distribuée ou mettre en place une cache distribuée (Redis) devant la base de données SQL. Pour un besoin de haute disponibilité, prévoyez le déploiement sur plusieurs instances et zones géographiques. Si votre domaine s’y prête, choisissez une architecture microservices pour isoler les contextes et permettre une scalabilité horizontale service par service. Veillez aussi à la conception de la base de données (normalisation vs dénormalisation, sharding possible, choix entre SQL/NoSQL selon les besoins). Identifiez dès la conception les bottlenecks potentiels : par exemple, un service central par lequel passent toutes les requêtes, assurez-vous qu’il puisse monter en charge (le cas échéant, introduisez de la mise en cache ou un mécanisme de répartition de charge). Si une fonctionnalité risque d’être très consommatrice (par exemple, la génération de rapports lourds), pensez à l’isoler dans un service ou un processus asynchrone. En résumé, anticipez les points de contention possibles et cherchez à les mitiger dans le design (via du parallélisme, une distribution de la charge, etc.). Bonnes pratiques de conception : Appliquez les principes de base d’une architecture performante : faible couplage, haute cohésion, stateless autant que possible, idempotence des traitements, etc. Par exemple, un service stateless (sans état en mémoire entre les requêtes) peut être cloné à l’infini derrière un load balancer, ce qui est idéal pour l’autoscaling. Adoptez aussi dès le départ les patterns qui améliorent la performance et la résilience : circuit breaker et retry pour les appels externes (afin d’éviter d’attendre indéfiniment un service en panne), bulkheads (pour compartimenter les ressources et éviter l’effet domino), mise en file des tâches non-urgentes, utilisation d’un CDN pour les contenus statiques, etc. Ne négligez pas la phase de revue d’architecture, faites éventuellement des “threat modeling” de performance, c’est-à-dire demander “que se passe-t-il si X utilisateur font telle action en même temps ?” et voir si l’architecture tient la route ou si un composant deviendrait le goulot.2. Développement et optimisation Coder avec l’efficacité en tête : Au niveau du code, suivez les bonnes pratiques de performance .NET. Évitez les allocations mémoire inutiles, en particulier dans les boucles ou les méthodes appelées fréquemment. Par exemple, privilégiez l’utilisation de types comme Span&lt;T&gt; ou Memory&lt;T&gt; pour manipuler des segments de données sans copie. Ces types permettent de réduire drastiquement les allocations et le garbage collection, ce qui améliore les temps d’exécution. Pour illustrer : au lieu de faire un substring qui alloue une nouvelle string, on peut utiliser un ReadOnlySpan&lt;char&gt; pointant vers la portion de la chaîne d’origine, puis parser directement ce span. Le gain est notable : plus aucune allocation, et un temps d’exécution réduit (~30% plus rapide dans cet exemple simple). Voici un petit comparatif en C# : string s = \"Le résultat est 1532.\";// Approche classique – alloue une nouvelle string pour \"1532\"string nombreStr = s.Substring(15, 4);int value1 = int.Parse(nombreStr); // 'value1' vaut 1532// Approche optimisée avec Span&lt;T&gt; – aucune nouvelle allocationReadOnlySpan&lt;char&gt; span = s.AsSpan(15, 4);int value2 = int.Parse(span); // 'value2' vaut aussi 1532, sans copie Dans ce cas, Substring créait un nouvel objet string alors que l’utilisation de AsSpan évite cette allocation. À grande échelle (par exemple, traitement de nombreuses lignes de texte), ces optimisations réduisent la pression mémoire et accélèrent le programme. De même, soyez attentifs à vos allocations d’objets en boucle : utiliser des structures (struct) quand c’est pertinent, réutiliser des objets via des pools (exemple : ArrayPool&lt;T&gt;), ou encore utiliser des algorithmes in-place peuvent aider. Prioriser l’asynchronisme et éviter le code bloquant : .NET offre un modèle asynchrone puissant avec async/await et le Task-based programming. Exploitez cela pour toute opération d’entrée-sortie (accès BD, appels HTTP, lecture de fichier…) de sorte à ne pas bloquer les threads inutillement. Un thread bloqué en attente d’I/O est un thread qui ne sert à rien pendant ce temps, limitant la scalabilité (surtout sur un serveur web où le nombre de threads est limité). Donc, “avoid blocking on async code with .Result or .Wait(), instead use fully async calls”. En pratique, évitez des choses comme : // Mauvaise pratique – bloque le thread en attendant le résultat var data = SomeLongOperationAsync().Result; // STOP, potentiellement bloquant Préférez systématiquement la propagation de l’asynchronisme : // Bonne pratique – l’appel est asynchrone de bout en bout var data = await SomeLongOperationAsync(); // Non-bloquant, libère le thread en attente Ne mélangez pas code synchrone et asynchrone sans raison, cela peut mener à des deadlocks subtils (en particulier dans les applications ASP.NET ou GUI qui ont un contexte de synchronisation). Évitez également les verrous globaux ou les sections critiques longue durée qui empêcheront l’exploitation du parallélisme. Si vous devez limiter un accès concurrent (par exemple, pour une ressource partagée), utilisez des mécanismes non bloquants quand possible (exemple : SemaphoreSlim async au lieu d’un lock classique, collections thread-safe, etc.). L’asynchronisme bien utilisé permet au runtime d’optimiser l’utilisation des threads, et donc de traiter plus de requêtes simultanées avec la même infrastructure. Optimiser les accès aux données : Dans une application de gestion, l’accès à la base de données est souvent le facteur limitant. Il faut donc porter une attention particulière aux requêtes SQL générées ou écrites. Évitez le N+1 query problem (quand une boucle engendre une requête par itération) en utilisant les jointures ou Include nécessaires pour tout récupérer en une fois. Indexez correctement vos tables selon les requêtes réelles en production (analyses de plans d’exécution à l’appui). Si vous utilisez un ORM comme Entity Framework, traquez les requêtes non souhaitées et évaluez le coût du suivi de changements (le mode tracking par défaut a un coût mémoire, envisagez le mode AsNoTracking pour les requêtes purement lecture). Pour les lectures intensives, envisagez une cache applicative afin de ne pas solliciter la BD inutilement. Enfin, surveillez les appels réseau ou externes : regroupez-les si possible (appel d’API en lot plutôt qu’un par élément) et utilisez le caching des réponses externes quand c’est pertinent. Mesurer et profiler le code critique : Introduisez dès le développement des tests de performance sur les méthodes sensibles. Par exemple, si vous avez un algorithme de calcul intensif, créez un micro-benchmark pour comparer différentes implémentations. La bibliothèque BenchmarkDotNet est idéale pour cela : elle permet de transformer facilement des méthodes en benchmarks et de mesurer précisément leur temps d’exécution, allocations mémoire, etc. Cet outil gère le warming, les itérations multiples et fournit un rapport complet. Selon CODE Magazine, “benchmarking code is critical for knowing the performance metrics of your methods… ça aide à identifier les bottlenecks et à savoir quelles parties du code optimiser”. N’hésitez pas à écrire un petit projet console de benchmarks pour vos fonctions critiques (par exemple, comparer deux méthodes de parsing, ou deux approches de tri, etc.). De plus, utilisez les profilers lors du débogage (Visual Studio Diagnostic Tools, dotTrace, PerfView…) pour voir où le temps est passé et où la mémoire est allouée lors d’un scénario complet. Ces informations guideront vos optimisations de manière objective. Rappelez-vous : il est facile de se tromper sur l’origine d’un ralentissement, seules les mesures peuvent vous le confirmer. Tests unitaires et de charge en local : Durant le développement, outre les tests unitaires fonctionnels, pensez à effectuer de petits tests de charge localement sur vos endpoints (avec un outil comme K6) pour avoir un aperçu de comment se comporte votre API avec, par exemple, 100 requêtes concurrentes. Cela peut révéler tôt des soucis (contenention, exceptions, etc.). Assurez-vous également d’avoir des environnements de staging sur lesquels vous pouvez simuler des charges plus réalistes avant la mise en production.3. Déploiement et suivi en production Activer le monitoring en production : Une fois l’application déployée, branchez-la sur des outils de monitoring. Sur Azure, activez Application Insights pour votre application .NET, c’est un APM qui va collecter les logs, métriques et traces automatiquement (requêtes HTTP, dépendances externes, requêtes SQL, exceptions…). Application Insights peut même “analyser automatiquement les performances de votre application et vous alerter en cas de problèmes potentiels”. Il détecte par exemple une hausse anormale du taux d’erreurs ou une dégradation de la durée de certaines requêtes, et génère des alertes (Smart Detection). Configurez également Azure Monitor pour vos ressources (par exemple, surveiller la métrique de DTU ou d’utilisation CPU de votre base de données Azure SQL, la saturation de vos instances App Service, etc.). Pensez aux logs distribués, dans une architecture microservices, centralisez les logs de chaque service dans un outil (Elastic Stack/ELK, Azure Log Analytics, Seq…) et corrélez-les avec du tracing distribué (propagation d’un ID de corrélation pour suivre une requête de bout en bout à travers les services, via des outils comme Jaeger ou Zipkin). Ce niveau d’observabilité vous permettra de diagnostiquer rapidement en production les éventuels problèmes de performance (exemple : identifier qu’un ralentissement global vient en fait du service X spécifique, ou même d’une étape précise dans un workflow). Tests de charge réguliers en environnement de pré-production : Ne faites pas l’impasse sur des tests de charge avant chaque version majeure. Idéalement, reproduisez un environnement aussi proche que possible de la production (en termes de configuration, de volume de données, etc.) et exécutez-y des scénarios de charge avec vos outils (k6, JMeter…). Ceci pour valider que la nouvelle version supporte toujours la charge prévue et qu’aucune régression de performance n’a été introduite. Vous pouvez même automatiser un test de performance rapide après le déploiement (par exemple, un test qui envoie pendant 5 minutes du trafic à X req/s et vérifie que les temps de réponse restent conformes). 💡 Astuce : conservez des baseline (métriques de référence) des tests de charge des versions précédentes, de sorte à pouvoir comparer l’évolution. Si vous constatez une dégradation, mieux vaut la comprendre avant de mettre en prod que subir un incident. Les tests de charge réguliers garantissent aussi que votre infrastructure d’autoscaling est bien calibrée ! Par exemple, vérifier qu’à 80% de CPU vos instances se dupliquent correctement et absorbent le pic. Adapter les règles d’autoscaling aux habitudes réelles : Après quelques temps en production, utilisez les données collectées pour affiner vos paramètres. Peut-être que vous aviez prévu un autoscaling sur CPU à 70%, mais vous réalisez que la mémoire est le facteur limitant sur vos services : il faudrait alors ajouter une règle sur la mémoire (exemple : scale-out si &gt;85% mémoire utilisée) pour ne pas saturer les instances. Inversement, si vous voyez que l’application scale trop fréquemment (phénomène de flapping), envisagez d’augmenter un peu les seuils ou d’ajouter du délai pour éviter les oscillations inutiles. Revoyez aussi la capacité maximale : si régulièrement vous touchez le plafond d’instances en heure de pointe, réfléchissez à l’augmenter ou à opter pour des instances plus puissantes. L’objectif est d’ajuster en continu vos ressources en fonction des tendances d’utilisation, afin d’assurer la performance tout en optimisant les coûts. Supervision et réponses en temps réel : En exploitation, mettez en place des routines de revue des indicateurs (par exemple, un petit stand-up hebdomadaire dédié performance/résilience où l’on passe en revue les alertes de la semaine, les métriques hors normes, etc.). Investiguez toute alerte ou anomalie de performance dès que possible, même si aucun utilisateur ne s’en est plaint (exemple : si un pic de latence a eu lieu la nuit, chercher la cause : opération batch, sauvegarde, garbage collection majeur, etc.). Avoir une approche SRE (Site Reliability Engineering) peut aider : définir un budget d’erreurs (erreur budget) et se fixer des objectifs de disponibilité/performance. En cas d’incident (panne ou forte dégradation), procédez à une analyse post-mortem pour en tirer des leçons et éviter la répétition. Par exemple, si un service a crashé faute de mémoire, vous pourriez implémenter un recycle automatique de ce service avant qu’il n’atteigne la limite, ou améliorer son code pour consommer moins. Enfin, continuez de tester en production de manière contrôlée : par exemple les chaos tests (débrancher un service pour vérifier que le failover fonctionne) ou des canary releases pour mesurer l’impact perf d’une nouvelle version sur un sous-ensemble du trafic avant déploiement global.En suivant ces étapes de manière disciplinée, vous créez un cycle vertueux : planification soignée, développement optimisé, surveillance active, et boucle de rétroaction pour continuellement améliorer la performance. Chaque phase alimente la suivante – les enseignements de la production guident la prochaine planification, etc. Ainsi, votre application pourra évoluer en fonctionnalités tout en restant rapide, scalable et fiable.ConclusionLa performance applicative est un effort transversal qui commence à l’architecture initiale et se poursuit tout au long du cycle de vie du logiciel. En concevant dès le départ pour la scalabilité, vous évitez de sérieux écueils plus tard. En instaurant une amélioration continue (mesure, optimisation, réduction de la dette technique), vous prévenez la dégradation progressive qu’on observe souvent dans les systèmes qui vieillissent. Et en automatisant la résilience via l’autoscaling, le monitoring proactif et les tests réguliers, vous vous assurez que l’application peut encaisser la charge et rester stable face aux imprévus.Une application .NET bien pensée, utilisant par exemple une architecture microservices découplée, des patterns asynchrones, des optimisations comme Span&lt;T&gt;, et outillée de métriques et d’APM, peut atteindre des niveaux de performance élevés de manière pérenne. La clé est de considérer la performance comme un critère de qualité à part entière, à chaque décision technique. Ainsi, vous livrerez non seulement des fonctionnalités, mais aussi une expérience fluide et réactive aux utilisateurs, et vous pourrez dormir sur vos deux oreilles lors des pics de charge 😄.En appliquant ces conseils et en restant à l’écoute de votre application (les données de production sont vos meilleures amies), vous développerez un véritable sens de la performance. Rappelez-vous : “Build it, but also make sure it runs fast and scales!”. Bonne optimisation à tous !" }, { "title": ".NET 10 - Nouveautés, Performances et Support Prolongé", "url": "/posts/dotnet10/", "categories": "", "tags": "dotnet", "date": "2025-10-06 19:00:00 -0400", "snippet": "En novembre 2025, Microsoft lancera .NET 10, probablement lors de .NET Conf (du 11 au 13 novembre). Cette version s’annonce riche en améliorations de performance et nouvelles fonctionnalités, tout ...", "content": "En novembre 2025, Microsoft lancera .NET 10, probablement lors de .NET Conf (du 11 au 13 novembre). Cette version s’annonce riche en améliorations de performance et nouvelles fonctionnalités, tout en apportant des changements de support importants. À l’heure où j’écris cet article en fin septembre 2025, .NET 10 est en RC1 (depuis le 9 septembre), et la version finale sera une LTS (Long Term Support) prise en charge jusqu’en novembre 2028. Notons d’ailleurs un ajustement de stratégie : les versions STS (Standard Term Support, impaires comme .NET 9) bénéficient désormais de 24 mois de support au lieu de 18 auparavant. En clair, .NET 9 (sorti en 2024) aura sa fin de support repoussée à novembre 2026, soit le même jour que .NET 8 LTS..NET 10 étant une version majeure, il est illusoire de vouloir lister tous les changements dans un seul article. Nous allons donc nous concentrer sur les faits marquants. Préparez-vous à découvrir des gains de performance notables, des nouveautés du côté du framework et du langage C#14, ainsi que quelques conseils sur la migration.Des performances encore amélioréesChaque nouvelle version de .NET apporte son lot d’optimisations, et .NET 10 ne fait pas exception. L’ingénieur Stephen Toub a publié son traditionnel pavé de plus de 200 pages détaillant “des centaines de petites améliorations qui, mises bout à bout, rendent .NET 10 plus rapide”. Inutile de chercher une révolution unique : les gains viennent d’une multitude de micro-optimisations dans le JIT, le GC, les collections, etc., qui grattent des nanosecondes par-ci, quelques octets par-là, sur des opérations exécutées des milliards de fois. Par exemple, le runtime est désormais plus intelligent pour allouer certains objets sur le stack plutôt que dans le heap (grâce à l’escape analysis), évitant des allocations mémoire. Les boucles foreach sur des collections sont un poil plus efficaces, tout comme de nombreuses méthodes LINQ courantes qui ont été retravaillées.En somme, .NET 10 est encore plus performant. Vos applications web, services et outils gagneront en rapidité sans changer une ligne de code, grâce aux améliorations du JIT, des algorithmes de tri, du threading et bien d’autres. De quoi faire plaisir aux développeurs en quête de performance… et aux utilisateurs finaux qui apprécieront la réactivité.Nouvelles fonctionnalités côté framework et runtimeJSON Patch intégré à System.Text.JsonEnfin une bonne nouvelle pour les API REST : .NET 10 introduit une implémentation native de JSON Patch (RFC 6902) basée sur System.Text.Json. Jusqu’à présent, appliquer un patch JSON dans ASP.NET Core nécessitait la bibliothèque externe Newtonsoft.Json. Désormais, plus besoin de cet extra, on peut manipuler des JsonPatchDocument directement via System.Text.Json, avec de bien meilleures performances et une empreinte mémoire réduite. Concrètement, un package Microsoft.AspNetCore.JsonPatch.SystemTextJson fait son apparition pour activer cette fonctionnalité dans vos contrôleurs Web API. 💡 Note : cette nouvelle implémentation ne gère pas les types dynamic et n’est pas totalement compatible à 100% avec l’ancienne (quelques cas limites diffèrent). Mais pour l’écrasante majorité des usages, on peut enfin se passer de Newtonsoft et appliquer proprement des opérations de patch (add, remove, replace, etc.) sur nos modèles JSON.Observabilité renforcée : métriques ASP.NET CoreLes équipes .NET ont beaucoup travaillé sur l’observabilité. .NET 10 enrichit ASP.NET Core de nouvelles métriques intégrées pour mieux monitorer vos applications. Par exemple, le framework expose désormais des compteurs (counters, histograms…) pour suivre les événements d’authentification et d’autorisation : nombre de nouvelles créations d’utilisateurs, changements de mot de passe, tentatives de connexion, etc. Des métriques spécifiques à ASP.NET Core Identity ont été ajoutées pour mesurer la durée de certaines opérations (exemple : aspnetcore.identity.user.create.duration pour la création d’un utilisateur).De même, le gestionnaire de mémoire interne d’ASP.NET (pools de mémoire) expose maintenant des compteurs de «memory eviction», utiles pour voir si votre application évacue souvent des données du cache en mémoire. Même Blazor a droit à des métriques de cycle de vie de composant et du traçage plus poussé, ce qui facilitera le diagnostic de vos applications WebAssembly côté client.Toutes ces métriques sont accessibles via le système de métriques .NET (basé sur EventCounters/OpenTelemetry Metrics). En clair, vous pouvez brancher vos tableaux de bord de monitoring (Prometheus, Grafana, Application Insights…) et obtenir une observabilité fine sans écrire de code maison. Un vrai plus pour détecter les goulots d’étranglement et surveiller la santé de vos applications en production.Sécurité et identité : support des PasskeysLa révolution passwordless arrive dans .NET 10 ! ASP.NET Core Identity prend désormais en charge les Passkeys (clés d’authentification FIDO2/WebAuthn). Concrètement, le template d’application Blazor (avec identité individuelle) a évolué : l’espace « Passkeys » permet aux utilisateurs d’enregistrer une clé biométrique ou un appareil de sécurité pour se connecter.Les passkeys fournissent une méthode d’authentification sans mot de passe, utilisant des dispositifs biométriques ou des clés sécurisées liées à l’appareil de l’utilisateur. Avec .NET 10, on peut donc offrir aux utilisateurs la possibilité de se connecter avec Windows Hello, Touch ID, clés USB de sécurité, etc., plutôt qu’avec le traditionnel couple login/mot de passe. C’est à la fois plus sécurisé (immunisé contre le phishing) et plus pratique une fois configuré.Petite dose de réalisme toutefois : dans l’état actuel (Preview 6), le gabarit impose encore de créer un mot de passe lors de l’inscription initiale, le passkey venant en option ensuite. Cela a fait tiquer certains (après tout, le but des passkeys est d’éliminer totalement les mots de passe…). Gageons que de futures itérations permettront une inscription 100% sans mot de passe. Quoi qu’il en soit, intégrer WebAuthn nativement dans le framework est un grand pas en avant. Si la sécurité de vos applications web vous tient à cœur, vous pourrez tirer parti de cette fonctionnalité pour proposer le login le plus sûr à vos utilisateurs.Mode script en C#Avec .NET 10, Microsoft continue de faciliter la vie des développeurs en matière de déploiement et de prototypage. Le Mode script C# (dotnet run &lt;fichier&gt;.cs) est l’une des nouveautés les plus cool pour les développeurs : .NET 10 permet d’exécuter directement un fichier .cs sans projet ni solution. En tapant simplement dotnet run monScript.cs, le SDK compile et lance le code C# immédiatement. Idéal pour tester un bout de code, écrire un petit utilitaire jetable, ou apprendre C# sans passer par la case .csproj. Plus besoin de créer un projet complet pour un programme de 5 lignes ! Cette expérience « script » rapproche C# de langages comme Python ou JavaScript pour les scenarios rapides. (Ne jetez pas encore Visual Studio, pour des applications sérieuses, le projet reste pertinent, mais pour un script d’administrateur, c’est la vie.).NET 10 améliore la productivité sur tout le cycle de vie : du prototype au déploiement final.Évolutions du langage C# 14Qui dit nouvelle version .NET dit souvent nouvelle version du langage C#. .NET 10 s’accompagne de C# 14, qui apporte quelques raffinements très appréciables pour les développeurs, sans bouleverser la syntaxe existante.Extension Members : la fonctionnalité “extension everything”C# 14 réalise un fantasme de longue date : permettre des members d’extension, pas juste des méthodes. Depuis C# 3.0 (en 2007), on peut écrire des méthodes d’extension (extension methods) pour ajouter des méthodes “virtuelles” à des types existants. Mais on ne pouvait pas créer de propriétés d’extension, ni étendre les méthodes statiques. Plusieurs tentatives avaient échoué dans le passé, surnommées “extension everything”. Cette fois c’est la bonne : les extension members débarquent en C# 14.Concrètement, une nouvelle syntaxe permet de déclarer dans une classe statique un bloc extension ciblant un type (par exemple IEnumerable&lt;T&gt;) et d’y définir : des propriétés d’extension (comme un IsEmpty accessible sur n’importe quel IEnumerable&lt;T&gt;), des méthodes d’extension statiques (s’apparentant à des méthodes de classe du type étendu), même des opérateurs d’extension (surcharge de operator+ par exemple) qui agissent comme si le type les proposait nativement.Cette syntaxe est un peu verbeuse au premier abord, mais elle organise mieux le code.Par exemple, on peut maintenant écrire :public static class Enumerable { extension&lt;T&gt;(IEnumerable&lt;T&gt; source) { public bool IsEmpty =&gt; !source.Any(); // propriété d’extension public IEnumerable&lt;T&gt; Where(Func&lt;T,bool&gt; p) { … } // méthode d’extension (instance) } extension&lt;T&gt;(IEnumerable&lt;T&gt;) { public static IEnumerable&lt;T&gt; Combine(IEnumerable&lt;T&gt; first, IEnumerable&lt;T&gt; second) { ... } // méthode statique d’extension public static IEnumerable&lt;T&gt; operator+ (IEnumerable&lt;T&gt; left, IEnumerable&lt;T&gt; right) =&gt; left.Concat(right); // opérateur d’extension }}Dès lors, je peux appeler maListe.IsEmpty comme s’il s’agissait d’une propriété native du type List, ou encore IEnumerable&lt;int&gt;.Combine(seq1, seq2) comme une méthode statique de IEnumerable Ça rend le code plus lisible et discoverable (surtout dans IntelliSense) qu’une méthode d’extension classique perdue dans une classe utilitaire. Bref, une fonctionnalité favorite de beaucoup (dont moi-même 😊), qui va permettre de mieux organiser nos extensions.(Et pour les puristes : pas de panique, vos méthodes d’extension actuelles continuent de fonctionner comme avant, cette nouvelle syntaxe est optionnelle et rétrocompatible.)Mot-clé field : propriétés auto-géréesAutre petit sucre syntaxique bienvenu : le mot-clé field, qui simplifie la gestion des champs privés dans les propriétés. Si vous avez déjà implémenté manuellement le backing field d’une propriété pour, par exemple, contrôler la valeur lors du set (value ?? throw…), vous savez que c’est un peu verbeux. C# 14 permet maintenant d’écrire directement la logique dans l’accesseur set, en utilisant field pour représenter le champ sous-jacent.Exemple : au lieu de :private string _msg;public string Message { get =&gt; _msg; set =&gt; _msg = value ?? throw new ArgumentNullException(nameof(value));}On peut écrire simplement :public string Message { get; set =&gt; field = value ?? throw new ArgumentNullException(nameof(value));}Le compilateur se charge de générer le champ privé caché. Le code est plus concis et clair. Bien sûr, field n’est valable que dans le contexte d’un accesseur de propriété, il représente le stockage interne. Attention si vous avez déjà une variable nommée field dans votre classe, cela peut ambiguïser, il est conseillé de la renommer ou d’utiliser @field pour lever l’ambiguïté.Numeric String Comparer : le tri “humain” débarqueUn petit irritant qui nous donnait la vie dure : le tri de chaînes “avec des nombres” rend souvent des résultats surprenants. Par exemple, quand on trie des versions ou des étiquettes comme [\"v1\", \"v2\", \"v10\"], un tri lexicographique donnera quelque chose comme [\"v1\", \"v10\", \"v2\"], ce qui est contre-intuitif. 😅Avec .NET 10, le numeric string comparer corrige cela. Il permet de comparer les chaînes en traitant les morceaux numériques à l’intérieur comme des nombres, ce qui donne un tri “logique” :var list = [\"Windows 10\", \"Windows 7\"];// tri par défautlist.Sort(StringComparer.Orginal);Console.WriteLine(string.Join(\", \"), list)); // \"Windows 10\", \"Windows 7\"list.Sort(StringComparer.NumericOrdering);Console.WriteLine(string.Join(\", \"), list)); // \"Windows 7\", \"Windows 10\"Pourquoi c’est utile : Pour trier des versions ou des tags dans les interfaces utilisateurs, logs, ou listes de fichiers, où le résultat “humain” est préférable. Pour le frontend ou les API qui renvoient des listes de fichiers ou de révisions, afin d’éviter de devoir post-traiter ou écrire une logique “maison”. Pour les développeurs de NuGet ou outils CLI qui manipulent des noms de versions, identifiants, etc.nameof plus puissant et autres bricolesLe mot-clé nameof(...) évolue aussi légèrement : il supporte à présent les types génériques non construits (unbound generic types). En clair, vous pouvez obtenir le nom d’un type générique sans spécifier ses paramètres. Par exemple, nameof(Dictionary&lt;T&gt;) renverra “Dictionary” (ou une forme qui indique les paramètres génériques). Auparavant, ce n’était pas possible directement, ce qui obligeait à des contournements. Ce changement, bien que mineur, facilite la réflexion ou la génération de code source en évitant des exceptions ou manipulations de chaînes peu élégantes.Parmi les autres nouveautés de C# 14, plus ponctuelles, citons : les opérateurs d’affectation composés personnalisés (on peut surcharger operator += par exemple, si le type supporte déjà +), les constructeurs partiels (utile pour les générateurs de code source), la possibilité d’ajouter des modificateurs ref, out ou params aux paramètres des lambdas, ou encore l’affectation null-conditionnelle (x?.Prop = value qui évite de devoir faire un if null avant d’assigner). Ces raffinements visent à rendre le langage plus cohérent et expressif, sans bouleverser vos habitudes.Améliorations ASP.NET Core et écosystèmeMinimal APIs : support de ProblemDetails et autresLe modèle des Minimal API (introduit en .NET 6) continue de combler ses manques. En .NET 10, la gestion des erreurs de validation de modèle s’améliore : les minimal APIs intègrent désormais nativement le support de IProblemDetailsService pour formater les réponses d’erreurs. En d’autres termes, si vous avez des Endpoints en minimal API qui valident l’entrée et retournent une 400 en cas d’erreur, vous pourrez profiter d’une réponse JSON standardisée ProblemDetails sans effort supplémentaire. Fini les tours de magie pour aligner le format d’erreur sur celui du reste de votre application : le framework fournit une sortie cohérente, et vous pouvez même la personnaliser en enregistrant votre implémentation de ProblemDetailsService (par exemple, pour ajouter un code erreur personnalisé, un lien de documentation, etc.).Au niveau des APIs, ASP.NET Core 14 supporte aussi pleinement les Server-Sent Events (SSE) en sortie des contrôleurs ou minimal APIs, via une méthode utilitaire TypedResults.ServerSentEvents(...). Cela facilite l’envoi de flux temps-réel du serveur vers le client sans WebSockets (utile pour du monitoring, des notifications live, etc.). C’était faisable à la main avant, mais là encore le framework le prend en charge nativement.Ouverture et standards : OpenAPI 3.1 et gRPCLe support d’OpenAPI 3.1 est désormais intégré dans .NET 10. Si vous générez la documentation Swagger de vos APIs, elle sera produite par défaut au format OpenAPI 3.1 (au lieu de 3.0 précédemment). Cette version 3.1 apporte notamment une meilleure prise en charge de JSON Schema 2020-12 (ceux qui ont sacré avec les schémas de nullabilité en OpenAPI 3.0 apprécieront : plus besoin du champ nullable: true, on utilise directement un type null dans le schéma). Concrètement, vos int et long nullable apparaîtront correctement comme type: [\"integer\",\"null\"] dans le JSON de la spécification. Quelques changements breaking sont à noter dans la librairie OpenAPI.NET interne (passage en v2.0 de Microsoft.OpenApi), surtout si vous avez écrit des document filters ou operation filters custom : les types ont un peu changé (les schémas utilisent une interface IOpenApiSchema au lieu d’une classe concrète, etc.). En résumé, .NET suit l’évolution du standard OpenApi, pour que vos APIs restent au goût du jour.Pendant qu’on parle de services web : du côté de gRPC, notons une amélioration sympathique : la gestion du streaming côté client (Client Streaming) devient plus ergonomique. .NET 10 apporte la prise en charge des message handlers HTTP dans les appels gRPC client, permettant des scénarios de retry ou de logging plus intégrés. (D’accord, c’est un peu pointu, mais si vous faites du gRPC, jetez un œil aux release notes pour découvrir ces ajustements.)EF Core 10 : filtres globaux nommés et Left JoinEntity Framework Core 10 accompagne la sortie de .NET 10, et apporte lui aussi son lot de nouveautés. Le plus notable est sans doute l’arrivée des filtres de requête nommés (named query filters). EF Core propose depuis longtemps les filtres globaux (HasQueryFilter) pour, par exemple, implémenter le soft delete (exclure les entités dont IsDeleted = true) ou la multi-tenance (filtrer par TenantId). Cependant, jusqu’à présent on ne pouvait définir qu’un seul filtre global par entité, pas très pratique si l’on voulait combiner, puis en désactiver un sélectivement. EF 14 lève cette limitation : on peut maintenant attacher plusieurs filtres globaux sur un même modèle en leur donnant un nom unique.Exemple :modelBuilder.Entity&lt;Blog&gt;() .HasQueryFilter(\"SoftDeletionFilter\", b =&gt; !b.IsDeleted) .HasQueryFilter(\"TenantFilter\", b =&gt; b.TenantId == tenantId);Puis, dans une requête LINQ, décider d’ignorer l’un des deux filtres :// Récupérer tous les blogs en incluant ceux supprimés, // mais en gardant le filtre de Tenantvar allBlogs = context.Blogs .IgnoreQueryFilters(\"SoftDeletionFilter\") .ToList();Cette granularité était très attendue pour les applications complexes : on peut enfin combiner proprement des filtres globaux multiples (et éviter les contorsions du style Enable/Disable Filter sur tout le contexte).Parmi les autres améliorations d’EF Core 14, on notera le support direct des jointures Left Join et Right Join en LINQ. Auparavant, écrire une requête LINQ équivalente à un LEFT JOIN SQL demandait une syntaxe peu intuitive avec GroupJoin + DefaultIfEmpty.Désormais, on dispose d’une méthode d’extension .LeftJoin(...) (et .RightJoin(...)), rendant le code plus lisible.Par exemple :var query = context.Students .LeftJoin( context.Departments, student =&gt; student.DepartmentID, dept =&gt; dept.ID, (student, dept) =&gt; new { student.Name, Department = dept.Name ?? \"[NONE]\" } );EF Core sait traduire ça en SQL (LEFT JOIN ou RIGHT JOIN). Cela ne change rien en termes de performance par rapport à avant, mais niveau lisibilité du code, c’est le jour et la nuit.Enfin, EF 14 ajoute plein de petits plus : la recherche Full-Text sur Azure Cosmos DB, des traductions SQL supplémentaires (exemple, la méthode DateOnly.ToDateTime() désormais convertie nativement en SQL), des améliorations de performance sur les split queries (pour éviter des incohérences de tri dans les requêtes fractionnées), etc. De quoi rendre vos accès aux données plus flexibles et efficaces.En route vers .NET 10 : faut-il migrer ?La grande question pour les équipes : devez-vous passer à .NET 10 rapidement ? La réponse dépend de votre situation, mais voici quelques éléments de réflexion : .NET 10 est une version LTS (support long), ce qui en fait un candidat solide pour vos applications en production. Elle sera supportée 3 ans (jusqu’à la fin de 2028), ce qui donne une vision claire à long terme. Si vous êtes encore sur .NET 8 LTS, rien ne presse (le support de .NET 8 court jusqu’en nov. 2026, mais .NET 10 représente la prochaine cible LTS “naturelle”). Si vous êtes sur .NET 9 (STS), notez que le nouvel alignement du support fait que .NET 9 expirera en même temps que .NET 8 en 2026. Autrement dit, .NET 9 ne prolonge pas votre horizon par rapport à .NET 8. Dans ce cas, migrer vers .NET 10 (LTS) dès que possible vous remet sur un cycle plus confortable. Les gains de performance et les nouvelles fonctionnalités peuvent justifier la migration, surtout si vous avez besoin de l’une des nouveautés (par exemple, le JSON Patch natif ou les passkeys pour vos utilisateurs). .NET 10 apporte aussi de nombreuses corrections de bugs et améliorations de stabilité accumulées depuis .NET 8 et .NET 9. Breaking changes : évidemment, qui dit nouvelle version dit potentiels changements incompatibles. Microsoft a publié la liste des breaking changes de .NET. Au moment de la RC1, cette liste n’était pas totalement finalisée, mais on sait déjà qu’il y aura quelques ajustements de comportements (par exxemple, le nameof sur types génériques mentionné plus haut, ou des API retirées après obsolescence). Avant de migrer, examinez attentivement ces notes de compatibilité pour évaluer l’impact sur votre code. La plupart du temps, l’upgrade se passera sans encombre pour du code standard, mais mieux vaut prévenir que guérir !En pratique, .NET 10 s’annonce comme une version mature et aboutie, avec un bel équilibre entre performance, productivité et modernisation du stack. Si vos tests de validation passent et que vos dépendances (NuGet, frameworks) supportent .NET 10, il n’y a pas de raison d’attendre trop longtemps. Microsoft elle-même encourage la mise à niveau : “Si vous prévoyez de passer de .NET 9 à 10 bientôt, continuez dans cette voie, .NET 10 apporte plein de nouvelles capacités et de meilleures performances”.Pour finir : outillage et écosystèmeUn dernier mot pour les développeurs curieux : en parallèle de .NET 10, Microsoft commence à dévoiler la prochaine version de Visual Studio. Eh oui, Visual Studio 2026 (Insiders) est déjà disponible en aperçu. Au menu : améliorations de l’éditeur, support de C# 14 bien sûr, et d’une intégration toujours plus poussée de GitHub Copilot et de l’IA. Vous pouvez installer Visual Studio 2026 Insiders en parallèle de votre installation existante sans risque, la grande majorité des extensions de Visual Studio 2022 sont compatibles, et dès cette version la couverture de code est incluse par défaut dans toutes les éditions, inutile désormais d’avoir la version Entreprise pour en profiter.Enfin, l’écosystème .NET suit le mouvement : attendez-vous à voir arriver Entity Framework 14, ASP.NET Core 14, Blazor, MAUI 14, etc., alignés sur cette nouvelle mouture. Les outils de build, CI/CD, containers Docker et autres seront mis à jour également. La documentation Microsoft Learn est enrichie progressivement (il existe déjà des pages “What’s new” pour .NET 10, EF 14, C# 14… très utiles pour approfondir chaque nouveauté).En résumé, .NET 10 marque une étape importante en 2025 pour les développeurs .NET. Support étendu, optimisation de tous les recoins du runtime, nouvelles fonctionnalités qui suppriment des irritants (adieu certaines bibliothèques tierces obligatoires), langage C# toujours plus expressif… Le tout sans révolution brutale : la montée en version devrait être relativement fluide. Que vous soyez un développeur back-end curieux des moindres gains de performances, ou un lead dev prudent qui ne jure que par les LTS stables, .NET 10 a de quoi vous séduire.Alors, allez-vous migrer ? Si vous cherchez la sécurité et la durabilité, .NET 10 LTS est là pour vous jusqu’en 2028. Si vous aimez les nouveautés, foncez, vous aurez de quoi vous amuser. Dans tous les cas, commencez à évaluer cette version, à faire tourner vos tests unitaires dessus, et préparez sereinement l’avenir de vos applications.Bonne exploration de .NET 10, et happy coding !" }, { "title": "Les design patterns - les bons Legos pour vos applications .NET", "url": "/posts/utilisation-design-patterns/", "categories": "architecture", "tags": "", "date": "2025-09-22 10:00:00 -0400", "snippet": "Introduction : Ne réinventez pas la roue, assemblez les LegosEn conception logicielle, un design pattern (ou patron de conception) est une solution éprouvée à un problème de conception récurrent. C...", "content": "Introduction : Ne réinventez pas la roue, assemblez les LegosEn conception logicielle, un design pattern (ou patron de conception) est une solution éprouvée à un problème de conception récurrent. Ce sont en quelque sorte nos briques de Lego à nous, développeurs : plutôt que de tailler chaque pièce dans le bois, on pioche dans la boîte le composant adapté au bon moment. Bien comprendre ces patterns permet d’assembler des systèmes plus robustes et évolutifs, un peu comme construire un château de Lego solide sans recoller les briques au chewing-gum. 😉En d’autres termes, les design patterns nous évitent de réinventer la roue sur chaque projet. Ils définissent des structures de classes ou d’objets qui ont fait leurs preuves. Par exemple, le Singleton s’assure de l’existence d’un seul objet de son genre et fournit un point d’accès global à celui-ci. Utiliser un pattern approprié au bon contexte, c’est comme choisir la bonne pièce Lego standard plutôt que de mouler la vôtre. Cela accélère le développement et améliore la maintenabilité, tout en respectant les principes SOLID.Avant de plonger dans sept patterns importants pour les applications de gestion en .NET Core, mentionnons une excellente ressource : la série de vidéos de Christopher Okhravi sur les design patterns (disponible sur YouTube). Elle reste encore aujourd’hui une référence pédagogique (et humoristique) sur le sujet. Vous verrez qu’avec un peu de pratique, les design patterns deviendront vos alliés du quotidien pour structurer proprement un code métier (facturation, inventaire, etc.) sans transformer votre code en plat de spaghetti. 🍝Nous allons explorer les patterns suivants, avec pour chacun une explication, un exemple concret, un extrait de code C#, les cas d’usage à favoriser (et à éviter), ainsi que des astuces d’intégration en .NET (notamment via l’IoC container et autres outils comme Scrutor pour le pattern Decorator).En piste !Singleton : Un seul pour les gouverner tous, un seul pour les instancier,Le pattern Singleton garantit qu’une classe n’a qu’une seule instance accessible globalement.Le Singleton est le Highlander des patterns : « There can be only one! » 😅 Son objectif est de restreindre l’instanciation d’une classe à un seul objet, partagé et accessible partout. Dans une application de gestion, on utilise souvent ce pattern pour des objets qui doivent être uniques : par exemple un gestionnaire de configuration global, une cache en mémoire ou encore un logger central. Cela évite d’avoir plusieurs copies incohérentes de ces ressources critiques disséminées un peu partout.Exemple concretImaginons un système de facturation qui doit accéder aux paramètres de configuration (taux de TVQ, devise, etc.) depuis n’importe quel module. On pourrait créer une classe ConfigurationManager en Singleton. Ainsi, que ce soit le module de génération de facture ou le module d’inventaire, tous récupèrent la même instance unique du ConfigurationManager avec les paramètres chargés au lancement de l’application.public sealed class ConfigurationManager{ private static readonly ConfigurationManager _instance = new(); public static ConfigurationManager Instance =&gt; _instance; // Données de configuration (exemple) public decimal TauxTVQ { get; private set; } public string DeviseParDefaut { get; private set; } // Constructeur privé pour empêcher les instanciations externes private ConfigurationManager() { // Simulation de chargement depuis une source de vérité (fichier, base, ou grimoire fiscal) TauxTVQ = 0.09975m; // TVQ actuelle au Québec DeviseParDefaut = \"CAD\"; // Vive le dollar canadien! } // Méthode d'accès optionnelle public static ConfigurationManager GetInstance() =&gt; Instance;}Dans le code ci-dessus, ConfigurationManager.Instance renverra toujours la même instance, initialisée une seule fois. On déclare le constructeur en private pour empêcher de faire new ConfigurationManager(). Seule la propriété statique Instance permet d’obtenir l’objet unique.Quand l’utiliser 🟢 : Lorsqu’une seule instance d’un service doit exister et être partagée partout. Exemples classiques : configuration globale, journalisation (logging), connexion unique à une base de données, cache applicatif. Cela permet de centraliser un état ou un accès et d’éviter les duplications.À éviter 🔴 : N’abusez pas du Singleton ! Un Singleton mal placé peut devenir un god object omniprésent qui rend le code difficile à tester (dépendances globales cachées) et moins modulable. Si plusieurs instances seraient acceptables ou que l’objet a un cycle de vie limité, préférez des instances normales gérées par injection de dépendances. En .NET, beaucoup considèrent d’ailleurs le Singleton comme un antipattern si utilisé à tort et à travers.Intégration .NET 👩‍💻 : Plutôt que d’implémenter le Singleton “à la main” comme ci-dessus, on profite souvent du conteneur d’inversion de contrôle (IoC) de .NET. En enregistrant une classe en singleton dans le container, le framework garantit lui-même l’unicité de l’instance. Par exemple, dans le Program.cs :services.AddSingleton&lt;IConfigurationManager, ConfigurationManager&gt;();Ici le container va créer une seule instance de ConfigurationManager pour toute l’application. C’est thread-safe et plus simple à tester (on peut éventuellement substituer l’implémentation via l’interface). Astuce: Si votre Singleton est lourd à instancier et pas toujours utilisé, envisagez le pattern du Lazy Singleton (instanciation différée).Factory : Fabrique d’objets, évitez le newLe pattern Factory (ici la Factory Method) crée des objets selon le type demandé sans exposer la logique de création.Le pattern Factory est un patron de création qui repose sur le principe suivant : « Ne dites pas new, dites Factory! ». Plutôt que d’instancier directement des classes dans votre code (ce qui le couplerait à des implémentations concrètes), vous déléguez cette tâche à une fabrique. La Factory centralise la logique de création et peut décider quel sous-type concret retourner en fonction du contexte. C’est un peu comme une usine qui, sur base d’une commande, sort le bon produit de la chaîne sans que l’acheteur ne sache exactement comment il a été fabriqué.Exemple concretDans un système de gestion documentaire (factures, bons de commande, devis, etc.), on peut avoir une interface commune IDocument et plusieurs implémentations (Facture, Devis, BonCommande…). Au lieu d’éparpiller du code new Facture() partout, on définit une factory qui va produire le bon type de document selon un paramètre. Par exemple, une méthode DocumentFactory.CreateDocument(TypeDocument type) qui retourne un IDocument :// Type énuméré pour les différents documents possiblespublic enum TypeDocument{ Facture, Devis, BonCommande}public interface IDocument { string Numero { get; } DateTime DateEmission { get; } // ... autres membres communs}public class Facture : IDocument { /* ... */ }public class Devis : IDocument { /* ... */ }public class BonCommande : IDocument { /* ... */ }public static class DocumentFactory { public static IDocument CreateDocument(TypeDocument type) { return type switch { TypeDocument.Facture =&gt; new Facture(), TypeDocument.Devis =&gt; new Devis(), TypeDocument.BonCommande =&gt; new BonCommande(), _ =&gt; throw new ArgumentException(\"Type de document inconnu\") }; }}Ici, le DocumentFactory expose une méthode statique de création. Le client appelle par exemple IDocument doc = DocumentFactory.CreateDocument(TypeDocument.Facture); pour obtenir une facture prête à l’emploi, sans savoir quelle classe concrète est instanciée. Si demain on ajoute un nouveau type de document (par exemple un avoir), il suffit d’étendre la factory sans impacter le code client.Quand l’utiliser 🟢 : Quand la logique d’instanciation est complexe ou conditionnelle. Par exemple, si la création d’un objet dépend de paramètres (contexte utilisateur, configuration) ou nécessite de décider parmi plusieurs types dérivés. Les Factories améliorent la lisibilité (on donne un nom explicite au processus de création) et centralisent en un seul endroit le code qui fait des new. Très utile pour : Factory Method : laisser des sous-classes décider de l’instanciation (patron utilisé dans les frameworks, par exemple pour créer des contrôles UI spécifiques). Abstract Factory : groupe de factories pour créer des familles d’objets liés (exemple une AbstractFactory pour GUI qui peut produire soit des boutons et fenêtres version “Windows”, soit version “macOS”).À éviter 🔴 : Si la création de l’objet est triviale et ne nécessite pas de logique conditionnelle, une factory ajoute une complexité inutile. Inutile de sur-ingénierie : on ne va pas faire une factory pour instancier un simple DTO par exemple. De plus, trop de factories peuvent rendre le code abstrait à l’excès (on finit par avoir des usines qui fabriquent d’autres usines…). Comme toujours, dosez le pattern où il apporte une réelle valeur.Intégration .NET 👩‍💻 : Les IoC containers réduisent parfois le besoin explicite de factory, car ils peuvent eux-mêmes choisir quelle implémentation injecter selon la configuration. Par exemple, on peut enregistrer plusieurs implémentations d’une interface et en sélectionner une par nom ou via une factory lambda dans le container. Néanmoins, le pattern Factory reste utile pour créer des objets métiers complexes.En .NET Core, vous pouvez aussi injecter une factory sous forme de fonction. Ex: Func&lt;TypeDocument, IDocument&gt; que l’IoC pourrait construire. Une autre approche est d’utiliser la factory en tant que service : vous créez une classe DocumentFactory (non statique) enregistrée en singleton, et qui a peut-être besoin d’autres services (elle peut les recevoir via injection dans son constructeur) pour fabriquer les objets. Cette technique vous permet par exemple de choisir l’implémentation en fonction de données à l’exécution (on le verra avec le patron Strategy juste après).Strategy : des algorithmes interchangeables, choisis à la voléeLe pattern Strategy définit des algorithmes interchangeables. Le contexte délègue à une stratégie concrète (ConcreteStrategyA ou ConcreteStrategyB) choisie dynamiquement.Le pattern Strategy permet de définir une famille d’algorithmes, de les encapsuler chacun dans une classe distincte, et de les rendre interchangeables à la volée dans le contexte où ils sont utilisés. En clair, on sépare le quoi (l’algorithme à exécuter) du quand/comment il est utilisé.L’un des grands bénéfices de ce pattern est qu’il respecte le principe d’ouverture/fermeture (Open/Closed Principle) : votre code est ouvert à l’extension, mais fermé à la modification. Autrement dit, si un jour un nouveau mode de calcul doit être ajouté (par exemple une livraison par drone 🛸), pas besoin de toucher au calculateur existant : on ajoute une nouvelle stratégie, et c’est tout.C’est une alternative élégante et maintenable aux chaînes de if/else ou de switch sur des enums, qui violent souvent ce principe (car chaque ajout nécessite de modifier le bloc conditionnel existant). Avec le patron Strategy, on étend le comportement en ajoutant une nouvelle classe, plutôt qu’en modifiant du code existant — ce qui réduit les risques de régression.Exemple concretDans une application de gestion de commerce en ligne, prenons le calcul des frais de livraison. Selon le mode d’expédition choisi par le client, le calcul du coût diffère (transporteur standard, express, retrait en magasin gratuit, etc.). Sans pattern, on aurait peut-être dans la classe Commande un code semblable à :decimal frais = mode switch{ \"Standard\" =&gt; CalculerStandard(), \"Express\" =&gt; CalculerExpress(), \"Magasin\" =&gt; 0, _ =&gt; throw new NotImplementedException()};...Avec le patron Strategy, on va créer une interface IFraisLivraisonStrategy avec une méthode CalculerFrais(Commande commande). Puis on implémente une classe concrète par mode : StandardStrategy, ExpressStrategy, RetraitMagasinStrategy, etc., chacune encapsulant son calcul. Le contexte (par exemple la classe CalculateurLivraison) utilise une référence à IFraisLivraisonStrategy.On peut pousser plus loin en combinant avec une Factory Method pour choisir la stratégie en fonction de la commande :// Stratégie de calcul des frais de livraisonpublic interface IFraisLivraisonStrategy { decimal CalculerFrais(Commande cmd);}// Implémentations concrètespublic class LivraisonStandard : IFraisLivraisonStrategy { public decimal CalculerFrais(Commande cmd) =&gt; 5m; // Forfait simple}public class LivraisonExpress : IFraisLivraisonStrategy { public decimal CalculerFrais(Commande cmd) =&gt; cmd.Poids * 1.5m;}public class RetraitMagasin : IFraisLivraisonStrategy { public decimal CalculerFrais(Commande cmd) =&gt; 0m;}// Contexte qui utilise une stratégiepublic class CalculateurLivraison { private IFraisLivraisonStrategy _strategie; // On devrait injecter la stratégie via le constructeur ! public void ChoisirStrategie(IFraisLivraisonStrategy strat) =&gt; _strategie = strat; public decimal CalculerFrais(Commande cmd) =&gt; return _strategie?.CalculerFrais(cmd) ?? throw new InvalidOperationException(\"Stratégie non définie.\");}Et quelque part dans le code appelant (par exemple lors de la finalisation de la commande) :var calculateur = new CalculateurLivraison();IFraisLivraisonStrategy strategie = modeChoisi switch { \"Standard\" =&gt; new LivraisonStandard(), \"Express\" =&gt; new LivraisonExpress(), \"Magasin\" =&gt; new RetraitMagasin(), _ =&gt; throw new InvalidOperationException(\"Mode inconnu\")};calculateur.ChoisirStrategie(strategie);decimal frais = calculateur.CalculerFrais(commande);Ici, on choisit la stratégie dynamiquement en fonction d’un paramètre modeChoisi. Le calculateur de livraison n’a pas besoin de connaître les détails de chaque mode, il délègue à la stratégie. Si un jour on ajoute un mode Drone ou à dos de vache 🐮, on crée une nouvelle classe implémentant IFraisLivraisonStrategy et on ajuste la sélection (idéalement via une factory au lieu d’un switch inline).Quand l’utiliser 🟢 : Quand vous avez plusieurs variantes algorithmiques interchangeables selon un critère (par exemple, différents règles de calcul, politiques, stratégies de tarification, etc.). Le Strategy apporte de la flexibilité : on peut même changer la stratégie en cours d’exécution si besoin. C’est aussi un bon moyen d’adhérer au principe Open/Closed – ajouter une nouvelle stratégie n’impacte pas les existantes ni le contexte.À éviter 🔴 : Si vos variantes d’algorithmes sont très simples ou qu’il n’y en a qu’une ou deux peu susceptibles d’évoluer, le pattern peut être overkill. Aussi, n’introduisez pas une stratégie juste pour éviter un if unique — le remède serait pire que le mal. Enfin, veillez à ce que les stratégies partagent bien la même interface commune et puissent réellement varier indépendamment du reste : si elles finissent par dépendre fortement du contexte externe, le gain de découplage diminue.Intégration .NET 👩‍💻 : On peut tirer parti de l’IoC container pour gérer les stratégies. Par exemple, vous pourriez injecter toutes les implémentations d’une interface et sélectionner la bonne à l’exécution (via un dictionnaire de stratégies, ou en taguant chaque implémentation d’un attribut \\ [Strategy(\"Nom\")]). Une autre approche consiste à enregistrer une Factory (comme vue plus haut) dans l’IoC qui retourne la stratégie voulue. .NET Core facilite même cela avec les Named Options ou en combinant avec le pattern Policy. Bref, le patron Strategy se marie bien avec l’injection de dépendances pour éviter de faire de new manuellement : on peut demander au container de résoudre la stratégie dont on a besoin, ce qui simplifie le remplacement (ex: injection d’une fausse stratégie en tests unitaires).Decorator : Ajouter des fonctionnalités à la volée (comme une cache sur vos services)Le pattern Decorator ajoute dynamiquement des comportements à un objet en l’enveloppant dans un “décorateur” qui implémente la même interface.Le pattern Decorator (ou Wrapper) est un patron structurel qui permet d’ajouter dynamiquement des responsabilités à un objet, sans modifier son code source. Imaginez un cadeau qu’on emballe et re-emballe : le contenu reste le même, mais chaque couche de papier ajoute une fonctionnalité (un message, un ruban, etc.). En informatique, un décorateur est un objet qui implémente la même interface que l’objet qu’il décore, et qui contient une référence vers lui-même. Il peut ainsi intercepter les appels, faire quelque chose en plus, puis déléguer à l’objet réel.Exemple concretConsidérons un microservice d’inventaire qui expose un service IProduitService avec une méthode ObtenirProduit(int id) retournant les détails d’un produit. Les appels à ce service peuvent être coûteux (imaginons qu’il interroge une base de données distante). On souhaite mettre en cache les résultats pour améliorer les performances. Sans changer le code du service existant, on peut créer un décorateur de cache.public interface IProduitService{ ValueTask&lt;Produit&gt; ObtenirProduit(int id);}// Implémentation principale (accès base de données par exemple)public class ProduitService : IProduitService{ public async ValueTask&lt;Produit&gt; ObtenirProduit(int id) { Console.WriteLine($\"Accès BDD pour le produit {id}\"); await Task.Delay(50); // Simule un accès I/O return new Produit { Id = id, Nom = $\"Produit {id}\" }; }}// Decorator de cachepublic class ProduitServiceCacheDecorator : IProduitService{ private readonly IProduitService _produitService; // Devrait provenir de l'IoC private readonly MemoryCache _cache = new(new MemoryCacheOptions()); public ProduitServiceCacheDecorator(IProduitService produitService) { _produitService = produitService; } public ValueTask&lt;Produit&gt; ObtenirProduit(int id) { if (_cache.TryGetValue(id, out Produit produit)) { Console.WriteLine($\"Cache hit pour le produit {id}\"); return new ValueTask&lt;Produit&gt;(produit); // disponible de manière synchrone } return ObtenirEtCacher(id); } private async ValueTask&lt;Produit&gt; ObtenirEtCacher(int id) { var resultat = await _produitService.GetProduit(id); _cache.Set(id, resultat, TimeSpan.FromMinutes(5)); return resultat; }} 💡 Pourquoi ValueTask ici ? Parce que dans le cas où la cache répond, il est inutile de créer une Task, ValueTask permet de réduire la pression sur le GC pour les cas de réponse rapide. Mais attention : si le service sous-jacent est très souvent asynchrone, Task&lt;T&gt; reste le bon choix !Ici, ProduitServiceCacheDecorator décore un IProduitService concret (_produitService). Il ajoute la fonctionnalité de caching autour de l’appel réel. Pour le consommateur, c’est transparent : il utilise un IProduitService sans savoir si c’est le cache ou le service de base. On peut composer plusieurs décorateurs les uns avec les autres si besoin (log, sécurité, etc.), chaque décorateur enveloppant le précédent.Quand l’utiliser 🟢 : Quand vous voulez enrichir ou modifier dynamiquement le comportement d’un objet sans altérer son code. C’est idéal pour les fonctionnalités transversales (caching, logging, contrôle d’accès, mesure de performance…). Le Decorator est plus souple que l’héritage car on peut empiler plusieurs décorateurs et en activer/désactiver certains à la configuration.À éviter 🔴 : Si la hiérarchie de décorateurs devient trop complexe ou que vous en avez un très grand nombre, ça peut devenir difficile à suivre en debug (effet oignon, on se perd dans les couches). Si l’objet de base a déjà une bonne extension via des mécanismes plus simples, inutile de rajouter en plus des décorateurs. Enfin, n’utilisez pas un décorateur juste pour factoriser du code commun entre deux classes : dans ce cas, une abstraction ou un mixin serait plus approprié.Intégration .NET 👩‍💻 : .NET Core facilite la vie avec l’IoC container pour chaîner les décorateurs sans douleur, grâce à des librairies comme Scrutor. Scrutor fournit une méthode d’extension Decorate&lt;,&gt;() qui enregistre un décorateur pour un service existant. Par exemple, pour notre cas ci-dessus :services.AddScoped&lt;IProduitService, ProduitService&gt;();services.Decorate&lt;IProduitService, ProduitServiceCacheDecorator&gt;();Comme le décrit Andrew Lock, “Scrutor cherche tout service enregistré en IProduitService (ici ProduitService) et le remplace par ProduitServiceCacheDecorator qui prendra en paramètre de constructeur l’ancien service”. En un appel, on a intercalé le cache entre l’application et le service réel. Scrutor gère l’ordre (il faut ajouter le décorateur après le service de base) et la résolution des dépendances supplémentaires du décorateur.Sans Scrutor, on peut toujours enregistrer manuellement un décorateur : par exemple, on enregistre ProduitService puis on enregistre IProduitService pointant vers un provider =&gt; new ProduitServiceCacheDecorator(new ProduitService(...)). Mais avouons que Scrutor fait ça proprement, de façon déclarative.Middleware : le pipeline HTTP dans ASP.NET CorePipeline de middlewares ASP.NET Core : chaque middleware (1, 2, 3) traite la requête puis appelle le suivant, formant une chaîne autour de la requête/réponse.Le terme Middleware désigne un composant logiciel intermédiaire qui s’insère dans une chaîne de traitement. En ASP.NET Core, le pipeline de requête HTTP est une illustration concrète du pattern Chain of Responsibility appliqué aux requêtes et réponses HTTP. Chaque middleware a la possibilité d’agir sur la requête entrante, de décider de la court-circuiter (renvoyer directement une réponse) ou de faire quelque chose avant/après de passer la main au middleware suivant.En fait, un middleware ASP.NET Core est une implémentation spécialisée du pattern Decorator/Chain, dédiée aux requêtes HTTP. Vous en utilisez à chaque fois que vous faites app.UseXyz(...) dans le Program.cs : authentification, logging, routing, etc., sont des middlewares standards.Exemple concretSupposons que l’on veuille journaliser le temps de traitement de certaines requêtes pour une API de gestion d’inventaire. On peut écrire un middleware custom LoggingMiddleware qui, pour chaque requête, enregistre l’heure de début, appelle le composant suivant, puis enregistre l’heure de fin et calcule la durée.public class LoggingMiddleware{ private readonly RequestDelegate _next; private readonly TimeProvider _timeProvider; public LoggingMiddleware(RequestDelegate next, TimeProvider timeProvider) { _next = next; _timeProvider = timeProvider; } public async Task Invoke(HttpContext context) { var debut = _timeProvider.GetUtcNow(); await _next(context); var duration = _timeProvider.GetUtcNow() - debut; Console.WriteLine($\"Requête {context.Request.Path} traitée en {duration.TotalMilliseconds} ms\"); }}Dans le Program.cs, on l’enregistre dans le pipeline :var app = builder.Build();app.UseMiddleware&lt;LoggingMiddleware&gt;();Chaque requête HTTP va passer par ce middleware, puis continuer. On pourrait chaîner d’autres middlewares avant/après. Par exemple, on pourrait ajouter un middleware de cache en amont qui vérifie si la réponse n’est pas déjà en cache (et ne pas appeler _next du tout s’il trouve quelque chose), ou un middleware d’authentification qui vérifie le token JWT puis appelle _next si OK, ou renvoie 401 immédiatement si non autorisé.Quand l’utiliser 🟢 : Les middlewares sont incontournables en développement ASP.NET Core pour tout ce qui est cross-cutting au niveau des requêtes HTTP. C’est littéralement la façon de faire officielle pour filtrer/travailler sur les requêtes et réponses. Dans un contexte plus large, on peut assimiler n’importe quelle suite de traitements séquentiels modulaires à ce concept de middleware. Donc utilisez ce pattern/pipeline dès que vous avez un traitement en plusieurs étapes où chaque étape peut décider de stopper ou de modifier le flux.À éviter 🔴 : En dehors du contexte web, évitez d’abuser des pipelines si un simple appel direct suffit. Inutile de sur-architecturer une simple séquence d’appels en pipeline ultra-générique si elle ne sera jamais modifiée. Pour ASP.NET, faites attention à l’ordre des middlewares : une mauvaise ordre (par exemple placer l’authentification après le routing alors qu’on en a besoin avant) peut causer des bugs subtils. Ce n’est pas tant le pattern en lui-même qu’il faut éviter, mais plutôt sa mauvaise configuration.Intégration .NET 👩‍💻 : Pour ASP.NET Core, l’intégration est native (méthodes UseMiddleware&lt;T&gt;(), etc.). Vous pouvez profiter de l’injection de dépendances dans vos middlewares en définissant un constructeur qui prend les services voulus (le framework les injectera). Ainsi votre middleware peut très bien être un décorateur qui utilise un service de cache ou un repo injecté du container. Enfin, notez que d’autres librairies adoptent ce pattern de pipeline configurable : citons MediatR (avec ses Pipeline Behaviors qui agissent autour des requêtes MediatR), ou Message delegating handlers pour HttpClient. Comprendre le fonctionnement des middlewares vous aidera donc dans de nombreux recoins du framework .NET.Mediator : un hub pour la communication, grâce à MediatRLe pattern Mediator définit un objet intermédiaire qui centralise les communications entre plusieurs composants au lieu qu’ils interagissent directement. Imaginez une tour de contrôle d’aéroport : les avions (composants) ne se parlent pas entre eux en direct, ils parlent tous à la tour (médiateur) qui coordonne tout. Cela réduit le couplage : chaque objet a juste à connaître le médiateur, pas les détails des autres.En .NET, on utilise fréquemment une librairie nommée MediatR (de Jimmy Bogard) pour implémenter ce pattern. MediatR permet d’envoyer des requêtes/commandes et de les faire traiter par un ou plusieurs handlers enregistrés, sans que l’émetteur et le récepteur se connaissent.Exemple concretDans une application de gestion d’inventaire, imaginons qu’on veuille décoller la logique métier des contrôleurs API. On peut définir une commande du domaine, par exemple AjouterProduitCommand (avec les infos du produit), et un handler associé AjouterProduitHandler qui contient la logique pour ajouter le produit (vérifier stock, enregistrer en base de données, etc.). Le contrôleur va juste envoyer la commande via le médiateur, qui lui se charge de trouver et exécuter le handler adéquat.Avec MediatR, cela se traduit par :// Définition d'une commande (implémente IRequest&lt;T&gt; de MediatR)public record AjouterProduitCommand(string Reference, int Quantite) : IRequest&lt;ResultatProduit&gt;;// Implémentation du handler pour cette commandepublic class AjouterProduitHandler : IRequestHandler&lt;AjouterProduitCommand, ResultatProduit&gt;{ private readonly IProduitRepository _produitRepository; public AjouterProduitHandler(IProduitRepository produitRepository) { _produitRepository = produitRepository; } public async Task&lt;ResultatProduit&gt; Handle(AjouterProduitCommand command, CancellationToken cancellationToken) { Produit prod = new Produit { Reference = command.Reference, Quantite = command.Quantite }; await _produitRepository.Ajouter(prod, cancellationToken); return new ResultatProduit(prod.Id, \"Produit ajouté avec succès\"); }}Dans le contrôleur (ou n’importe quel endroit du code) qui a besoin d’ajouter un produit, on ferait :// _mediator est injecté via IMediator de MediatRvar resultat = await _mediator.Send(new AjouterProduitCommand(\"REF123\", 50));Console.WriteLine(resultat.Message);Ici, _mediator.Send(...) va en coulisses trouver le AjouterProduitHandler (grâce au container IoC et MediatR), exécuter sa méthode Handle, et renvoyer le résultat. Le contrôleur n’a aucune référence directe sur le handler ou le repository, il passe par le médiateur.Quand l’utiliser 🟢 : Lorsque vous voulez découpler fortement les composants qui interagissent. Le Mediator est roi dans les architectures CQRS / Médiation : il permet d’envoyer des Commandes et Query sans lier le code d’envoi et le code de traitement. Utile aussi pour implémenter un système de notifications/événements internes : par exemple, plusieurs parties de l’application écoutent un événement via Mediator (avec MediatR ce sont les INotification et leurs handlers multiples). En somme, dès que votre application commence à ressembler à un enchevêtrement de signaux entre modules, introduire un médiateur peut apporter de la lisibilité et une architecture en étoile plutôt qu’en plat de nouilles.À éviter 🔴 : Si le trafic via le médiateur devient trop centralisé, le Mediator peut lui-même devenir un goulot d’étranglement ou un god object déguisé (ex: un seul mediator gigantesque qui sait trop de choses). Il faut l’utiliser pour ce qu’il fait bien : la découplage de l’envoi et du traitement. Si deux composants sont naturellement faits pour interagir directement (faible couplage, usage local), inutile de forcer le passage par un médiateur. Par ailleurs, un excès de médiation peut compliquer le suivi du flux d’appel (on perd un peu la trace de “qui appelle qui” car tout passe par le hub central). Comme toujours, c’est un équilibre.Intégration .NET 👩‍💻 : L’intégration de MediatR en .NET Core est très simple : on installe le package, puis dans Program.cs on fait services.AddMediatR(cfg =&gt; cfg.RegisterServicesFromAssemblyContaining&lt;Program&gt;()); (ou équivalent) pour enregistrer tous les handlers du projet. MediatR utilise le container IoC pour résoudre les handlers. On peut configurer des comportements additionnels (les fameux Pipeline Behaviors mentionnés plus haut, qui permettent d’implémenter des cross-cutting concerns autour des requêtes Mediator, comme la validation, le logging, etc. – c’est en fait un Chain of Responsibility autour du Mediator!).Avec MediatR, on peut envoyer des commandes synchrones ou asynchrones, des requêtes qui attendent une réponse, ou des notifications sans réponse (pub/sub interne). C’est un outil formidable pour structurer du code métier dans les applications de gestion, en appliquant les principes CQRS (séparer écriture/lecture) et Clean Architecture. À noter : plusieurs implémentations du pattern Mediator existent dans l’écosystème .NET. Il est possible d’en coder une version minimaliste maison, ou d’utiliser des bibliothèques comme MediatR, qui a longtemps été une référence. Toutefois, attention : MediatR qui sera prochainement sous licence commerciale, ce qui limite son usage dans les projets professionnels.Builder : construction pas-à-pas et syntaxe fluideLe Builder pattern sépare la construction d’un objet complexe de sa représentation. Ici, le Director utilise un Builder (fluent) pour assembler un Product étape par étape.Dernier pattern mais non des moindres : Builder. Si je vous dis « constructeur avec 12 paramètres », vous me dites 🤮. En effet, quand un objet possède beaucoup de propriétés optionnelles, le passage de paramètres devient illisible et sujet à erreur. Le Builder vient à la rescousse en fournissant une interface de construction progressive (souvent fluide). On crée l’objet en plusieurs étapes, via des méthodes dédiées, plutôt qu’un seul gros constructeur. C’est un peu le mode d’emploi Ikea : on assemble pièce par pièce, et à la fin on obtient le meuble.Exemple concretDans une application de gestion, imaginons un module de génération de rapport PDF complexe (par exemple un rapport d’activité mensuel avec plusieurs sections, en-tête, pied de page, etc.). Plutôt que d’avoir une méthode géante qui prend 20 arguments pour tout configurer, on va utiliser un ReportBuilder qui va fournir des méthodes pour ajouter les différentes parties du rapport de manière lisible.public class Report { public string Titre { get; set; } public List&lt;string&gt; Sections { get; set; } = []; public string PiedDePage { get; set; } // ... éventuellement d'autres propriétés complexes (graphique, tableau, etc.)}public class ReportBuilder{ private readonly Report _report = new Report(); public ReportBuilder Titre(string titre) { _report.Titre = titre; return this; // on retourne le builder pour chaîner } public ReportBuilder AjouterSection(string contenuSection) { _report.Sections.Add(contenuSection); return this; } public ReportBuilder PiedDePage(string textePied) { _report.PiedDePage = textePied; return this; } public Report Build() { return _report; }}Utilisation en syntaxe fluide (fluent interface) :Report rapport = new ReportBuilder() .Titre(\"Rapport d’activité - Mars 2025\") .AjouterSection(\"Chiffre d'affaires : 1M €\") .AjouterSection(\"Nouveaux clients : 50\") .PiedDePage(\"Confidentiel - interne\") .Build();On lit quasiment du français dans le code ! On a progressivement construit l’objet rapport sans se soucier de l’ordre interne des initialisations ni d’oublier un paramètre requis. Le Builder encapsule la logique d’assemblage (ici triviale, mais elle pourrait être plus complexe avec par exemple calcul de totaux en interne avant Build).Quand l’utiliser 🟢 : Quand la création d’un objet est complexe, c’est-à-dire : comporte de nombreux paramètres (optionnels ou obligatoires) rendant un constructeur classique impraticable ou ambigu, nécessite des étapes multiples (par exemple certaines parties doivent être construites avant d’autres), ou quand on veut offrir à l’API utilisateur une syntaxe fluide très lisible pour configurer un objet.Les cas d’utilisation concrets abondent en applications de gestion : construction d’un rapport comme vu, configuration dynamique d’un objet de paramétrage, montage d’une requête SQL/ElasticSearch complexe via un QueryBuilder, etc. Le Builder pattern permet aussi d’isoler la logique de création en un point unique (respect du single responsibility).À éviter 🔴 : Si l’objet à créer est simple (quelques paramètres), un constructeur ou les object initializers de C# (init { Prop1 = …, Prop2 = … }) suffisent amplement. Le Builder, s’il est mal conçu, peut également laisser l’objet dans un état partiel incohérent tant que Build() n’a pas été appelé – attention donc à garantir une utilisation correcte (parfois on marque le constructeur de l’objet cible internal pour forcer le passage par le builder). Évitez aussi le builder juste pour faire du fluent sur des opérations statiques ou non liées à un objet complexe — là c’est le nom qui peut prêter à confusion : Builder sert vraiment à construire un objet.Intégration .NET 👩‍💻 : La syntaxe fluide (fluent syntax) est très répandue dans l’écosystème .NET moderne, souvent inspirée du Builder pattern. Par exemple, les Options de configuration se construisent via des optionsBuilder.AddX() en chaîne, les requêtes LINQ en chaîne, l’API Fluent Validation (RuleFor(x =&gt; x.Name).NotEmpty().WithMessage(\"...\")), etc. Sans être toujours de purs “Builders”, ces syntaxes fluides améliorent la lisibilité et l’enchaînement d’appels.En interne, .NET utilise aussi le Builder pattern pour la construction de gros objets comme l’HostBuilder / WebApplicationBuilder (qui configureront étape par étape votre application).Côté IoC, un Builder est souvent créé via une factory ou fourni par un framework. On ne l’enregistre généralement pas dans le container (on crée le builder quand on en a besoin, puis on jette). Toutefois, rien n’empêche d’injecter un builder pré-configuré si cela a du sens dans votre design.En résumé, le Builder est un peu l’inverse du Factory : on l’utilise pour composer petit à petit un objet complexe, là où la Factory crée d’emblée un objet souvent simple ou retourne une implémentation. Avec le Builder pattern, on prend le temps d’assembler et grâce au fluent interface, le code appelant est clair et expressif.Conclusion : Choisir le bon pattern au bon momentNous avons fait un tour d’horizon de plusieurs design patterns clés en .NET Core. Chaque pattern est un outil dans votre boîte à outils : il a un usage privilégié, des avantages, des inconvénients. L’art de l’architecture logicielle consiste à choisir le bon Lego au bon moment. Il n’y a pas de solution universelle : parfois un simple if vaut mieux qu’un Strategy surdimensionné, parfois un Mediator apporte une structure bienvenue là où le couplage devenait ingérable.Un conseil : entraînez-vous à reconnaître dans votre code ou dans les frameworks que vous utilisez quels patterns sont à l’œuvre. Vous verrez que ASP.NET Core, Entity Framework, etc., sont truffés de ces concepts (Singletons pour les services, Factory methods pour les DbContext, Decorators dans les pipeline, etc.). Comprendre les design patterns vous permettra non seulement de mieux utiliser les API .NET, mais aussi de concevoir vos propres composants de manière élégante et maintenable.Enfin, pour aller plus loin, je le redis, la série de Christopher Okhravi sur YouTube est un excellent complément visuel et pédagogique, avec une touche d’humour qui, je l’espère, aura fait écho à la lecture de cet article. 😉En maîtrisant ces patterns, vous éviterez de réinventer la roue carrée et vous construirez des applications évolutives brique par brique. Alors à vos Legos, prêts, codez ! 🚀" }, { "title": "Faut-il vraiment adopter une architecture microservices", "url": "/posts/quand-utiliser-architecture-microservices/", "categories": "architecture", "tags": "", "date": "2025-09-08 20:00:00 -0400", "snippet": "Préambule “On part en microservices ou on reste monolithique ?”C’est une des questions les plus posées et les moins bien tranchées du monde logiciel. Une question qui revient à chaque projet un pe...", "content": "Préambule “On part en microservices ou on reste monolithique ?”C’est une des questions les plus posées et les moins bien tranchées du monde logiciel. Une question qui revient à chaque projet un peu d’envergure, et qui déclenche systématiquement les mêmes débats passionnés entre collègues, souvent avec des effets secondaires comme la hausse du débit de voix et des soupirs lourds de sens.Certains brandissent l’argument de la scalabilité, d’autres invoquent le Graal du déploiement indépendant. Puis quelqu’un évoque Netflix ou Amazon, et ça y est, tout le monde commence à vouloir des centaines de services, un orchestrateur et une armée de pipelines CI/CD.Mais en même temps… on a tous vu (ou vécu) des projets où l’adoption des microservices a amené plus de douleurs que de solutions. Complexité opérationnelle, effet spaghetti distribué, équipe dépassée par les appels réseau qui se perdent dans la brume. Bref, la promesse des microservices peut vite se transformer en cauchemar… si on s’est trompé de combat.Alors voilà : aujourd’hui, on prend le temps de vraiment répondre à cette question. Sans dogme. Sans buzzword. En regardant froidement (mais gentiment 😄) ce que ça implique de faire — ou pas — du microservice dans un projet .NET Core. L’objectif ? Te donner une méthodologie claire, des exemples concrets, et surtout de quoi prendre une vraie bonne décision pour ton prochain projet.Allez, on déballe tout ça !Monolithe vs microservices : le duel en brefDans le coin gauche, l’application monolithique : un seul bloc déployable, rassemblant toutes les fonctionnalités (base de données, logique métier, UI) dans une même application. C’est l’architecture « tout-en-un » classique. À droite, l’architecture microservices : une constellation de petits services autonomes, chacun focalisé sur une fonctionnalité métier spécifique, qui communiquent entre eux via des APIs réseau. En somme, monolithe = une application unique, microservices = plein d’applications collaborant ensemble.🧱 Architecture monolithique – toutes les fonctionnalités (paiement, panier, inventaire, etc.) cohabitent dans une seule application déployée.🧩 Architecture microservices – chaque fonctionnalité métier est un service indépendant (paiement, panier, inventaire…), communiquant via des appels réseau. Un API Gateway (ou une interface unifiée) sert d’entrée pour le client.Concrètement, dans un monolithe, un module peut appeler directement une fonction d’un autre module en mémoire, comme on passe d’une pièce à l’autre dans la même maison. C’est simple et rapide (une bonne vieille fonction appelée directement). En microservices, ces appels deviennent des requêtes réseau (HTTP, gRPC, etc.), un peu comme passer des coups de fil entre maisons distinctes : c’est plus lourd et ça peut échouer pour tout un tas de raisons indépendantes du code métier.Exemple : imaginons un module de paiement qui doit vérifier une carte de crédit. En monolithe, un simple appel de méthode suffit :// Appel interne en monolithebool estPaiementValide = _servicePaiement.ValiderCarte(numeroCarte);if (!estPaiementValide){ return \"Paiement refusé\";}En microservices, le module paiement serait un service séparé ; il faut alors faire un appel HTTP (ou autre) :// Appel distant en microservicesvar reponse = await _httpClient.GetAsync($\"http://service-paiement/api/verifierCarte/{numeroCarte}\");// gestion d'erreur...bool estPaiementValide = await reponse.Content.ReadFromJsonAsync&lt;bool&gt;();👆 Vous voyez la différence : on échange la simplicité d’un appel direct contre la complexité (gestion des erreurs réseau, sérialisation JSON, etc.) d’un appel distant. Cet exemple illustre bien le surcoût inhérent aux microservices : ce qui était un détail trivial en monolithe (appeler une fonction) devient une mini-aventure technique quand on découpe tout en services indépendants.Cela ne veut pas dire que monolithes = bien et microservices = mal (ou vice-versa). Chacun a ses avantages et inconvénients. Un monolithe, c’est simple à développer et à débugger (tout est au même endroit) et déployer se résume à lancer une “seule application”. En revanche, un monolithe peut devenir lourd à faire évoluer quand il grossit trop : la moindre modification nécessite de redéployer l’ensemble, et on ne peut pas passer à l’échelle un composant spécifique indépendamment des autres. À l’opposé, les microservices apportent de la flexibilité : chaque service est plus petit, modulaire, déployable indépendamment, potentiellement scalable séparément. Mais ils introduisent une tonne de complexité distribuée : appels réseau, gestion de la cohérence des données entre services, multiplication des projets, monitoring plus ardu… bref, ce n’est pas la panacée universelle non plus.Maintenant que le décor est planté, entrons dans le vif du sujet : dans quels cas concrets les microservices sont-ils pertinents, et quand risquent-ils de vous causer plus de soucis qu’autre chose ? Pour le savoir, suivez le guide en 6 étapes.Méthodologie : Faut-il partir sur une architecture microservices ?Avant de “découper à la scie mécanique” votre application en microservices, passez en revue les étapes suivantes. Elles forment une checklist pour évaluer si le jeu en vaut la chandelle dans votre contexte spécifique.Étape 1 : Évaluer la taille et complexité du projetPremier réflexe : prenez du recul et regardez l’ampleur du projet. Quelle est la taille de votre application et la richesse de son domaine métier ? S’agit-il d’un petit site web ou d’un outil interne avec trois formulaires, ou bien d’une plateforme tentaculaire à la Netflix/Amazon avec des dizaines de fonctionnalités métiers distinctes ? Si votre projet est modeste ou en phase de démarrage, partir d’emblée sur des microservices serait comme vouloir désosser un mulot avec un scalpel de chirurgien. 🐭⚡ En clair, c’est overkill. Il est souvent recommandé de débuter par un monolithe bien structuré, quitte à le faire évoluer plus tard si nécessaire. Martin Fowler note d’ailleurs que presque toutes les success stories de microservices ont commencé par un monolithe qui a grossi avant d’être découpé, tandis que les rares projets démarrés directement en microservices ont souvent accumulé les ennuis. Le monolithe initial permet de valider rapidement que l’application répond à un besoin, sans s’alourdir d’une complexité prématurée (principe YAGNI : You Ain’t Gonna Need It). Il vaut mieux un petit système qui marche qu’une usine à gaz microservicielle pour un produit incertain. En revanche, si vous anticipez que votre application va devenir très large, très complexe, avec de multiples sous-domaines métier clairement identifiables, là un découpage pourra se justifier à terme. Par exemple, une plateforme de commerce électronique internationale a des sous-domaines évidents (catalogue produits, gestion des commandes, facturation, recherche, recommandations, etc.) qui pourraient devenir chacun un service. Mais attention : même dans ce cas, rien ne presse de tout micro-découper dès le jour 1. Il est souvent plus sage de commencer monolithique, puis de refactorer en microservices une fois que les frontières naturelles entre composants se sont clarifiées dans le temps. Un mauvais découpage précoce peut faire plus de mal que de bien. En résumé, taille modeste = monolithe favorisé, grande échelle potentielle = microservices envisagés, mais idéalement après avoir atteint les limites du monolithe. Comme le dit de dicton : “n’optimisez pas prématurément”. Visez la simplicité d’abord, la sophistication ensuite, seulement si nécessaire.Étape 2 : Considérer la taille de l’équipe et l’organisationDeuxième facteur clé : qui va développer et maintenir tout ça ? La taille et la structure de votre équipe influencent énormément le choix d’architecture : Si vous êtes une toute petite équipe (ou un développeur solo), lancer 10 microservices serait un peu comme un agriculteur qui court après 10 vaches échappées dans des champs différents. ⚠️ Spoiler : y’en a toujours une qui finit chez le voisin 😅. Vous risquez de vous épuiser rapidement … Un monolithe est bien plus adapté aux petites équipes : tout le monde travaille sur le même code, c’est plus facile à suivre et à tester. N’oublions pas que chaque microservice additionnel, c’est du fardeau opérationnel en plus (pipelines CI/CD multiples, déploiements multiples, versions multiples…). Quand on n’a que 2-3 développeurs, mieux vaut les concentrer sur une base de code unique que de les disperser. À l’inverse, si vous disposez de plusieurs équipes dédiées, avec chacune son périmètre fonctionnel, les microservices peuvent aider à découpler le travail. C’est là qu’intervient la fameuse règle d’Amazon des “two-pizza teams”. Jeff Bezos a instauré que chaque équipe doit être suffisamment petite pour être nourrie avec deux pizzas. En pratique, chaque équipe chez Amazon est propriétaire d’un service et peut le faire évoluer à son rythme. Cette organisation en microservices a permis à Amazon de scaler tant au niveau technique qu’humain : des équipes autonomes, livrant indépendamment, sans se marcher sur les pieds. On retrouve une idée similaire chez Uber : quand l’entreprise est passée de quelques dizaines à des centaines de développeurs, le monolithe d’origine est devenu un goulot d’étranglement, car toutes les équipes étaient couplées par ce code unique. Le passage à une multitude de services a permis à chaque groupe d’avancer plus librement, sans attendre que “le monolithe veuille bien déployer”. En gros, conformez l’architecture à votre organisation (c’est la fameuse Conway’s Law). Si vous avez déjà des équipes ou des domaines bien séparés, les microservices peuvent refléter ce découpage naturel. Si votre équipe est un bloc unique, imposer des microservices crée artificiellement des frontières… et potentiellement des silos injustifiés. Un autre aspect humain : les compétences. Une petite équipe full-stack “classique” sera plus à l’aise à travailler sur un seul projet monolithique. Tandis que dans une grande organisation, on peut avoir des équipes spécialisées (ex: une team pour chaque microservice, avec éventuellement des technos différentes). Pas d’équipe dédiée = pas de microservice dédié, c’est un bon réflexe à avoir.Petite anecdote : Amazon n’est pas dogmatique non plus. Récemment, leur division Prime Video a opéré un mouvement surprise en abandonnant une architecture microservices/serverless au profit d’un bon gros monolithe… Résultat : des coûts réduits et de bien meilleures performances pour leur workload! Preuve que même avec des centaines de développeurs, la solution “plein de microservices” n’est pas toujours la plus efficiente – cela dépend du contexte et du problème à résoudre.Étape 3 : Identifier les besoins de scalabilité et de performancePassons au côté technique : quelle charge votre application doit-elle encaisser et comment veut-on la faire monter en charge (scaling) ? Les microservices sont souvent vendus comme LE remède anti-surcharge, mais la réalité est nuancée. Si votre application doit gérer des volumes massifs de trafic ou de données, et surtout de façon inégale selon les fonctionnalités, alors une architecture microservices est pertinente. Elle permet de scaler indépendamment chaque service selon la demande. Par exemple, imaginons un jeu en ligne où le module “classement des points” est ultra-sollicité, bien plus que le module “profil utilisateur”. En microservices, on pourrait déployer 10 instances du service Classement pour chaque instance du service Profil, afin d’absorber la charge là où c’est nécessaire. De grandes entreprises ont adopté ce principe : Netflix a scindé sa plateforme en plus de 700 microservices pour gérer chaque partie du système de manière autonome, après avoir souffert des limites d’un monolithe. En 2008, un incident célèbre a vu la base de données centrale de Netflix corrompue, plongeant tout le service dans le noir pendant trois jours. Cette panne a mis en lumière le point faible du monolithe : un seul composant qui flanche peut tout faire tomber. Netflix a alors migré vers AWS et une architecture microservices, éliminant les points uniques de défaillance et permettant de faire évoluer séparément chaque brique (lecture de vidéos, recommandations, facturation, etc.). Depuis, l’infrastructure Netflix repose sur une multitude de petits services robustes, capables de servir des millions d’utilisateurs et de déployer des changements en continu – parfois des milliers de déploiements par jour ! De même, Amazon (encore eux) a besoin que certains pans de son site puissent encaisser des pics énormes (pensez au Black Friday sur le panier d’achat ou les paiements). Leur architecture microservices permet de renforcer uniquement les services critiques en pic de charge, sans toucher aux autres. C’est comme pouvoir ajouter des voies sur l’autoroute là où il y a des embouteillages, sans avoir à refaire toutes les routes du pays. En revanche, si votre application n’a pas le besoin d’une scalabilité granulaire, un monolithe bien conçu peut suffire largement. Beaucoup d’applications métier “classiques” tournent très bien avec un déploiement monolithique dupliqué sur quelques serveurs en load-balancing pour gérer la charge. Même un gros monolithe peut scaler horizontalement en déployant plusieurs instances identiques derrière un répartiteur de charge. On sous-estime parfois jusqu’où un monolithe peut aller : vous pouvez déjà bâtir un commerce qui tourne rondement avec une seule base de données et une appclication web bien optimisée sur un serveur aux ressources généreuses. Si une partie de votre application commence à saturer les ressources, il est tout à fait possible de l’optimiser ou de monter en vertical (plus de CPU, de RAM) avant d’envisager un découpage. Par ailleurs, microservices n’égale pas automatiquement meilleures performances. En fait, chaque appel distant ajoute de la latence et de la charge (sérialisation/désérialisation, routage réseau…). Donc, pour de fortes exigences de performance en temps réel, trop de microservices peuvent nuire. Il faut un certain seuil de complexité/échelle pour que les bénéfices surpassent les coûts. L’anecdote d’Amazon Prime Video l’illustre bien : ils avaient découpé un workflow en fonctions serverless (microservices), pensant gagner en scaling, mais ont atteint des limites à seulement 5% de charge prévue. Leur solution a été de re-fusionner les composants critiques en un seul service optimisé et bingo, ça a bien mieux tenu la charge. Morale : si vos besoins de scalabilité peuvent être satisfaits par un bon vieux monolithe optimisé, inutile d’ajouter de la complexité sans justification concrèt. En résumé, posez-vous les questions suivantes : Est-ce que certaines parties du système ont des profils de charge très différents des autres ? Dois-je pouvoir scaler un module sans toucher aux autres ? Ai-je des contraintes de disponibilité très fines qui nécessitent d’isoler les pannes potentielles ? Si oui, orientez-vous vers une segmentation en services. Sinon, un monolithe peut très bien faire le boulot, plus simplement.Étape 4 : Examiner la fréquence de déploiement et l’indépendance des fonctionnalitésUn avantage souvent mis en avant des microservices, c’est de permettre des déploiements indépendants et fréquents de chaque composant. Voyons si c’est pertinent pour vous : Votre équipe déploie-t-elle très souvent de nouvelles fonctionnalités, de manière découplée ? Par exemple, si le module “Facturation” doit être mis en production 3 fois par semaine, alors que le reste du système bouge peu, l’architecture microservices vous permettrait de déployer le service Facturation seul, sans interrompre ni revalider toute l’application. Idem, s’il y a une équipe dédiée qui ne travaille que sur le moteur de recherche du site, elle pourrait livrer ses évolutions indépendamment des autres. Ce scénario plaide en faveur de microservices, pour gagner en vitesse de livraison. Les géants du web en profitent : Amazon a des déploiements en continu de microservices tout au long de la journée, sans quoi il leur serait impossible de faire évoluer leur plateforme tentaculaire sans tout casser. Chaque équipe publie son service quand elle est prête, point. Cela a considérablement accéléré l’innovation et le time-to-market. Dépendances modulaires : Si vos fonctionnalités sont relativement indépendantes les unes des autres sur le plan métier, les microservices évitent que déployer A implique de retester B, C et D qui n’ont rien à voir. C’est un atout pour la qualité et l’agilité. Par exemple, chez Netflix, les équipes peuvent modifier le service de recommandation de films et le déployer, sans devoir geler le service de lecture vidéo ou le service d’abonnement. Dans un monolithe, une petite modification dans le code de recommandations nécessiterait de reconstruire et redéployer tout le monolithe, avec les risques que cela comporte. À l’inverse, si vous déployez plutôt rarement et que vos releases englobent de toute façon toutes les parties de l’application en même temps (train de livraison coordonné), le bénéfice de microservices diminue. Beaucoup de systèmes internes d’entreprise suivent des cycles de mise en production globales (par ex. une release mensuelle de l’application entière). Le fait d’être en microservices ne changerait pas grand-chose, puisque vous attendriez quand même d’avoir tout packagé pour déployer. Voire pire : cela pourrait compliquer la synchronisation (s’assurer que les 10 services sont tous alignés pour la release, avec les bonnes versions d’API, etc.). Dans ce genre de contexte, un monolithe peut simplifier la livraison. Posez-vous la question de la coordination des versions : si chaque microservice évolue indépendamment, il faut gérer la compatibilité entre eux. C’est jouable (contrats d’API stables, versions rétrocompatibles…), mais c’est du travail de plus. Si votre équipe a déjà du mal à coordonner du versioning au sein d’un monolithe, la multiplication des services ne va pas arranger les choses par magie, au contraire. En somme, les microservices prennent tout leur sens si vous visez un modèle DevOps/CI-CD très poussé, avec des livraisons continues par composant. Vous pourrez publier plus vite, en isolant les risques. Si ce niveau de fréquence n’est ni nécessaire ni réaliste pour vous, ne vous infligez pas la complexité d’une architecture distribuée. Parfois, déployer calmement un bon gros monolithe une fois toutes les deux semaines, ça suffit amplement pour rendre les utilisateurs heureux.Étape 5 : Vérifier les capacités techniques (DevOps, monitoring, etc.)C’est un aspect souvent sous-estimé : avez-vous l’infrastructure et les outils pour supporter une galaxie de microservices ? Et l’expertise qui va avec ?Mettre en place des microservices, ce n’est pas juste découper du code en petits morceaux. Il faut aussi tout un écosystème technique pour les faire tourner correctement : Conteneurs, orchestrateurs : En général, qui dit microservices dit Docker, Kubernetes, ou autre plateforme orchestrée. Déployer manuellement 20 services sur des VM, c’est ingérable, vous aurez besoin d’automatisation. Votre équipe est-elle familière avec ces technologies ? A-t-elle quelqu’un qui maîtrise Kubernetes (AKS, EKS, peu importe) pour opérer l’infrastructure ? Si ce n’est pas le cas, prévoyez une solide courbe d’apprentissage et du temps en “Recherche et Développement”, sinon vous allez vous retrouver à héberger vos microservices “à la main”, ce qui est aventureux 😅. Monitoring &amp; logging : Un monolithe a souvent une seule source de logs, facile à parcourir. Avec 10 microservices, bonjour la chasse aux logs dans 10 endroits différents. Il vous faudra mettre en place une solution de logs centralisés (ELK, Application Insights, etc.) et du monitoring distribué. Idem pour le tracing des requêtes entre services (systèmes de traçage distribué de type OpenTelemetry). Sans ces outils, vous serez aveugle lorsque quelque chose plantera à 3h du matin dans un enchaînement de 5 services. Il faut donc investir du temps à instrumenter chaque service, à se doter de dashboards, d’alertes… Est-ce que votre équipe a les compétences pour ça ? Et le temps ? Si non, c’est un signal fort que le passage aux microservices pourrait être prématuré. Robustesse et tolérance aux pannes : Dans un système distribué, tout peut arriver : un service peut tomber, un appel réseau peut échouer ou expirer, etc. Vos développeurs doivent adopter des pratiques de code résilient (patterns de retry, circuit-breaker – avec Polly en .NET par exemple). Ils doivent penser “et si le service en face ne répond pas ?” en permanence. C’est un état d’esprit et une expertise différente de la programmation monolithique où un NullReferenceException est souvent le pire scénario. Ici on parle de gérer des time out, des files d’attente, des messages perdus… Si l’équipe n’a jamais fait ça, il y aura des ratés. Ce n’est pas insurmontable, mais c’est un coût de formation et d’expérience à anticiper. Netflix, par exemple, a dû inventer l’outil Chaos Monkey qui éteint aléatoirement des services en production pour s’assurer que le système global survit aux pannes individuelles – c’est dire le niveau de maturité à atteindre pour dompter un tel écosystème ! Complexité de debug : Préparez-vous à ce que votre débogage “F5 dans Visual Studio” se transforme en parties de cache-cache dans une ferme de services. Un développeur racontait, un brin désabusé, comment ses microservices passaient leur temps à “échanger entre eux sur le réseau”, et que dès qu’un service “faisait un caprice”, bonne chance pour identifier la source du problème. Ce qui était un simple appel de fonction devient une succession de requêtes asynchrones à travers le réseau, avec des erreurs possiblement silencieuses en chemin. Debugger ça sans outillage, c’est comme chercher une aiguille dans un champ de foin… la nuit… avec une lampe frontale déchargée. 🔦😅 Autrement dit, assurez-vous d’avoir les outils et les compétences de débogage distribué, sinon vos nuits pourraient devenir très courtes. En résumé, une architecture microservices n’est viable que si votre stack technique et votre équipe sont prêtes à en assumer les à-côtés. Posez-vous sincèrement la question : Mon équipe est-elle suffisamment DevOps ? Avons-nous les ressources pour mettre en place : CI/CD, conteneurs, monitoring, tests end-to-end sur un système distribué ? Si la réponse est “nahhhhhh”, il vaut peut-être mieux consolider vos pratiques sur un monolithe d’abord. Rien n’empêche d’adopter certaines bonnes pratiques devops (intégration continue, conteneurisation) sur le monolithe en prévision, mais ne pas se jeter dans le grand bain “multi-services” avant de savoir nager.Étape 6 : Peser les coûts et la valeur ajoutéeDernier point, et non des moindres : le coût et le retour sur investissement. Les microservices ont un coût technique et organisationnel. Il faut s’assurer que les bénéfices espérés valent cette dépense. Coût de développement : Diviser une application en 10 services, c’est potentiellement 10 dépôts git, 10 pipelines de build, 10 projets à tester et maintenir. La productivité peut en prendre un coup. Un architecte rapportait que dans son entreprise, développer une fonctionnalité équivalente prenait 8 fois plus de temps avec une architecture microservices qu’avec un monolithe, et nécessitait deux fois plus de développeurs! Pourquoi ? Parce qu’il faut définir des contrats d’API, gérer les versions, écrire du code de communication, synchroniser les déploiements… tout cela, un monolithe nous l’épargne en grande partie. Alors oui, 8x c’est un cas extrême, mais ne soyez pas surpris si au début vos livraisons ralentissent en passant aux microservices. C’est un investissement long terme : on accepte de ralentir maintenant en espérant aller plus vite plus tard, quand l’application sera très grosse et que les équipes paralléliseront vraiment le travail. Pour un petit projet, cet investissement ne sera jamais rentabilisé. Coût opérationnel : Plus de services = plus de ressources serveurs (mémoire, CPU, instances multiples). Si chaque microservice a sa base de données séparée (idéalement oui), ça fait autant de serveurs de BD à gérer/payer/licencier. Sur le cloud, multiplier les conteneurs ou fonctions serverless peut faire grimper la facture à la fin du mois. Là où un monolithe tournait sur 2 VM à 80% CPU, vos 10 microservices tournent peut-être sur 20 conteneurs globalement sous-utilisés… mais facturés quand même. Avez-vous le budget pour ce overhead ? Parfois, le jeu en vaut la chandelle (si vos microservices attirent 100× plus d’utilisateurs, le coût supplémentaire est justifié). Parfois non : on a vu des startups flamber leur budget infonuagique en orchestrant une flotte de microservices qui auraient pu tenir dans un seul petit serveur. Complexité = risques : La complexité additionnelle peut engendrer plus de pannes ou de bugs subtils. Chaque service ajouté est un point potentiel de défaillance en plus (une panne réseau, un plantage de service, ou un échec de synchronisation des données…). Et quand ça plante, le temps passé à résoudre (MTTR) risque d’être plus long qu’avec un monolithe. Ce coût-là est dur à chiffrer, mais bien réel (heures supplémentaires, interruptions de service, image de marque, etc.). Pour le minimiser, il faut investir dans la fiabilité (tests, monitoring, redondance) ce qui renchérit encore le coût technique initial. ROI métier : demandez-vous quels problèmes concrets les microservices vont résoudre pour votre business. Si la réponse est du genre “c’est à la mode” ou “on le fait pour faire propre”, ce n’est pas un vrai ROI 😅. En revanche, si vous identifiez clairement que “ça nous permettra de déployer plus vite des features pour nos clients” ou “ça éliminera les indisponibilités totales en cas de bug” ou “on pourra tenir la charge pendant les soldes sans planter”, là on parle. Quantifiez autant que possible : par exemple, passer de 1 déploiement par mois à 10 déploiements par jour grâce aux microservices a une valeur énorme si votre marché exige cette réactivité. Mais si votre application interne n’a pas besoin d’évoluer si fréquemment, ce gain est inutile. En somme, faites le bilan coût/bénéfice le plus objectivement possible. Les microservices apportent des bénéfices indéniables (échelle, résilience, agilité) mais au prix d’une complexité qui, dans bien des cas, n’est tout simplement pas justifiée. Il n’y a pas de honte à rester sur un bon monolithe efficace si c’est la solution la plus rentable pour vous ! D’ailleurs, on voit un certain retour de balancier dans l’industrie : après l’engouement débridé pour “tout microserviciser”, certaines entreprises reviennent à des architectures plus simples pour réduire les coûts et la complexité, quitte à sacrifier un peu de hype. Le tout, c’est d’y gagner au final.Conclusion : Trouver le juste milieu (et dormir sur ses deux oreilles)La grande leçon à retenir ? “Microservice” n’est pas synonyme de “mieux” par défaut. C’est un outil architectural parmi d’autres, avec ses cas d’usage idéaux et ses pièges. De même, le monolithe n’est pas un dinosaure ringard bon pour le musée : il demeure tout à fait pertinent dans bon nombre de situations.Pour récapituler avec un petit guide quand ou ne pas utiliser une architecture microservices : ✅ Oui aux microservices dans des contextes tels que : Une application très complexe, couvrant plusieurs domaines fonctionnels clairement séparables, et/ou une grande organisation avec des équipes dédiées par domaine. Ex: Un site web de type Amazon avec panier, paiement, recommandations, chacun évoluant séparément par des équipes différentes. Des besoins de scalabilité fine : certaines parties du système doivent pouvoir monter en charge indépendamment (ex: moteur de recherche à scaler sans scaler toute l’application). Une nécessité de déploiements fréquents et indépendants de certaines fonctionnalités, pour livrer plus vite aux utilisateurs. Une exigence de haute résilience : on veut qu’une panne d’un composant n’impacte pas tout le reste (architecture tolérante aux pannes). Un environnement technique mature en DevOps, avec outils en place (CI/CD, container orchestration, monitoring distribué) et une équipe à l’aise avec ces concepts. En somme, quand le gain attendu (autonomie, vitesse, robustesse, échelle) dépasse le coût en complexité. 🚫 Non (ou pas encore) aux microservices dans les cas suivants : Projet trop petit ou jeune : si vous pouvez développer l’ensemble de votre application en quelques mois avec 2-3 devs, un microservice ne va que ralentir la livraison. Partez monolithique, vous verrez plus tard si ça devient trop gros. Équipe réduite/inexpérimentée en devops : pas d’expertise conteneurs/Cloud, pas d’équipe dédiée à l’infrastructure… Mieux vaut ne pas se compliquer la vie tout de suite. Comme on dit, “il ne faut pas plusieurs chefs pour faire bouillir la marmite” – une petite brigade de développeurs fonctionne mieux autour d’un seul chaudron (le monolithe). Domaines très interconnectés : si vos fonctionnalités sont toutes fortement liées, les séparer en services entraînera beaucoup d’appels réseau entre eux (le syndrome du distributed monolith). Vous n’y gagnerez rien, si ce n’est d’échanger un couplage interne contre un couplage réseau tout aussi contraignant. Contraintes de performance temps réel strictes : une application low-latency (ex : transactions boursières automatisées ultra-rapides, calcul scientifique synchronisé) peut difficilement tolérer la latence ajoutée des microservices. Un monolithe optimisé en C# (ou en Rust natif 😉) sera plus efficace. Budget serré : si chaque dollar compte, sachez que la complexité microservices peut impliquer plus de dépenses (machines, temps de développeur, consultants spécialisés…). Assurez-vous que votre responsable budgétaire soit d’accord avant de multiplier les déploiements comme des lapins. En bref, si les bénéfices ne sont pas clairs et immédiats, ou si votre contexte n’est pas prêt, il est parfaitement raisonnable de rester sur un monolithe ou d’adopter une approche intermédiaire (par ex. un monolithe modulaire, bien découpé en couches ou en modules internes – parfois appelé “modulithe” ou “monolithique modulable”). On peut très bien concevoir son code comme des microservices (avec de bonnes séparations logiques), tout en déployant une seule application. C’est souvent un excellent compromis pour démarrer. En guise de clôture, rappelons-nous que l’architecture sert les objectifs du logiciel, pas l’inverse. Ne choisissez pas microservices ou monolithe pour suivre la mode, mais en fonction de ce qui apporte le plus de valeur à votre produit et vos utilisateurs. La prochaine fois qu’on vous dira “Il faut absolument des microservices, c’est plus scalable et hype“, vous pourrez rétorquer : “Peut-être, mais encore faut-il que ce soit justifié – parlons concret ! “.Au fond, monolithes et microservices cohabitent dans la grande boîte à outils de l’architecte. Le vrai talent est de sortir le bon outil au bon moment. Ce n’est pas parce qu’on a vu des fermes industrielles avec des robots à traire qu’il faut transformer sa petite étable en usine à microservices. Et à l’inverse, pour bâtir une cathédrale logicielle fréquentée par des millions de personnes, un seul bloc monolithique pourrait devenir un carcan rigide – un échafaudage microservices offre alors plus de souplesse.En définitive, soyez pragmatique : commencez simple, ayez conscience des trade-offs, et évoluez l’architecture quand les signaux le demandent (goulots d’étranglement, blocage organisationnel, etc.). Si vous suivez les étapes et conseils ci-dessus, vous devriez éviter aussi bien le “microservice washing” inconsidéré que le “monolith forever par inertie”.Et surtout, n’oubliez pas de savourer le voyage : que vous construisiez un majestueux monolithe ou une myriade de microservices, l’essentiel est d’apprendre en s’amusant. Après tout, on écrit du code pour résoudre des problèmes et pour le plaisir intellectuel. Alors faites les bons choix au bon moment, et laissez l’architecture servir votre projet, pas l’inverse." }, { "title": "Architecture Vertical Slice dans l’écosystème .NET", "url": "/posts/vertical-slice-architecture/", "categories": "architecture", "tags": "", "date": "2025-06-29 20:00:00 -0400", "snippet": "L’architecture Vertical Slice (ou architecture en tranches verticales) est une approche de conception logicielle qui consiste à organiser le code par fonctionnalités verticales plutôt que par couch...", "content": "L’architecture Vertical Slice (ou architecture en tranches verticales) est une approche de conception logicielle qui consiste à organiser le code par fonctionnalités verticales plutôt que par couches techniques. Autrement dit, chaque fonctionnalité de l’application est implémentée de bout en bout dans une tranche (un slice) comprenant tous les aspects nécessaires : interface utilisateur, logique métier et accès aux données. Chaque tranche verticale est autonome et correspond à un cas d’utilisation distinct, ce qui favorise un développement modulaire centré sur les fonctionnalités. En .NET, cette approche s’accompagne souvent du patron CQRS (Command Query Responsibility Segregation) : on sépare les opérations de lecture (Query) de celles d’écriture (Command) et on les traite différemment. La bibliothèque MediatR est fréquemment utilisée pour mettre en œuvre ce style – elle agit comme un médiateur qui envoie chaque commande ou requête vers son gestionnaire (handler) dédié, permettant de découpler l’émetteur de la requête de son traitement.Avantages de l’architecture Vertical Slice Lisibilité et cohésion par fonctionnalité : Le code de chaque fonctionnalité est regroupé au même endroit, ce qui le rend plus facile à comprendre et à maintenir qu’une base de code où les éléments d’une même fonctionnalité sont dispersés entre plusieurs couches. Cette forte cohésion interne à la tranche améliore la lisibilité : on peut se concentrer sur un use case à la fois sans être noyé dans la complexité globale. Découplage des fonctionnalités : Chaque tranche est indépendante des autres, ce qui réduit le couplage du système. Modifier ou ajouter une fonctionnalité impacte peu (voire pas du tout) les autres modules, minimisant les effets de bord. On obtient ainsi un système plus robuste face aux changements, chaque slice pouvant évoluer isolément. Facilité de test : Puisque la logique de chaque use case est isolée, on peut tester une tranche verticalement sans dépendre du reste de l’application. Par exemple, il est simple d’écrire des tests unitaires pour un handler donné en simulant ses dépendances (bases de données, services externes, etc.), ce qui améliore la testabilité globale. Développement parallèle accéléré : Les équipes peuvent travailler simultanément sur différentes fonctionnalités sans se marcher sur les pieds, étant donné que chaque slice est autonome. Ceci réduit également les risques de conflits de merge et facilite l’intégration du code produit par plusieurs développeurs en parallèle. En grande équipe, segmenter le travail par vertical slices permet d’accélérer le développement en divisant le système en sous-problèmes indépendants. Ajout de fonctionnalités simplifié : L’architecture Vertical Slice tend à rendre l’application extensible par addition plutôt que par modification. L’ajout d’une nouvelle fonctionnalité se traduit par la création d’une nouvelle tranche (nouvelle requête, nouveau handler, etc.) sans modifier du code existant, ce qui limite le risque de régression. Cette propriété s’aligne bien avec le principe Open/Closed – le code en place est fermé aux modifications intempestives, tandis que le système est ouvert à de nouvelles extensions fonctionnelles. Flexibilité d’implémentation : Chaque tranche peut adopter l’approche technique la plus adaptée à son cas sans impacter le reste du projet. Par exemple, une fonctionnalité très simple pourra se contenter d’un traitement procédural direct (transaction script), tandis qu’une autre plus complexe pourrait utiliser des patterns de domaine riche – le choix peut se faire au niveau de la slice elle-même. De même, chaque tranche pourrait interagir avec des ressources externes différemment (base de données SQL, appel d’un service tiers, etc.) sans casser une “architecture globale” figée. Cette souplesse permet de personnaliser la solution par fonctionnalité et d’éviter les abstractions ou couches supplémentaires inutiles au sein d’une slice donnée.Inconvénients et limites de l’architecture Vertical Slice Duplication de code (anti-DRY) : À force d’isoler chaque fonctionnalité, on risque de répéter du code commun dans plusieurs slices. Par exemple, des composants d’infrastructure ou des routines similaires peuvent se retrouver dupliqués dans chaque module fonctionnel. Cette redondance va à l’encontre du principe DRY (Don’t Repeat Yourself) et peut alourdir la maintenance : une correction de bug ou un changement commun doit être répercuté à plusieurs endroits. En adoptant Vertical Slice, on accepte implicitement une certaine duplication en échange d’une meilleure isolation, mais il faut surveiller que cela ne devienne pas ingérable (d’où le commentaire humoristique « Adieu DRY ! » souvent évoqué sur le sujet). Fragmentation et vision d’ensemble réduite : Un risque de cette approche est de morceler l’application en de nombreux petits silos de code. Si la modularité est poussée à l’extrême sans garde-fou, on peut perdre la vue d’ensemble du système. Chaque équipe ou développeur pouvant structurer “sa” tranche à sa manière, l’architecture globale peut manquer de cohérence. En l’absence de conventions communes, le code peut diverger d’une tranche à l’autre, rendant la maintenance transversale plus difficile. En outre, il devient délicat de comprendre le flux applicatif global puisque la logique est éclatée en morceaux indépendants. Il est donc crucial de définir des standards (structuration interne des slices, conventions de nommage, etc.) pour conserver une certaine homogénéité. Complexité de gestion des interactions : L’architecture Vertical Slice fonctionne mieux lorsque les fonctionnalités sont bien découplées. Si, en pratique, vos cas d’utilisation ont de fortes interactions ou partagent des processus communs, la gestion peut devenir compliquée. Par exemple, une transaction métier impliquant plusieurs slices distinctes s’avère difficile à orchestrer proprement avec ce modèle. On devra alors multiplier les contournements (appels de slice A vers slice B, utilisation de librairies tierces pour orchestrer du workflow, etc.), ce qui peut complexifier le code et le rendre inmaintenable. En d’autres termes, Vertical Slice n’est pas pensée pour des fonctionnalités étroitement liées ou fortement interdépendantes, et forcer ce modèle dans de tels cas peut conduire à une usine à gaz. Performances et logique transversale : Dans la même veine, si une opération utilisateur nécessite de coordonner plusieurs tranches verticales, l’application peut souffrir d’une certaine surcouche. Par exemple, devoir appeler successivement plusieurs handlers MediatR pour réaliser une action complexe peut introduire du surcoût (sérialisation/désérialisation de messages, I/O multiples, etc.). Des scénarios transactionnels touchant plusieurs slices seront difficiles à optimiser. Certes, dans la plupart des applications ce surcoût restera négligeable, mais sur un système à très hautes performances ou très complexe, c’est un point à considérer. L’architecture Vertical Slice est surtout plébiscitée pour des projets modulaires plutôt que pour de grosses transactions multi-domaines – lorsqu’on tente de l’appliquer à des contextes qu’elle gère mal (transactions distribuées, règles globales), on se heurte à ses limites.En résumé, Vertical Slice améliore la lisibilité locale et la séparation des préoccupations par fonctionnalité, au prix potentiel de duplications et d’une fragmentation du codebase. Ces inconvénients ne sont pas rédhibitoires, mais requièrent une vigilance et possiblement la mise en place de patterns complémentaires (par exemple un service commun pour factoriser du code partagé si nécessaire, ou un cadre d’architecture global minimal pour guider les devs).Quand utiliser l’architecture Vertical Slice ?Comme tout choix architectural, l’approche Vertical Slice est plus ou moins adaptée selon le contexte. Voici des situations où son usage est recommandé, et d’autres où il l’est moins :Scénarios recommandés Applications modulaires ou microservices : Si votre application peut être découpée en modules indépendants (par exemple par domaine métier ou bounded context), l’architecture Vertical Slice s’aligne naturellement. Chaque module ou microservice peut correspondre à un ensemble de slices cohérents. On parle d’ailleurs souvent de modular monolith pour désigner un monolithe structuré en tranches verticales, qui offre une maintenabilité et une compréhension aisée du code. Dans ces architectures modulaires, chaque slice (ou ensemble de slices) représente un composant du système, facilement isolable pour le développement, le déploiement ou les tests. APIs REST simples ou projets de petite envergure : Pour une petite application, un MVP ou un service avec des cas d’utilisation simples, adopter d’emblée une architecture hexagonale ou Clean complète peut être surdimensionné. Dans ces cas, l’approche Vertical Slice apporte de la simplicité. On évite de créer pléthore de couches et d’abstractions alors que ce n’est pas nécessaire pour un petit projet. En d’autres termes, si vos besoins sont limités et bien circonscrits, « bricoler quelque chose rapidement » en séparant par fonctionnalités est tout à fait raisonnable, et formaliser une couche de domaine complète serait une perte de temps et d’énergie. Vous irez plus vite tout en gardant une bonne organisation par features. Développement de nouvelles fonctionnalités isolées : Même au sein d’un projet existant, vous pouvez utiliser Vertical Slice pour des modules très spécifiques ou autonomes. Par exemple, l’ajout d’une petite API indépendante dans un grand système peut se faire sous forme de slice verticale sans impacter l’architecture globale. De manière générale, si une fonctionnalité n’a quasiment pas de dépendance sur le reste du système, la développer comme une unité verticale permet de la livrer rapidement et de façon propre. Équipes multiples sur des domaines différents : Si vous avez plusieurs équipes ou développeurs qui travaillent en parallèle sur des sous-domaines différents, leur attribuer des vertical slices différentes peut réduire la collision de leurs travaux. Chaque équipe peut évoluer dans “son” périmètre avec moins de risques de modifier du code utilisé par les autres. Dans un contexte Agile, cela facilite la livraison continue de features indépendantes. (Il faut toutefois, comme mentionné, veiller à garder une cohérence globale via des guidelines communes.)Scénarios moins adaptés Grand monolithe avec logique métier partagée : Si votre application forme un monolithe très complexe avec de nombreuses règles métier transverses, une architecture strictement Vertical Slice risque de montrer ses limites. Par exemple, dans un système bancaire monolithique, des règles de gestion comme “un client ne peut pas dépasser tel découvert” s’appliquent à plusieurs fonctionnalités. Les implémenter séparément dans chaque tranche conduirait à de la duplication et possiblement des incohérences. Dans ce genre de contexte, il est souvent préférable d’unifier la logique métier centrale dans un modèle commun (par exexemple, via une couche Domaine partagée, comme le propose Clean Architecture). L’architecture “propre” et dérivées (Hexagonale, etc.) sont le fruit de décennies d’expérience pour bâtir des solutions complexes de grande taille, que ce soit en monolithe ou en microservices. Elles apportent un cadre rigoureux pour garantir la cohérence de la logique métier à travers tout le système. À l’inverse, une approche Vertical Slice pure serait hasardeuse ici, car chaque slice devrait réimplémenter ou appeler ces règles communes, augmentant les risques d’erreur. Fonctionnalités fortement couplées entre elles : Si vos cas d’utilisation interagissent étroitement, qu’ils doivent se coordonner ou partager beaucoup de données, une séparation stricte par slices peut être artificielle. Par exemple, une opération complexe pourrait nécessiter l’enchaînement de plusieurs slices (ce qui revient à simuler une transaction répartie). Ce genre d’opération est pénible à réaliser proprement sans couche de service ou orchestrateur commun. Une architecture classique en couches permettrait ici de centraliser cette orchestration dans un service métier, là où Vertical Slice ne fournit pas de réponse évidente. En bref, plus vos fonctionnalités sont interdépendantes, moins l’architecture en tranches verticales est indiquée. Besoins de réutilisation du code métier : Dans certains projets, on cherche à construire une bibliothèque ou un noyau de composants réutilisables (par exemple, un moteur de calcul utilisé par plusieurs applications différentes). Dans ce cas, isoler complètement chaque feature n’est pas souhaitable : on veut au contraire factoriser le cœur commun. Vertical Slice, qui cloisonne le code par use case, n’est pas orientée vers la mutualisation du code. Si l’un de vos objectifs est de réutiliser une partie significative de la logique dans d’autres contextes ou d’exposer un modèle de domaine cohérent, une architecture du style Clean/DDD sera plus adaptée. Equipe débutante ou hétérogène sans guidelines : Enfin, il faut noter qu’une équipe peu expérimentée ou sans discipline pourrait dériver avec une architecture Vertical Slice. Parce qu’elle impose moins de structure prédéfinie qu’une architecture en couches, chaque développeur pourrait être tenté d’organiser « à sa sauce » sa slice. Sans concertation, le code risque de manquer d’uniformité et la maintenance deviendra difficile. Pour une équipe junior, une architecture plus classique (Clean, couches MVC, etc.) sert parfois de filet de sécurité grâce à son cadre clair. Cela ne disqualifie pas Vertical Slice, mais souligne l’importance d’un leadership technique pour guider sa mise en place dans un contexte d’équipe moins expérimentée.En somme, l’architecture Vertical Slice brille dans les contextes modulaires, évolutifs et indépendants, ou lorsque l’on veut aller vite sur des fonctionnalités ciblées. En revanche, dès qu’une forte mutualisation ou des invariants globaux sont nécessaires, il faut soit l’adapter (en combinant avec d’autres approches), soit envisager une architecture différente plus appropriée.Comparaison avec le Clean ArchitectureIl est fréquent d’opposer Vertical Slice à le Clean Architecture (architecture “propre” d’après Robert C. Martin), car ces deux approches structurent le code très différemment. En réalité, elles poursuivent des objectifs communs (modularité, maintenabilité) mais via des principes distincts. Comparons-les selon quelques axes clés :Principes fondamentauxLe Clean Architecture (ainsi que les architectures couches classiques, hexagonale, oignon, etc.) s’articule autour de la séparation des préoccupations horizontale et du respect de la dépendance inverse. Elle vise à rendre le cœur du logiciel indépendant des détails d’implémentation. Par exemple, la logique métier ne doit dépendre ni d’un framework particulier, ni de la base de données, ni de l’UI. Concrètement, on retrouve dans Clean Architecture des couches bien définies (Entités du domaine, Cas d’usage, Interfaces d’accès aux données, etc.), avec la règle que les dépendances pointent vers l’intérieur du cercle (vers le domaine). L’accent est mis sur la stabilité du modèle métier : il s’agit de protéger les règles de gestion des changements technologiques ou des caprices de l’interface.L’architecture Vertical Slice, de son côté, est guidée par la cohérence fonctionnelle. Son principe central est de grouper le code par fonctionnalité métier de manière autonome. Chaque slice traite une et une seule fonctionnalité et embarque tout ce qu’il faut pour la réaliser. L’idée sous-jacente est d’obtenir une forte cohésion interne (tout le code lié à un cas d’utilisation est assemblé) et une faible dépendance externe (peu de liens avec d’autres parties du système). On couple “verticalement” le long d’un flux fonctionnel, plutôt qu’horizontalement par type de composant. En adoptant Vertical Slice, on accepte un certain cloisonnement par feature, au bénéfice d’une flexibilité locale et d’une simplicité de compréhension par cas d’usage.En résumé, Clean Architecture met l’emphase sur le domaine et les abstractions stables, alors que Vertical Slice met l’emphase sur les features et leur isolation mutuelle. Clean cherche à éliminer le couplage technique (UI, DB, frameworks) vis-à-vis du métier, tandis que Vertical Slice cherche à éliminer le couplage fonctionnel entre les différentes parties du système.Organisation du code et du projetSi on regarde l’organisation concrète d’un projet .NET dans chaque approche, la différence saute aux yeux. Clean Architecture propose typiquement de structurer la solution en couches ou couches concentriques. On peut avoir par exemple des projets ou folders séparés pour : le domaine (entités et interfaces), les cas d’application (use cases ou services applicatifs), l’interface (contrôleurs API, UI) et l’infrastructure (implémentations des dépôts, accès BD, services externes). Chaque couche a une responsabilité spécifique et des dépendances restreintes (p.ex. l’infrastructure dépend du domaine pour implémenter ses interfaces, mais l’inverse n’est pas vrai). Cette organisation par couches apporte de la clarté sur le rôle de chaque classe, mais peut introduire de la verbosité (de nombreux projets et fichiers pour une seule feature). Il faut naviguer entre plusieurs dossiers pour suivre le fil d’une fonctionnalité donnée.Vertical Slice, à l’opposé, organise le code par regroupement vertical. On va créer un dossier (ou un namespace) par fonctionnalité ou par module métier, et y placer tous les éléments liés : contrôleur API ou endpoint correspondant, classes de commande/requête, handler MédiatR, modèles spécifiques, etc.. Par exemple, au lieu d’avoir un dossier Controllers avec tous les contrôleurs de l’application, on aura un dossier Features contenant des sous-dossiers par fonctionnalité : Produit, Commande, Client, etc., et à l’intérieur de chacun, les fichiers pour créer un produit, mettre à jour un produit, etc. Chaque slice peut ainsi avoir ses propres modèles ou services internes s’il en a besoin, sans impacter les autres slices. Cette organisation par feature facilite la localisation du code – pour toucher à une fonctionnalité, on sait exactement où aller – mais rend plus difficile la réutilisation d’une classe d’une slice à l’autre (puisqu’a priori on évite de le faire). En pratique, il est courant qu’un projet Vertical Slice n’ait qu’un ou deux assemblages (ex: un projet Web et éventuellement un projet pour les contrats), là où une Clean Architecture en comporte plusieurs pour séparer les couches. Cela réduit la complexité initiale (moins de projets .NET à configurer), au prix d’une séparation moins nette entre logique métier et détails techniques.Couplage et dépendancesLa Clean Architecture excelle à réduire le couplage entre le domaine et les détails d’implémentation. Grâce à l’inversion des dépendances, le cœur métier ne “connaît” pas la base de données, ni le framework web utilisé. Cela permet, par exemple, de changer de base de données ou de technologie d’UI sans toucher au domaine (en théorie tout au moins). Le couplage est ainsi contrôlé et dirigé : les couches haut-niveau dépendent de couches bas-niveau abstraites (interfaces), jamais l’inverse. En revanche, les fonctionnalités dans Clean Architecture sont couplées via le domaine commun. Par exemple, deux use cases différents vont manipuler la même classe Produit ou Commande. Cela garantit une logique uniforme, mais signifie aussi que ces use cases sont indirectement reliés : toute modification dans la structure d’une entité du domaine peut impacter de multiples fonctionnalités. Le couplage fonctionnel n’est pas éliminé, il est centralisé dans le domaine.L’approche Vertical Slice, de son côté, cherche à minimiser le couplage entre fonctionnalités au profit d’une forte cohésion interne à chaque fonctionnalité. Idéalement, chaque slice a ses propres entités ou modèles et n’interagit pas directement avec les autres slices. Le couplage technique (ex: appel base de données) n’est pas nécessairement inversé comme en Clean Architecture – une tranche peut très bien appeler directement un ORM ou une requête SQL si c’est le plus simple – mais ce choix n’affecte que cette tranche précise. En réduisant la portée d’influence du code, Vertical Slice fait pencher la balance vers une multitude de petits couplages locaux plutôt qu’un grand couplage central. Le risque est bien sûr d’introduire du couplage dupliqué (plusieurs slices dépendant des mêmes concepts implémentés en parallèle), d’où l’importance de bien délimiter les frontières. On peut résumer en disant que Clean Architecture vise un faible couplage “vertical” (technique), tandis que Vertical Slice vise un faible couplage “horizontal” (fonctionnel). Les deux ne sont pas incompatibles, mais l’accent n’est pas mis au même endroit.Scalabilité du code et de l’architectureLorsqu’on parle de scalabilité ici, on entend la capacité de l’architecture à supporter la croissance de l’application (plus de fonctionnalités, plus de développeurs, plus de complexité) tout en restant maintenable.Du côté de Clean Architecture, la structure en couches apporte une rigueur appréciable pour les projets de grande envergure. C’est une architecture éprouvée pour organiser des solutions complexes avec de nombreuses règles métier et de gros équipes. En séparant nettement le domaine, les cas d’utilisation et les détails techniques, on peut faire évoluer chacun de ces aspects indépendamment. Clean Architecture favorise la factorisation du code commun, ce qui évite l’effet boule de neige lors de changements globaux. Par exemple, si une règle métier impacte plusieurs cas d’utilisation, on la codera une fois dans le domaine (ou un service commun) plutôt que de devoir la répliquer. Ainsi, quand la complexité augmente, l’architecture en couches offre un cadre qui encaisse la montée en volume (avec cependant le compromis d’une complexité interne plus élevée et d’une courbe d’apprentissage pour les nouveaux développeurs). On note aussi que Clean Architecture améliore la maintenabilité et l’évolutivité du système sur le long terme, car les préoccupations sont bien isolées – ce qui peut justifier son investissement initial dans des projets appelés à grossir.Du côté de Vertical Slice, la scalabilité se joue différemment. Ajouter de nouvelles fonctionnalités est très simple (comme évoqué, on ajoute du code neuf sans impacter l’existant), ce qui donne une impression de facilité pour faire croître le périmètre fonctionnel. De plus, plusieurs équipes peuvent contribuer en parallèle sur des slices différentes, ce qui scale bien humainement (moins de collisions de code). Beaucoup de projets web ou API croissent initialement plus vite avec ce modèle qu’avec une architecture hexagonale plus formelle. Cependant, plus l’application grossit, plus on accumule de slices – et les risques mentionnés de duplication ou d’incohérence peuvent augmenter exponentiellement. Sans discipline, une grosse base de code en Vertical Slices peut devenir difficile à maintenir si chaque tranche a implémenté sa version de la réalité. En outre, la gestion des fonctionnalités transverses (par ex. logging commun, transactions multi-slices, règles de validation globales) peut devenir ardue. En pratique, Vertical Slice peut tout à fait s’appliquer à de grands systèmes, mais souvent en combinaison avec d’autres patterns pour encadrer la complexité. Par exemple, on peut très bien imaginer un grand monolithe modulable où chaque module est un ensemble de vertical slices, tout en conservant un schéma de base commun et des infrastructures partagées. D’ailleurs, les architectes recommandent souvent de ne pas opposer frontalement Clean Architecture et Vertical Slice, mais d’envisager d’associer les deux dans un même projet. Par exemple, on peut structurer son code en vertical slices au sein de chaque couche d’un modèle hexagonal (features folders + interfaces de repo, etc.), ou utiliser Vertical Slice pour la partie application tandis que le domaine reste centralisé. Ces deux approches ne sont pas mutuellement exclusives et peuvent se compléter pour obtenir un système à la fois modulaire et consistant.En synthèse, Clean Architecture offre une colonne vertébrale solide pour les systèmes complexes et de long terme, tandis que Vertical Slice apporte de la légèreté et de la flexibilité pour le développement orienté fonctionnalité.Le tableau ci-dessous résume quelques différences :Voici une version restructurée, plus fluide et symétrique pour chaque point : Principe directeur :Clean Architecture repose sur une organisation par couches avec des dépendances stables orientées vers le domaine. Vertical Slice se structure par fonctionnalités, avec une forte cohésion autour de chaque cas d’usage. Organisation du code :Clean Architecture répartit le code en couches séparées (domaine, application, infrastructure). Vertical Slice regroupe tout le code nécessaire à une fonctionnalité dans un même ensemble (commande, handler, modèles, etc.). Couplage :Clean Architecture vise un découplage fort vis-à-vis des détails techniques, mais centralise les fonctionnalités autour d’un modèle commun. Vertical Slice réduit le couplage entre fonctionnalités, en favorisant une autonomie locale, au prix potentiel d’une duplication. Échelle et complexité :Clean Architecture convient bien aux applications complexes ou d’envergure, avec une structure rigide mais robuste à long terme. Vertical Slice est plus adapté aux projets modulaires ou aux fonctionnalités isolées, avec un démarrage rapide mais qui demande un encadrement au fil de la croissance.Il n’y a pas de « gagnant » absolu : le choix dépend du contexte de votre projet. L’important est de comprendre ces différences pour appliquer le bon dosage. La section suivante illustre concrètement la structure Vertical Slice dans un projet .NET pour mieux ancrer ces concepts.Exemple de projet Vertical Slice en .NET (C#)Pour rendre les choses plus concrètes, prenons un exemple d’application .NET utilisant l’architecture Vertical Slice. Imaginons une API pour gérer des commandes (Orders) dans un système e-commerce. Nous allons voir comment structurer le code par fonctionnalités, et comment s’articule l’utilisation de MediatR, des Commands et Queries.Structure par fonctionnalitésSupposons que notre API offre deux opérations principales pour les commandes : Créer une nouvelle commande (Create Order) et Obtenir les détails d’une commande (Get Order Details). En Vertical Slice, on va créer deux tranches séparées pour ces deux cas d’utilisation. La structure du projet pourrait ressembler à ceci :📦 MonProjet.API└── Features └── Orders ├── CreateOrder │ ├── CreateOrderCommand.cs // Commande pour créer une commande │ └── CreateOrderHandler.cs // Handler pour traiter la création └── GetOrderDetails ├── GetOrderDetailsQuery.cs // Requête pour obtenir les détails └── GetOrderDetailsHandler.cs // Handler pour traiter la requêteDans ce schéma, tout ce qui concerne “Create Order” vit dans son propre dossier (sous Orders/CreateOrder), et pareil pour “Get Order Details”. On pourrait également avoir des sous-dossiers par agrégat ou entité principale (ici Orders regroupe les features liées aux commandes). L’idée est qu’en ouvrant le dossier d’une fonctionnalité, on retrouve toutes les pièces du puzzle pour cette fonctionnalité, plutôt que de devoir chercher dans un dossier Controllers, puis un dossier Services, puis Repository, etc.Command, Query et Handler avec MediatRVoyons maintenant à quoi ressemblent les classes à l’intérieur d’une slice. Nous allons créer un Query pour la lecture des détails d’une commande, et une Command pour la création d’une commande. Nous utiliserons les interfaces de MediatR (IRequest&lt;T&gt; et IRequestHandler&lt;TRequest, TResponse&gt;) pour définir nos requêtes/commandes et leurs gestionnaires.Exemple : Lire les détails d’une commande (Query)// DTO (Data Transfer Object) représentant le résultat renvoyé au clientpublic record OrderDto(int OrderId, string ProductName, decimal TotalAmount);// Requête de lecture pour obtenir les détails d'une commande spécifiquepublic record GetOrderDetailsQuery(int OrderId) : IRequest&lt;OrderDto&gt;;// Handler associé à la requête GetOrderDetailsQuerypublic class GetOrderDetailsHandler : IRequestHandler&lt;GetOrderDetailsQuery, OrderDto&gt;{ private readonly IOrderRepository _repository; public GetOrderDetailsHandler(IOrderRepository repository) { _repository = repository; } public async Task&lt;OrderDto&gt; Handle(GetOrderDetailsQuery query, CancellationToken cancellationToken) { // Récupérer la commande depuis la base de données (ou autre source) var order = await _repository.GetById(query.OrderId); if (order is null) { return null; // ou lever une exception NotFound, selon les besoins } // Mapper les données de la commande vers le DTO de sortie return new OrderDto(order.Id, order.ProductName, order.TotalAmount); }}Dans cet extrait, GetOrderDetailsQuery est une simple classe (ici un record) qui porte les paramètres nécessaires (l’Id de la commande). Elle implémente IRequest&lt;OrderDto&gt; indiquant qu’elle attend une réponse de type OrderDto. Le GetOrderDetailsHandler contient la logique pour traiter la requête : typiquement, il va chercher la commande dans un dépôt (IOrderRepository) et convertir le résultat en DTO. Grâce à MediatR, ce handler sera automatiquement appelé lorsque nous enverrons un objet GetOrderDetailsQuery via le médiateur.Exemple : Créer une nouvelle commande (Command)// Commande pour créer une nouvelle commandepublic record CreateOrderCommand(int ProductId, int Quantity) : IRequest&lt;OrderDto&gt;;// Handler associé à la commande CreateOrderCommandpublic class CreateOrderHandler : IRequestHandler&lt;CreateOrderCommand, OrderDto&gt;{ private readonly IOrderRepository _repository; // on peut imaginer qu'on utilise un dépôt ou un service de domaine public CreateOrderHandler(IOrderRepository repository) { _repository = repository; } public async Task&lt;OrderDto&gt; Handle(CreateOrderCommand command, CancellationToken cancellationToken) { // Logique métier simplifiée: créer l'entité Order et l'enregistrer var newOrder = new Order { ProductId = command.ProductId, Quantity = command.Quantity, // ... (autres initialisations, calcul du total etc.) }; await _repository.Add(newOrder); // Retourner un DTO représentant la commande créée return new OrderDto(newOrder.Id, newOrder.ProductName, newOrder.TotalAmount); }}Ici, CreateOrderCommand encapsule les informations nécessaires pour créer une commande (par ex. identifiant du produit, quantité). Le CreateOrderHandler effectue la création : dans une vraie application, il pourrait appeler des règles de domaine (vérifier le stock, calculer le montant, etc.) et utiliser le dépôt pour sauvegarder la commande en base. À la fin, il retourne un OrderDto avec les détails de la nouvelle commande. Remarquez que toute la logique spécifique à “créer une commande” est confinée dans ce handler – si demain une règle change (par ex. limiter la quantité maximale), on viendra l’implémenter ici sans impacter d’autres features.Intégration dans un contrôleur ou un endpoint : Grâce à MediatR, nos controllers restent très simples. Par exemple, un contrôleur Web API pour créer une commande pourrait ressembler à :[ApiController][Route(\"api/[controller]\")]public class OrdersController : ControllerBase{ private readonly IMediator _mediator; public OrdersController(IMediator mediator) { _mediator = mediator; } [HttpPost] public async Task&lt;IActionResult&gt; CreateOrder([FromBody] CreateOrderCommand command) { OrderDto result = await _mediator.Send(command); return CreatedAtAction(\"GetOrder\", new { id = result.OrderId }, result); } [HttpGet(\"{id}\")] public async Task&lt;IActionResult&gt; GetOrder(int id) { OrderDto resultat = await _mediator.Send(new GetOrderDetailsQuery(id)); return resultat is null ? NotFound() : Ok(result); }}On voit que le contrôleur ne contient quasiment aucune logique : il se contente de transmettre la commande/requête au médiateur (_mediator.Send(...)) et de retourner la réponse appropriée (ici un code 201 Created ou 200 OK). Toute la “vraie” logique est implémentée dans nos handlers, ce qui correspond bien à l’esprit Vertical Slice : chaque requête HTTP correspond à un use case géré par un handler dédié. Cette structure rend les controllers minces et faciles à maintenir, et déplace le poids de la logique dans les slices où elle est plus facile à tester.Points à noter dans cet exemple On a un fichier par requête/commande et un par handler, mais on aurait pu regrouper la commande et son handler dans le même fichier (c’est une question de style). L’important est que la séparation est faite par cas d’utilisation et non par type d’objet. Les classes OrderDto, CreateOrderCommand, GetOrderDetailsQuery, etc., sont spécifiques à leur slice. Aucune d’entre elles n’est utilisée en dehors de la fonctionnalité en question. Cela garantit que modifier l’une n’impactera pas d’autres parties du système par effet de bord inattendu. Chaque handler pourrait avoir ses propres validations (par exemple, vérifier que Quantity est positive dans CreateOrderHandler). On pourrait utiliser des Behavior MediatR ou des filtres, mais souvent chaque slice gère ses validations soit inline, soit via des composants dédiés (par exemple un FluentValidation validator par commande). L’essentiel est que, là encore, la validation métier d’un use case vit à côté de ce use case. On peut tout à fait utiliser des patterns de conception à l’intérieur d’un vertical slice. Par exemple, si la logique de création de commande devenait très complexe, on pourrait introduire une couche de domaine (entité Order avec des méthodes, service de domaine, etc.) au sein de cette slice. Vertical Slice n’interdit pas d’écrire du code propre ! Il dit juste : « ne le rends pas partagé d’office si ce n’est pas nécessaire ». Vous pouvez donc combiner Vertical Slice et principes DDD/Clean à un niveau fin, en créant une mini-architecture hexagonale interne à une tranche si le besoin s’en fait sentir.Réflexion et questions ouvertesNous avons exploré ce qu’est l’architecture Vertical Slice, ses atouts et ses faiblesses, ainsi que son positionnement vis-à-vis de l’architecture Clean. Pour conclure cet article, il convient d’insister qu’il n’existe pas de solution universelle – le choix dépend de votre contexte spécifique. Voici quelques questions ouvertes pour alimenter votre réflexion et vous aider à évaluer la pertinence de Vertical Slice pour votre projet : Vos fonctionnalités sont-elles réellement indépendantes ? Partagez-vous beaucoup de règles métier et de données entre différentes parties de l’application, ou bien pouvez-vous facilement isoler des slices sans créer de doublons massifs ? Si votre domaine est très connecté, une architecture par couches pourrait mieux convenir. Si au contraire vous pouvez dessiner des frontières nettes, Vertical Slice peut apporter de la clarté. Quelle complexité anticipez-vous à long terme ? S’il s’agit d’un petit service ou d’un module simple, Vertical Slice vous fera gagner du temps et de la souplesse. En revanche, pour un produit stratégique qui va évoluer sur des années avec de nombreuses fonctionnalités, envisagez-vous que l’absence d’un modèle central puisse devenir un frein (duplication, incohérences) ? Faut-il prévoir une combinaison d’approches pour grandir sereinement ? Quelle importance accordez-vous à la réutilisation et aux abstractions ? Préférez-vous du code dupliqué mais simple et lisible, ou une factorisation poussée quitte à introduire des couches d’abstraction supplémentaires ? La première approche favorise Vertical Slice, la seconde s’aligne plus avec Clean/DDD. En fonction de votre priorité (rapidité de développement vs. rationalisation du code), l’une ou l’autre approche prendra l’avantage. Votre équipe et votre organisation sont-elles prêtes ? Considérez le niveau d’expérience de vos développeurs et la structure de votre équipe. Sont-ils à l’aise pour évoluer sans le filet d’une architecture standard ? Vont-ils suivre des conventions communes pour ne pas que chaque slice devienne un microcosme isolé ? Avez-vous les outils pour documenter et surveiller la cohérence de l’ensemble ? L’architecture Vertical Slice demande une certaine discipline dans un contexte d’équipe pour éviter l’effet « tour de Babel » où chacun code dans son coin. À l’inverse, une équipe responsable et bien synchronisée pourra prospérer grâce à la liberté qu’elle offre.En répondant à ces questions, vous serez en mesure de peser le pour et le contre de l’architecture Vertical Slice dans votre cas particulier. N’hésitez pas à expérimenter à petite échelle, voire à combiner des approches (comme structurer votre code en slices tout en conservant un noyau de domaine commun pour les invariants critiques). L’important est de choisir une architecture qui sert au mieux les besoins de votre projet et de votre équipe – que ce soit Vertical Slice, Clean Architecture, une combinaison des deux, ou toute autre variante architecturale. Après tout, une architecture n’est réussie que si elle vous permet de livrer un logiciel de qualité, maintenable et évolutif, dans les délais et avec le sourire de l’équipe 😊.👉 Pour approfondir la réflexion et découvrir un point de vue éclairant sur le sujet, je vous recommande vivement la vidéo de Milan Jovanović." }, { "title": "Changements de licences dans l’écosystème .NET – quelles implications ?", "url": "/posts/changements-licences-ecosysteme-dotnet/", "categories": "outil-developpement", "tags": "", "date": "2025-06-15 19:00:00 -0400", "snippet": "Introduction : une vague de changements inattendueL’écosystème .NET a récemment été secoué par plusieurs changements de licence touchant plusieurs bibliothèques couramment adoptées par les développ...", "content": "Introduction : une vague de changements inattendueL’écosystème .NET a récemment été secoué par plusieurs changements de licence touchant plusieurs bibliothèques couramment adoptées par les développeurs. Des composants autrefois entièrement open source adoptent désormais des modèles commerciaux ou des licences restrictives. Du moteur de cache Redis au framework de tests FluentAssertions, en passant par la bibliothèque de mapping AutoMapper, le médiateur MediatR ou encore le bus de messages MassTransit, ces évolutions soulèvent des questions. Pourquoi ces projets changent-ils de licence, quels impacts pour les développeurs et architectes, et comment la communauté réagit-elle ? Cet article propose un tour d’horizon de ces changements, avec un ton à la fois informatif et éditorial, pour inviter à la réflexion sur notre utilisation des dépendances externes.Redis : de l’open source à RSAL, puis retour aux sourcesDepuis son lancement, Redis était publié sous une licence permissive BSD. Cependant, Redis Labs (l’entreprise derrière Redis) a introduit en 2024 une nouvelle licence pour certains modules complémentaires de Redis, appelée Redis Source Available License (RSAL). Concrètement, seuls des modules comme RedisSearch ou RedisJSON sont concernés, pas le cœur de Redis qui reste sous BSD. La RSAL permet de consulter le code source mais restreint l’utilisation commerciale en mode SaaS : il est toujours gratuit de les utiliser en interne ou de les intégrer dans un produit déployé sur site, mais proposer un service cloud basé sur ces modules nécessite désormais une licence commerciale. L’objectif affiché était de protéger Redis des géants du cloud (Amazon Web Services, notamment) qui proposaient Redis en service managé sans contribuer au projet.Cette décision a eu des répercussions contrastées. Pour les développeurs utilisant Redis dans des projets open source ou des applications internes, le changement était mineur. En revanche, pour les entreprises offrant Redis dans leurs solutions cloud, cela impliquait de nouvelles obligations contractuelles et possiblement des coûts supplémentaires. Certaines startups ont dû examiner de près leur conformité vis-à-vis de la RSAL pour éviter tout risque juridique. À plus large échelle, on anticipait des coûts accrus pour les fournisseurs de services Redis-as-a-Service, et l’on a vu émerger l’idée de remplacer les modules concernés par des alternatives open source ou développées maison. D’autres acteurs du cloud, comme AWS, ont envisagé de se tourner vers des forks de Redis ou des solutions alternatives propriétaires, témoignant de l’impact stratégique de ce changement de licence.La communauté open source, de son côté, a très mal accueilli ce virage. Le passage d’une licence BSD à un modèle « source disponible » a été perçu comme une atteinte aux principes du logiciel libre. Dès lors, des forks communautaires ont vu le jour – par exemple le projet Valkey soutenu par la Linux Foundation, visant à fournir une version de Redis pleinement open source en réaction à la RSAL. Face à ces tensions et à un écho largement négatif, Redis Labs a finalement fait marche arrière en 2025 : Redis 8 est “redevenu” open source, sous licence AGPLv3 (une licence approuvée par l’OSI). Ce choix de l’AGPLv3 garantit que Redis reste un logiciel libre tout en imposant, ironiquement, des contraintes similaires aux objectifs initiaux – l’AGPL oblige toute offre SaaS basée sur Redis 8 à publier son propre code source modifié, ce qui dissuade l’exploitation sans contribution. Le retour de Salvatore Sanfilippo (alias @antirez, créateur de Redis) dans l’équipe en novembre 2024 a d’ailleurs coïncidé avec ce changement d’orientation. Il s’est réjoui publiquement de voir Redis renouer avec ses racines open source : « Je suis heureux que Redis soit à nouveau un logiciel open source, sous les termes de la licence AGPLv3 ». La communauté, initialement froissée, s’est dite rassurée par ce geste d’ouverture et de réconciliation après la période de turbulence. Redis 8 apporte en outre de nouvelles fonctionnalités et optimisations notables, preuve que l’innovation continue tout en revenant à un modèle plus ouvert.Fluent Assertions : une bibliothèque de tests qui devient payanteAutre changement majeur dans l’écosystème .NET : Fluent Assertions, l’une des bibliothèques les plus populaires pour l’écriture d’assertions fluides en tests unitaires, a basculé vers un modèle commercial. Depuis la version 8.0.0, sortie fin 2024, Fluent Assertions n’est plus pleinement open source : il est désormais distribué sous une licence commerciale, ce qui signifie que les utilisateurs commerciaux doivent acquérir une licence pour l’utiliser au-delà de la version 7. En d’autres termes, la version 7.0.0 (et antérieures) restait sous licence ouverte (Apache 2.0/MIT auparavant), mais toute mise à jour ultérieure nécessite un achat pour un usage en entreprise. Ce tournant a fait l’effet d’une petite bombe dans la communauté des testeurs .NET, habitués depuis des années à la gratuité de cet outil essentiel.La réaction de la communauté a été mitigée. Beaucoup de développeurs ont exprimé leur déception et leur désir de conserver une solution open source pour leurs tests. Très rapidement, la réponse communautaire s’est organisée : un projet alternatif nommé AwesomeAssertions a été créé, sous licence Apache 2.0, visant à reproduire les fonctionnalités phares de Fluent Assertions tout en restant libre. Ce fork communautaire illustre la volonté d’une partie des développeurs de s’affranchir d’une contrainte commerciale imposée a posteriori. Par ailleurs, pour ceux qui souhaitaient continuer à utiliser Fluent Assertions sans frais, une solution immédiate a consisté à geler la version de la bibliothèque à la dernière édition gratuite. En fixant par exemple la dépendance NuGet à la version 7.0.0 dans le fichier projet, on s’assure de ne pas passer involontairement à une version ultérieure payante. De nombreux projets ont adopté cette stratégie de rester sur la dernière version open source connue, évitant ainsi toute entorse légale.Ce changement soudain de licence a également alimenté un débat plus large. D’un côté, il pose la question de la soutenabilité des projets open source : les mainteneurs de Fluent Assertions ont sans doute estimé qu’un modèle payant était nécessaire pour continuer à faire évoluer la bibliothèque. De l’autre, certains membres de la communauté ont critiqué la manière de faire, qualifiant ce mouvement de “brutal”. Des contributeurs bénévoles se sont interrogés sur le fait que leur code, apporté sous une licence ouverte, se retrouve du jour au lendemain dans un produit commercial – une situation inconfortable et source de ressentiment. Fluent Assertions n’est pas un cas isolé dans l’historique .NET (on se souvient d’IdentityServer4 devenu payant en son temps, ou plus récemment de la controverse autour de Moq et SponsorLink), mais l’ampleur de son utilisation a rendu ce changement particulièrement sensible. Il a en tout cas servi de catalyseur, incitant développeurs et architectes à reconsidérer la confiance aveugle mise dans les dépendances externes gratuites.AutoMapper et MediatR : le choix d’une double licence équilibréeTrois mois après Fluent Assertions, l’actualité des licences a de nouveau fait les gros titres dans la communauté .NET. AutoMapper (outil de mapping objet/objet très répandu) et MediatR (implémentation populaire du patron médiateur) vont à leur tour emprunter la voie de la monétisation. Leur auteur principal, Jimmy Bogard, a annoncé début 2025 sa décision de prendre un « virage commercial » afin d’assurer le succès à long terme de ces projets. Toutefois, contrairement à un passage pur et simple à une licence payante exclusive, Bogard opte pour un modèle de double licence (« dual license »). Cette approche vise à concilier les deux mondes – open source et commercial – en fonction des profils d’utilisateurs.Concrètement, AutoMapper et MediatR resteront gratuits pour un grand nombre d’usages. Jimmy Bogard a exprimé sa volonté de conserver la gratuité pour : les développeurs et projets purement open source, les individus, étudiants et hobbyistes qui utilisent ces outils à des fins non lucratives, les organismes à but non lucratif et les associations, les startups ou petites entreprises en dessous d’un certain seuil de revenus ou de financement, même les usages en environnement non productif (par exemple, développement, test, CI).En somme, ceux qui « bricolent pour le fun ou l’intérêt général » ne devraient pas être pénalisés. En revanche, les entreprises à but lucratif qui exploitent AutoMapper et MediatR dans le cadre d’activités commerciales seront invitées à acquérir une licence payante. Ce sont principalement ces utilisateurs professionnels, tirant profit business de la bibliothèque, qui porteront le financement du projet. Bogard explique en filigrane une question simple : « qui devrait payer pour assurer la pérennité des outils open source dont tout le monde dépend ? » – et sa réponse est de ne pas faire payer les développeurs individuels, mais bien les entreprises qui en retirent de la valeur.Au moment de l’annonce, les détails précis (tarifs, modalités exactes des licences) ne sont pas encore finalisés. Jimmy Bogard souhaite proposer un modèle le plus simple et transparent possible, probablement sous forme de forfaits par entreprise plutôt qu’une tarification par utilisateur qui alourdirait la gestion. Il a insisté sur le fait que rien ne changera à court terme pour les utilisateurs actuels et qu’il communiquera en toute transparence sur l’évolution du modèle commercial. Cette démarche mesurée et expliquée a été plutôt bien reçue par la communauté, surtout en comparaison d’autres cas plus abrupts. Beaucoup de développeurs comprennent la réalité à laquelle il fait face : son travail sur ces bibliothèques n’était plus sponsorisé depuis qu’il est indépendant, et continuer bénévolement à un tel niveau de qualité et de support n’était plus viable. En choisissant une double licence non punitive et en respectant les utilisateurs open source, Jimmy Bogard offre un exemple de transition réfléchie qui, si elle se concrétise comme annoncé, pourrait bien servir de modèle positif. La communauté a salué sa transparence et son respect des utilisateurs historiques, soulignant que ce genre de changement – lorsqu’il est bien communiqué et justifié – peut être compris et accepté.MassTransit v9 : une transition encadrée vers le modèle commercialAutre pilier de l’écosystème .NET, MassTransit (framework de messaging distribué alternatif à NServiceBus) a lui aussi annoncé un tournant important. Prévue pour la fin 2025, la version MassTransit 9 sera publiée sous une licence commerciale et non plus open source. Jusqu’à la version 8 incluse, MassTransit était distribué gratuitement sous licence Apache 2.0, cumulant les contributions et les utilisateurs satisfaits. Pourquoi ce changement ? Les mainteneurs expliquent que MassTransit est devenu au fil des ans une infrastructure critique pour de nombreuses entreprises (finance, santé, logistique, etc.), avec plus de 30 packages NuGet formant un écosystème complet. Son succès a généré des besoins croissants – en termes de support, de corrections, de nouvelles fonctionnalités – qu’une équipe purement bénévole ne peut plus assumer facilement. Pour accélérer le développement et offrir le niveau de support attendu par les utilisateurs en entreprise, un financement pérenne est nécessaire. D’où la décision de passer en commercial, afin de pouvoir allouer des ressources à plein temps au projet et garantir un support de niveau enterprise.Les mainteneurs de MassTransit ont toutefois pris soin d’accompagner cette transition de manière responsable. Ils ont clairement annoncé que MassTransit v8 restera open source, maintenu sous sa licence actuelle (Apache 2.0). Le code existant ne disparaît donc pas du giron libre, et les projets qui utilisent v8 peuvent continuer à le faire sans frais. Mieux, l’équipe continuera à publier des correctifs de sécurité et bugs critiques sur v8 pendant une période de transition, pour ne pas léser les utilisateurs qui resteraient sur l’ancienne version. En parallèle, MassTransit v9 deviendra un produit commercial : toutes les nouveautés, améliorations de performance et fonctionnalités à venir seront réservées à cette version sous licence payante. Des plans de support adaptés à la taille des organisations seront proposés, y compris des tarifs pour les éditeurs de logiciels indépendants ou les consultants qui intègrent MassTransit dans des solutions pour leurs clients. En échange, les clients bénéficieront d’un support expert, de garanties de stabilité (SLA) et de l’assurance d’un développement actif soutenu financièrement.Le calendrier annoncé laisse le temps de s’adapter : une préversion de MassTransit 9 sera disponible Q3 2025 pour les early adopters, mais la sortie officielle sous licence commerciale n’interviendra qu’en Q1 2026. Jusqu’à fin 2026, la v8 continuera d’être maintenue, puis elle sera probablement figée une fois la transition terminée. Cette progression graduelle offre aux équipes utilisatrices plusieurs options stratégiques : rester sur v8 si les nouvelles fonctionnalités ne sont pas indispensables, préparer un budget pour passer à v9 lorsque nécessaire, ou évaluer d’autres solutions de messagerie open source avant l’échéance. Globalement, la communauté a accueilli cette annonce avec un mélange de compréhension et de résignation. Certes, personne ne se réjouit de voir un outil gratuit devenir payant, mais beaucoup reconnaissent le professionnalisme de la démarche MassTransit. La version open source n’est pas “sabotée”, elle reste disponible et fonctionnelle, et la communication proactive de l’équipe a permis d’anticiper le changement plutôt que de le subir. Des voix dans la communauté soulignent que MassTransit a donné l’exemple d’une transition transparente, avec versionnage clair (v8 vs v9) et respect des utilisateurs existants – une approche qui contraste avec d’autres projets au changement plus abrupt.Impacts potentiels sur les projets .NETPour les développeurs et architectes .NET, ces changements de licence ne sont pas qu’un sujet de discussion abstrait – ils peuvent avoir des implications concrètes sur les projets en cours ou à venir. Voici quelques impacts à considérer : Conformité légale : intégrer une dépendance dont la licence a changé sans s’y conformer peut mettre un projet en situation d’infraction. Il est impératif de suivre de près les licences de nos packages NuGet. Par exemple, continuer à mettre à jour Fluent Assertions vers la v8+ dans une application commerciale sans acheter la licence constituerait une violation explicite des nouvelles conditions. De même, utiliser Redis avec des modules RSAL dans un service SaaS sans accord commercial pourrait exposer à des poursuites. Certaines licences open source virales comme l’AGPL exigent aussi de distribuer son propre code source en cas d’utilisation réseau – un point à ne pas ignorer si l’on envisage Redis 8 AGPLv3 dans un produit propriétaire. En somme, les chefs de projet doivent intégrer la vérification de licence dans leur gouvernance (à l’instar de la vérification technique), pour éviter les mauvaises surprises juridiques. Coûts additionnels : l’impact financier est évidemment au premier plan des préoccupations. Des outils autrefois gratuits peuvent désormais engendrer des coûts non prévus dans le budget. Cela peut affecter la rentabilité d’un produit ou le coût de développement d’un projet. Par exemple, une entreprise qui faisait massivement usage de Fluent Assertions ou envisageait MassTransit v9 doit budgéter le prix des licences ou abonnements correspondants. Pour les solutions cloud, on l’a vu avec Redis, devoir payer une licence sur certaines fonctionnalités peut augmenter les frais d’exploitation. Cet aspect pousse d’ailleurs à évaluer le ROI (return on investment) de chaque dépendance : si un outil payant simplifie énormément le développement, son coût peut être justifié ; à l’inverse, pour un gain marginal, une alternative gratuite ou maison pourrait redevenir plus attractive. Stratégie technique et dépendances : ces changements invitent à revisiter nos choix techniques. En tant qu’architectes, il faut toujours avoir un plan B pour les composants clés. Désormais, plusieurs approches s’offrent aux équipes confrontées à une dépendance devenue payante : Rester sur la dernière version libre : c’est la solution court-terme adoptée par beaucoup suite au changement de Fluent Assertions (verrouiller la version 7.0.0 pour éviter la 8.0.0 commerciale). Cela permet de gagner du temps sans casser le build, mais c’est une solution figée qui n’apporte plus d’évolutions. Explorer des alternatives open source : souvent, l’écosystème propose d’autres bibliothèques au rôle similaire. Par exemple, Shouldly peut remplacer Fluent Assertions dans les tests, AwesomeAssertions a émergé pour reprendre le flambeau open source, et d’autres médiateurs ou mappeurs existent en lieu et place de MediatR/AutoMapper. Il faut cependant évaluer le coût d’apprentissage et de migration vers ces alternatives. Forker ou internaliser la maintenance : si la licence le permet (par exemple, rester indéfiniment sur une version Apache/MIT antérieure), une équipe pourrait décider de forker le projet et de maintenir son propre dérivé en interne. C’est ce qu’ont fait implicitement les initiateurs de AwesomeAssertions ou de Valkey. Cette voie offre une liberté totale, mais elle exige des ressources pour corriger bugs et ajouter des features soi-même – un engagement lourd que peu d’équipes peuvent se permettre sur le long terme. Passer au modèle payant : enfin, il ne faut pas écarter l’option de payer pour la qualité. Si un outil apporte une vraie valeur ajoutée et qu’aucune alternative n’égale son efficacité, souscrire une licence peut être le meilleur choix. Après tout, investir dans un composant logiciel n’est pas différent que payer pour un framework propriétaire ou un service cloud : c’est une décision à justifier par un bénéfice. Nombre de mainteneurs espèrent d’ailleurs que les entreprises comprendront qu’un logiciel libre a aussi un coût de développement, et qu’y contribuer financièrement via une licence revient à assurer sa pérennité. En un mot, chaque équipe doit désormais peser le pour et le contre : continuer gratuitement (avec d’éventuelles concessions techniques), ou payer pour le confort et le support. L’important est d’anticiper ces choix en amont, plutôt que de les subir dans l’urgence. Réactions de la communauté et débats émergentsCes changements de cap dans des projets emblématiques ont naturellement déclenché de vifs débats dans la communauté. Du côté des développeurs, on a entendu des craintes quant à un possible effet domino : « Et si demain d’autres bibliothèques indispensables faisaient de même ? » Il existe un véritable sentiment de “trahison” chez certains, qui parlent de bait-and-switch (gratuité initiale suivie d’un changement commercial inattendu). L’idée qu’un outil open source adopté massivement puisse changer de licence a posteriori est vécue comme un manquement à un « contrat social » implicite. Ce ressentiment a été particulièrement prononcé pour Fluent Assertions, jugé abrupt dans son annonce (peu de préavis, une licence perçue comme chère, etc.). De même, l’épisode Moq (où une bibliothèque de mocks avait ajouté subrepticement du code de collecte d’e-mails pour pousser au sponsoring) a laissé des traces, rendant les développeurs plus méfiants. En réaction à chaque annonce, les forums et réseaux sociaux se sont enflammés : faut-il continuer à faire confiance aux mainteneurs open source ? N’est-on pas en train de payer le prix d’une dépendance aveugle aux gratuits d’hier ?Face à ces inquiétudes, une partie de la communauté s’est retroussée les manches pour proposer des solutions alternatives. Nous avons déjà mentionné les forks libres comme AwesomeAssertions pour Fluent Assertions, ou Valkey pour Redis. Ces initiatives montrent l’attachement des développeurs aux valeurs open source. Cependant, elles posent aussi la question de leur viabilité à long terme : créer un fork est facile, le maintenir activement dans la durée l’est moins. D’autres projets, lorsqu’ils ont vu leur licence changer par le passé, ont incité les utilisateurs à rester sur l’ancienne version stable (par ex. Entity Framework Core quand il a évolué avec des changements majeurs, certains sont restés sur la v5 LTS). Bref, la communauté dispose de garde-fous, mais ceux-ci ont leurs limites techniques et humaines.Il ne faudrait pas non plus opposer systématiquement communauté et mainteneurs, car bien souvent ils partagent le même but : la pérennité du projet. Du point de vue des mainteneurs open source, la décision de commercialiser un produit n’est jamais anodine ni facile. Eux aussi font partie de la communauté et en subissent le jugement. Lorsqu’un projet atteint une popularité énorme (des millions de téléchargements NuGet) sans modèle économique, le mainteneur bénévole peut s’épuiser ou rencontrer des difficultés à concilier ce travail avec sa vie professionnelle. Jimmy Bogard l’a explicitement décrit en parlant de ses contributions qui ont chuté après la fin du sponsoring par son employeur. De même, l’équipe de MassTransit a clairement listé les demandes impossibles à satisfaire sans financement (support temps réel, nouvelles features, corrections rapides). Il y a donc une prise de conscience générale que les projets open source à fort impact nécessitent un soutien, d’une manière ou d’une autre.Le débat s’est ainsi déplacé vers les modalités de cette monétisation. Ce que la communauté reproche le plus, ce n’est pas tant qu’un mainteneur cherche à être rémunéré pour son travail (ce qui est légitime), mais la façon de le faire. Plusieurs facteurs ont été identifiés dans les réactions : Le manque de transparence ou de préavis : des changements soudains, annoncés après coup, passent très mal. À l’inverse, annoncer à l’avance (comme MassTransit l’a fait en prévenant pour v9 tout en gardant v8 libre) est vu d’un bon œil. L’ingratitude perçue : ignorer la contribution de la communauté peut générer du ressentiment. Par exemple, la licence commerciale de Fluent Assertions a soulevé des questions sur les PR et issues soumises par des bénévoles par le passé. Une solution parfois proposée est de “grandfatherer” les versions existantes (les laisser libres) et d’appliquer le commercial seulement aux nouvelles fonctionnalités majeures – ce que MassTransit a précisément fait. La justesse du modèle : toutes les licences payantes ne se valent pas. La communauté a salué certaines approches jugées équilibrées, comme la double licence d’ImageSharp (libre pour la plupart des usages, payante pour un usage commercial à grande échelle) ou la démarche de Jimmy Bogard qualifiée de non-punitive. En revanche, elle fustige les pratiques jugées abusives (ex: Moq insérant du code non consenti, ou une licence trop restrictive sans alternative). Le respect des utilisateurs existants : c’est un point-clé. Les projets qui ont su montrer du respect pour leur base d’utilisateurs ont atténué les critiques. L’exemple de MassTransit est souvent cité : laisser v8 open source et ne commercialiser que v9, c’est perçu comme une marque de respect envers ceux qui ont construit leurs systèmes sur l’outil. De même, Bogard qui dit vouloir éviter de faire payer les petites structures montre qu’il n’oublie pas d’où vient la popularité de ses libs.En résumé, la communauté .NET est en pleine discussion sur l’équilibre entre open source et viabilité économique. Ces débats, parfois passionnés, ont au moins le mérite de poser les bonnes questions (qu’on détaillera ci-dessous) et de pousser tout le monde à plus de clairvoyance. Mainteneurs comme utilisateurs peuvent y gagner : les premiers en apprenant à communiquer tôt et honnêtement, les seconds en devenant plus conscients de la valeur des outils qu’ils utilisent au quotidien.Les bonnes questions à se poser avant d’adopter un outil tiersÀ la lumière de ces événements, il devient crucial, pour tout responsable technique, de s’interroger en amont lorsqu’il ajoute une nouvelle dépendance à son projet. Voici quelques questions clés à considérer : Quelle est la licence de cet outil, et est-elle compatible avec mon usage ? Est-ce une licence permissive (MIT, BSD…) ou plus contraignante (LGPL, AGPL, licence commerciale, etc.) ? Mon projet est-il commercial ou open source, et la licence de l’outil est-elle en accord avec cela ? Par exemple, intégrer du code sous AGPL dans une application propriétaire distribuée à des clients poserait problème. Comprendre dès le départ les obligations légales (partage du code source, mention de copyright, interdiction d’un usage SaaS sans accord…) évite les mauvaises surprises ultérieures. Le projet est-il soutenu par une organisation ou un modèle économique pérenne ? S’agit-il d’un one-man-show développé sur le temps libre, d’un projet géré par une fondation, ou d’un produit sponsorisé par une entreprise ? Un projet sans soutien financier clair peut être amené à changer de licence pour survivre – on l’a vu récemment. Vérifiez si le mainteneur a mis en place un Patreon, GitHub Sponsors, ou si une société (par ex. Red Hat avec Entity Framework Core dans le passé, Redis Labs pour Redis, etc.) supporte le développement. Un indicateur : la présence d’une documentation sur la licence, d’une FAQ sur l’usage commercial, ou d’un historique de communication transparente. Sinon, restez attentifs aux annonces de version majeure qui pourraient changer la donne. Quelle est la criticité de cette dépendance dans mon architecture ? En d’autres termes, que se passerait-il si demain elle devenait payante ou n’était plus maintenue ? Si la réponse est « mon application serait paralysée », il faut prévoir un plan de secours. Cela peut signifier : garder un œil sur les alternatives existantes (même si on ne les utilise pas immédiatement), concevoir son code de manière à pouvoir changer de bibliothèque relativement aisément (abstraction, interfaces, etc.), ou contribuer soi-même à la maintenance du fork libre en cas de besoin. Évitez de bâtir un château sur une dépendance dont vous ignorez tout de la solidité financière ou communautaire. Serions-nous prêts à payer pour cet outil le moment venu ? Cette question peut sembler incongrue lorsqu’on ne jure que par l’open source gratuit, mais elle mérite d’être posée honnêtement. Si l’outil en question apporte une valeur énorme (gain de temps, fiabilité, performance) à votre produit, pourriez-vous justifier un coût annuel de licence dans votre budget ? Par exemple, MassTransit envisage ~4000 $ par an pour une PME – est-ce un investissement envisageable par rapport au coût de développement d’une solution maison équivalente ? Si la réponse est non, il peut être prudent de limiter la dépendance à cet outil dès le départ ou d’explorer des solutions alternatives moins risquées. Dans le cas contraire, prévoir un éventuel coût dans le business plan du projet logiciel montre une approche mature et évite de se retrouver bloqué. Comment puis-je contribuer à la pérennité des outils dont je dépends ? Adopter une dépendance n’est pas un acte anodin : cela crée une relation de fait avec son mainteneur. Une bonne question à se poser est : ai-je les moyens de soutenir ce projet d’une manière ou d’une autre ? Cela peut passer par du sponsoring financier (beaucoup de bibliothèques acceptent les dons ou les sponsors entreprises), par des contributions techniques (pull requests, participation à la documentation, signalement proactif des bugs), ou même par du mentorat (aider à répondre aux issues des autres utilisateurs). Un projet bien soutenu aura moins tendance à chercher un financement drastique. En contribuant, vous investissez dans la santé à long terme de l’outil – et donc dans la sécurité de votre propre projet. Cette question renverse un peu la perspective : plutôt que de consommer passivement l’open source, quelle part de responsabilité suis-je prêt à endosser pour qu’il reste viable ?L’importance de comprendre licences et modèles open sourceLes événements récents sont riches d’enseignements. D’abord, ils rappellent que « open source » n’est pas synonyme de gratuité inconditionnelle et éternelle. Il existe une myriade de licences open source avec des implications juridiques variées, et il est primordial pour les professionnels du logiciel de s’y intéresser de près. Entre un projet sous MIT qui autorise presque tout, un projet sous GPL/AGPL qui impose la redistribution du code dérivé, ou un projet « source available » qui interdit carrément certains usages commerciaux, les différences sont majeures. De même, un code disponible sur GitHub sans licence explicite n’est pas forcément libre de droits. Lire et comprendre la licence d’un composant devrait faire partie du travail de base de tout intégrateur logiciel.Ensuite, il faut avoir conscience que derrière chaque projet open source se pose la question du modèle économique. Si un outil est crucial pour vous, renseignez-vous sur comment il est maintenu. De nombreux mainteneurs explorent aujourd’hui des modèles de durabilité : du simple consulting autour du projet, en passant par l’open core (une base gratuite, des fonctionnalités avancées payantes), les offres hébergées (SaaS), la double licence open source/commercial, etc.. Aucun modèle n’est intrinsèquement mauvais – c’est même sain que des auteurs puissent vivre de leurs créations – mais il vaut mieux le savoir à l’avance. Un projet sponsorisé par une grande entreprise a moins de chances de changer subitement de licence (quoique…), tandis qu’un projet porté par un mainteneur isolé sans source de revenus est plus susceptible, un jour, d’évoluer vers une formule payante ou d’être abandonné. L’initiative FOSSED (FOSS Emergency Dashboard) recense par exemple les changements de licence et fournit des conseils sur la marche à suivre. L’idée n’est pas de se méfier de tout, mais d’être informé. Un développeur ou architecte moderne doit ajouter cette corde à son arc : la veille sur les licences et la compréhension des enjeux économiques open source.Enfin, ces changements soulignent le risque d’une dépendance excessive à des outils externes. Cela ne signifie pas qu’il faille réinventer la roue pour chaque projet – l’usage de bibliothèques éprouvées reste une bonne pratique – mais qu’il faut mesurer le niveau de dépendance. Quand un composant devient central dans votre architecture, demandez-vous comment vous feriez s’il venait à manquer. L’exemple de Redis l’illustre bien : certaines entreprises ont découvert leur vulnérabilité quand la licence a changé, et ont dû réévaluer leur stratégie technique en urgence. Idéalement, concevez vos systèmes de manière modulaire pour pouvoir substituer une implémentation par une autre. Et surtout, gardez un œil sur l’évolution de vos dépendances : abonnez-vous aux newsletters des projets GitHub, lisez les notes de version, suivez les blogs des auteurs. Un changement de licence ne tombe pas du ciel : il est souvent précédé de signes avant-coureurs (ralentissement des commits, discussions sur le financement, version majeure à l’horizon, etc.). En restant attentif, on peut anticiper et s’adapter au lieu de subir.Conclusion : vers un équilibre entre confiance et vigilanceEn conclusion, l’écosystème .NET vit une période charnière où la gratuité d’hier n’est plus garantie pour demain. Cela pousse chaque acteur – du développeur individuel à l’architecte logiciel en entreprise – à gagner en maturité sur la gestion des dépendances. Il ne s’agit pas de rejeter l’open source, bien au contraire : ces outils restent d’une valeur inestimable et la plupart demeurent libres. Mais il s’agit de passer d’une confiance aveugle à une confiance éclairée. Oui, on peut continuer à adopter des bibliothèques externes pour aller plus vite et plus loin, à condition de faire preuve de vigilance : vérifier la licence, évaluer la pérennité, contribuer quand on le peut, et prévoir un plan B.Ces récents changements de licence, s’ils ont pu ébranler nos habitudes, auront au moins eu le mérite de déclencher une prise de conscience collective. Ils nous invitent à soutenir davantage les mainteneurs open source, à mieux comprendre l’économie cachée derrière nos lignes de code, et à refaire de la stratégie logicielle un exercice complet, intégrant les considérations techniques et légales. Pour un développeur ou un architecte logiciel, c’est un rappel que notre responsabilité ne s’arrête pas à écrire du code efficace : il faut aussi s’assurer que ce code repose sur des fondations solides et durables, dans tous les sens du terme. L’open source a un bel avenir, si chacun y met du sien – en connaissances, en contributions ou en financement – afin que l’innovation collaborative rime avec viabilité à long terme." }, { "title": "Valider les courriels - pourquoi une regex ne suffit plus", "url": "/posts/validation-courriel/", "categories": "outil-developpement", "tags": "", "date": "2025-06-02 19:00:00 -0400", "snippet": "PréambuleJe ne pensais pas écrire un article aujourd’hui sur la validation des courriels, mais une récente discussion m’a fait changer d’avis. Comme beaucoup, j’ai longtemps utilisé une simple expr...", "content": "PréambuleJe ne pensais pas écrire un article aujourd’hui sur la validation des courriels, mais une récente discussion m’a fait changer d’avis. Comme beaucoup, j’ai longtemps utilisé une simple expression régulière (regex) pour vérifier le format d’une adresse courriel saisie par un utilisateur. Après tout, la forme d’une adresse électronique semble facile à valider : on a tous déjà croisé la fameuse regex ^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$ pour « valider » un courriel. Pourtant, valider une adresse email de manière fiable est bien plus complexe, et se reposer uniquement sur une regex est une mauvaise idée.Récemment, dans sa vidéo Regex for Email Validation? Think Again!, Derek Comartin a expliqué pourquoi cette approche n’est ni fiable ni suffisante. Cela m’a encouragé à approfondir le sujet et à partager les bonnes pratiques modernes en matière de validation des adresses courriel.Les limites des regex pour les adresses courrielValider un email par regex atteint vite ses limites.Voici pourquoi : Une regex ne couvre jamais tous les cas d’adresse valides. La norme RFC 5322 (et suivantes) autorise des variantes d’adresses bien plus larges que nos regex simplifiées. Par exemple, de nombreux caractères spéciaux sont permis dans la partie locale (avant le @) d’une adresse : !, #, $, %, &amp;, ', _, /, =, ?, ^, `, {, |, } ou ~ peuvent théoriquement faire partie d’un courriel. Une regex « maison » oublie souvent d’en tenir compte, ce qui peut conduire à rejeter à tort des emails valides. De même, certaines adresses spéciales (contenant des guillemets, des caractères non-ASCII, un nom de domaine littéral en IP, etc.) respectent les standards mais ne passeront pas un filtre regex trop strict. À l’inverse, une regex peut accepter des adresses invalides. Même une expression régulière très élaborée peut laisser passer des adresses qui ne fonctionneront pas en pratique. Un exemple classique : une regex réputée “parfaite” autorisait tony@example.com. (avec un point à la fin) comme adresse valide, alors qu’aucun serveur de messagerie n’acceptera un courriel se terminant par un point. Autre cas : certaines implémentations basées sur le HTML5 considèrent test@test (sans domaine de second niveau) comme valide côté client, alors que cette adresse n’a pas de domaine complet et n’aboutirait pas. En bref, même les meilleurs patterns peuvent avoir des failles et donner un faux sentiment de sécurité. Une regex ne dit rien sur l’existence réelle de l’adresse. C’est le point le plus important. Une adresse peut très bien avoir la bonne forme et être acceptée par toutes vos validations syntaxiques, tout en n’existant pas du tout dans la réalité (domaine inexistant ou simplement aucune boîte à ce nom). La regex, aussi sophistiquée soit-elle, ne peut pas vérifier que le serveur de mail existe et que la boîte de réception est active. En d’autres termes, une validation purement syntaxique n’assure en rien que l’email pourra effectivement recevoir vos messages.En plus de ces limites inhérentes aux regex, se reposer uniquement sur une validation côté client (front-end) est une pratique dangereuse. Tout contrôle JavaScript ou Angular peut être contourné par un utilisateur malintentionné. Si votre application ne valide les courriels qu’au niveau du navigateur, vous risquez d’enregistrer en base de données des adresses totalement invalides envoyées directement à votre serveur. Il est impératif de reproduire les validations critiques côté serveur, sans se fier exclusivement au contrôle du formulaire côté client.Que proposent Angular, .NET et FluentValidation ?Face à la difficulté de définir LA regex parfaite, les principaux frameworks et bibliothèques ont adopté des approches pragmatiques – soit en utilisant des patterns larges, soit au contraire en simplifiant la validation. Angular (Front-end) – Le validateur d’email intégré d’Angular utilise une expression régulière inspirée de la spécification HTML5 (WHATWG) avec quelques ajustements basés sur les RFC. Concrètement, Angular va vérifier que l’adresse respecte le format général (présence d’un @ et d’un point, pas de point au début/à la fin de la partie locale, longueur maximale de 254 caractères, etc.). Ce choix couvre la majorité des cas courants tout en évitant des erreurs grossières. Cependant, même cette regex “améliorée” reste une solution générique : elle peut ne pas satisfaire tous les besoins métier, surtout pour des cas extrêmes ou des domaines moins standards. Angular permet d’ailleurs de la remplacer par un pattern personnalisé si nécessaire. En somme, Angular offre une première couche de validation utile côté client, mais n’élimine pas le besoin de validations supplémentaires côté serveur. .NET (Back-end) – Du côté .NET, l’évolution est intéressante. Historiquement, la classe d’attribut EmailAddressAttribute (dans System.ComponentModel.DataAnnotations) utilisait une énorme expression régulière pour valider les emails (dans les versions .NET Framework 4.x et antérieures). Cette regex « monstrueuse » couvrait un grand nombre de cas, mais elle alourdissait la validation et restait imparfaite. Depuis .NET Core, Microsoft a changé d’approche : l’attribut se contente désormais de vérifier que la chaîne contient un @ qui n’est ni en première position ni en dernière position. En clair, si un texte a au moins un caractère avant et après un @, il passe la validation de base. Par exemple, \"code.maze.com\" sera jugé invalide (pas de @), \"code@maze\" invalide (le @ est en fin de chaîne après “maze”?), et \"code@[email protected]\" valide (même si ce n’est pas une adresse fonctionnelle). Ce choix peut surprendre, mais il est volontairement minimaliste. L’objectif de .NET ici est juste d’éliminer les entrées grossièrement incorrectes, sans tenter de capturer toutes les subtilités du RFC. Microsoft assume que la véritable validation doit se faire autrement (via un workflow d’activation, etc.), plutôt que de maintenir une regex complexe et faillible. Résultat : on évite de fausses erreurs sur des adresses valides mais un peu atypiques, en attrapant seulement les cas évidents (absences de @, etc.). FluentValidation (Back-end, .NET) – FluentValidation, une bibliothèque populaire de validation en .NET, aligne sa stratégie sur celle de .NET Core. Son validateur intégré EmailAddress() réalise par défaut la même vérification simplifiée : présence d’un @ non situé au début ni à la fin de la chaîne. Cette démarche est assumée pour coller au comportement d’ASP.NET Core et à l’attribut EmailAddressAttribute. Dans la documentation officielle, les auteurs expliquent que le check est intentionnellement naïf, car “faire quelque chose d’infaillible est très difficile”. Ils recommandent de valider réellement l’adresse en envoyant un email de confirmation, la validation logicielle n’ayant pour but que d’attraper les valeurs grossièrement erronées destinées à l’UI. FluentValidation offre bien la possibilité d’utiliser l’ancien mode de validation (une regex héritée de .NET 4.x) via EmailAddress(EmailValidationMode.Net4xRegex), mais cette option est désormais dépréciée et génère un avertissement – signe que la validation basée sur une regex exhaustive n’est plus recommandée. En pratique, si vous utilisez FluentValidation, le conseil est donc le même : ne faites qu’une validation de format très basique, puis déléguez le reste du travail.En résumé, les frameworks modernes tendent à assouplir la validation syntaxique des emails plutôt qu’à la renforcer outre mesure. Angular propose une regex large couvrant les cas usuels, tandis que .NET/FluentValidation optent pour un contrôle minimal. L’idée sous-jacente est la suivante : plutôt que d’essayer de tout valider parfaitement avec du code statique (regex), mieux vaut attraper le 90% d’erreurs évidentes, et traiter le 10% restant via des mécanismes dynamiques.Valider l’existence de l’adresse (sans envoyer de mail)Attraper les erreurs de format, c’est bien – mais ça ne suffit pas, on l’a vu. La vraie question est : cette adresse existe-t-elle réellement ? Autrefois, certains systèmes envoyaient un email de confirmation à l’adresse fournie, espérant qu’un message invalide “reviendrait à l’expéditeur” si l’adresse n’existe pas. Mauvaise idée ! Envoyer un courriel juste pour vérifier qu’il arrive peut nuire à votre réputation : si vous envoyez en masse à des adresses invalides, les serveurs destinataires peuvent vous classer comme spammeur. De plus, vous risquez de froisser vos nouveaux utilisateurs en les obligeant à une confirmation pénible, ou pire, d’envoyer des emails non sollicités. Derek Comartin le mentionnait dans sa vidéo : cette approche n’est pas la meilleure non plus.La solution moderne consiste à déléguer la validation réelle à un service externe spécialisé. Il existe aujourd’hui des services SaaS de vérification d’adresses email (NeverBounce, ZeroBounce, Mailfloss, Kickbox et bien d’autres) qui font ce travail de manière efficace sans envoyer de courriel au destinataire. Comment est-ce possible ? Ces services effectuent une série de contrôles techniques : Vérification du domaine : ils examinent si le nom de domaine de l’adresse existe et possède des enregistrements MX (serveurs de mail) configurés. Pas de serveur mail = adresse forcément invalide. Ce premier filtre permet déjà d’éliminer les domaines fantaisistes ou désactivés. Réponse du serveur de messagerie : le service contacte le serveur de messagerie du domaine via le protocole SMTP (comme s’il allait envoyer un message) et interroge s’il peut accepter l’adresse destinataire donnée. En simulant cette conversation avec le serveur (un ping de la boîte mail), on peut savoir si l’adresse est reconnue sans envoyer réellement de mail. Si le serveur répond “boîte inexistante” (code d’erreur 550 par ex.), on sait que l’adresse est invalide. Si au contraire on obtient une réponse positive (250 OK), c’est bon signe – l’adresse semble valide et prête à recevoir du courrier. Détection avancée : ces services ajoutent souvent d’autres filtres utiles, comme l’identification des adresses jetables (fournisseurs de mails temporaires), des spam traps (adresses pièges qui ne servent qu’à piéger les spammeurs), ou des adresses ayant un historique d’abus. Par exemple, ZeroBounce signale si une adresse appartient à un domaine connu pour être toxique ou si c’est un alias “catch-all” qui accepte tous les mails sans garantir la délivrabilité. Ce sont des informations importantes pour maintenir la propreté de vos listes de diffusion.Le grand avantage de ces services est qu’ils effectuent toutes ces vérifications en temps réel via des API, généralement en quelques centaines de millisecondes, et ce de manière transparente pour l’utilisateur final. Par exemple, vous pouvez intégrer l’API de validation de NeverBounce ou ZeroBounce directement lors de l’inscription de vos utilisateurs : dès qu’ils soumettent leur adresse, une requête API vérifie en arrière-plan l’adresse et indique si elle est valide, sans que vous ayez à envoyer un seul courriel de test. Si l’adresse s’avère invalide, vous pouvez inviter l’utilisateur à la corriger, évitant ainsi d’enregistrer des contacts erronés dans votre base de données. Exemple de service SaaS : NeverBounce est l’un de ces outils de validation très fiables et rapides. Il s’intègre facilement à vos applications via API et peut vérifier des listes entières d’emails en quelques minutes. Lors d’un test comparatif, NeverBounce a pu vérifier 800 adresses en 5 minutes, identifiant correctement plus de 650 emails valides. Ce type de service assure une vérification complète de vos courriels (domaine actif, serveur répondant, adresse existante) sans effort de votre part. L’investissement en vaut la chandelle si la qualité de vos emails est critique (campagnes marketing, envois transactionnels, etc.), car il en va de votre taux de délivrabilité et de votre réputation d’expéditeur.En utilisant un service de validation externe, le workflow de validation “idéal” d’un courriel ressemble à ceci : Validation syntaxique légère (ex. via un contrôle Angular en front-end, puis une vérification minimale côté serveur avec .NET/FluentValidation) pour attraper les erreurs de frappe grossières immédiatement. Validation d’existence via un service externe pour vérifier que l’adresse existe et est opérationnelle (domaine OK, serveur OK, boîte OK), sans envoyer de mail de confirmation. Prise en compte dans votre logique métier : par exemple, marquer l’adresse comme “vérifiée” dans votre base de données ou votre CRM, permettre l’inscription du compte associé, etc. Si le service retourne un statut négatif (adresse inexistante, jetable ou autre), décider du traitement (demander une autre adresse, alerter l’utilisateur, refuser l’inscription, etc.).Ce processus à plusieurs couches assure une validation robuste tout en offrant une bonne expérience utilisateur : on ne bloque pas l’internaute sur des détails de format exotiques, mais on s’assure en coulisse que l’adresse fournie est exploitable.ConclusionLa validation des adresses courriel ne doit plus se résumer à trouver LA regex miracle. Les pratiques ont évolué : entre les spécifications extensibles du format email et les enjeux de délivrabilité, il est clair qu’une simple regex côté client ne suffit plus pour garantir la validité d’une adresse. Il faut adopter une approche en plusieurs étapes, combinant tolérance sur le format et fermeté sur l’existence réelle.En 2025, le bon workflow pour valider un courriel est : ✅ Vérifier légèrement le format (juste assez pour éviter les entrées absurdes, sans exclure à tort des adresses valides) ✅ Vérifier l’existence via un service externe (s’assurer que le domaine et la boîte sont valides, sans vous faire passer pour un spammeur) ❌ Ne plus s’acharner à écrire des regex compliquées, ni se fier uniquement à la validation du navigateur. Une regex peut filtrer le gros des erreurs, mais pas toutes, et certainement pas la délivrabilité réelle.En appliquant ces principes, vous améliorerez la qualité des données collectées et réduirez les problèmes d’emails invalides (bounces, plaintes, etc.) qui peuvent nuire à votre plateforme.Enfin, n’hésitez pas à documenter ce choix technologique dans votre registre de décisions interne. La manière dont vous validez les courriels est un choix d’architecture applicative important, qui mérite d’être tracé noir sur blanc. Comme le souligne Derek Comartin, garder un log de toutes les décisions clés d’un projet évite bien des devinettes par la suite, en fournissant le contexte du pourquoi chaque décision a été prise. Inscrire dans votre registre de décisions que “la validation des emails se fait via une vérification syntaxique minimale + service externe X, pour telles raisons” permettra aux futurs développeurs de comprendre et de respecter cette approche cohérente.En somme, valider un courriel aujourd’hui, ce n’est pas chercher la perfection avec une regex interminable, c’est mettre en place un processus intelligent : souple sur le format, strict sur l’existence, et toujours documenté. Adoptez ce workflow moderne – vos utilisateurs, votre équipe et votre serveur mail vous en remercieront." }, { "title": "GUID vs ID auto-incrémenté - dilemme et solutions en environnement .NET distribué", "url": "/posts/guid-versus-id/", "categories": "", "tags": "dotnet", "date": "2025-05-19 13:00:00 -0400", "snippet": "PréambuleJe me souviens encore d’une réunion mouvementée avec un DBA, alors que nous discutions de l’architecture d’un nouveau système .NET réparti sur plusieurs services. Lorsque j’ai osé proposer...", "content": "PréambuleJe me souviens encore d’une réunion mouvementée avec un DBA, alors que nous discutions de l’architecture d’un nouveau système .NET réparti sur plusieurs services. Lorsque j’ai osé proposer d’utiliser des GUID comme clés primaires au lieu des habituels identifiants auto-incrémentés, j’ai vu des yeux s’écartiller. « Pourquoi changer une recette qui marche ? » m’a-t-on lancé. En bon développeur, j’avais moi-même toujours apprécié la simplicité des IDs séquentiels (1, 2, 3, …). Mais dans un environnement distribué et asynchrone, cette vieille recette commençait à montrer ses limites.Dans cet article, je vous propose un retour d’expérience sur ce choix technique. Nous verrons pourquoi les GUID peuvent s’avérer pratiques (et comment les utiliser intelligemment), sans occulter les inconvénients bien réels qui rendent certains DBA méfiants. Installez-vous confortablement, on va parler d’identifiants, de .NET/Entity Framework, d’index, de sécurité, de tests et de compromis astucieux.Le dilemme dans un monde distribuéIdentifiants auto-incrémentés ou GUID ? Le débat est ancien. Dans un monde monolithique, les identifiants séquentiels (souvent de type int) brillent par leur simplicité, leur performance, et leur tri naturel. Mais dès que le système devient distribué, asynchrone, ou s’étend sur plusieurs bases de données, les problèmes commencent : collisions d’identifiants, coordination nécessaire pour garantir l’unicité, complexité lors des fusions de données ou synchronisations inter-sites.Les GUID (Globally Unique IDentifiers), quant à eux, peuvent être générés localement par l’application, sans dépendance à la base de données, tout en assurant une unicité quasi-absolue (2^128 combinaisons possibles).Nouveautés en .NET 9 : les GUID version 7Depuis la version 9, .NET introduit la méthode Guid.CreateVersion7(), qui génère des GUID ordonnables chronologiquement. Cela répond à une critique historique des GUID : leur tendance à provoquer de la fragmentation dans les index SQL.Les GUID v7 combinent un horodatage (48 bits) et des bits aléatoires.On peut les utiliser avec :Guid guid7 = Guid.CreateVersion7();Ces nouveaux GUID sont plus efficaces pour les insertions en base de données, car ils réduisent significativement les fragmentations d’index et offrent des performances comparables aux IDs séquentiels tout en conservant les avantages des GUID. Bien que EF Core ne les supporte pas encore nativement, il est possible de créer un générateur personnalisé pour les utiliser efficacement.Avantages des GUID dans un système distribué Unicité globale : pas besoin de coordination entre les bases de données ou les services. Génération côté client : utile pour le mode hors-ligne, les tests ou les systèmes à haute disponibilité. Scalabilité : pas de bottleneck sur une table centrale. Compatibilité avec la réplication : les GUID sont idéaux pour la réplication multi-site. Facilite les tests automatisés : permet de créer des identifiants prédictibles et stables entre exécutions. Uniformisation entre environnements : les mêmes identifiants peuvent être utilisés en dev, test, staging ou production.Cas concrets Fusion de deux bases de données : sans GUID, risque de collision d’IDs. Microservices : chaque service peut gérer ses propres identifiants. Tests automatisés : avec des GUID prévisibles, les assertions et les fixtures deviennent plus stables. Alimentation entre environnements : les mêmes GUID peuvent être utilisés pour des jeux de données identiques, simplifiant les déploiements.Implémentation en EF Corepublic class Commande { public Guid Id { get; set; } = Guid.NewGuid(); public string Description { get; set; } = \"\";}Pour utiliser un GUID séquentiel (côté SQL Server) :modelBuilder.Entity&lt;Commande&gt;() .Property(c =&gt; c.Id) .HasDefaultValueSql(\"NEWSEQUENTIALID()\");Inconvénients des GUID Taille (16 octets vs 4 pour un int) Moins lisibles pour les humains Risque de fragmentation si non ordonnés Légère surcharge en performance sur les jointuresDialogue avec les DBAs : trouver le bon équilibreTous les DBAs ne sont pas réfractaires au changement. Certains apportent même des solutions très pertinentes (index non cluster, fill factor adapté, etc.). Ce qu’ils attendent, c’est que les choix soient justifiés techniquement, mesurés, et documentés. Un DBA averti sera rassuré de savoir que vous utilisez des GUID v7 plutôt que des GUID aléatoires classiques. Le dialogue avec eux permet de mettre en place une stratégie gagnant-gagnant.Sécurité des routes HTTP exposéesUn autre avantage souvent oublié : la sécurité par l’obscurcité.Une route comme :GET /api/citoyens/5/enfantsest prévisible. Un utilisateur malveillant peut tester /6, /7, etc., pour tenter d’accéder à des données qui ne lui sont pas destinées. C’est une attaque de type IDOR (Insecure Direct Object Reference).Avec des GUID :GET /api/citoyens/6f8d2ac1-3b90-4dc5-9543-f490a8f5d8c2/enfantsl’exploration devient presque impossible sans information préalable. Attention : cela ne remplace pas une vérification des droits, mais cela constitue une couche supplémentaire de protection, conforme aux bonnes pratiques OWASP. Cela décourage aussi les attaques automatisées.ConclusionUtiliser des GUID, ce n’est pas une mode. C’est une réponse technique à des besoins très concrets dans les architectures modernes. Il faut être conscient des implications (taille, fragmentation, lisibilité), mais avec les GUID v7, les bonnes pratiques EF Core, une stratégie de tests solide, et une communication honnête avec vos DBAs, ils peuvent devenir un atout puissant. Et dans un monde où la distribution, l’asynchronisme, les tests déconnectés, la sécurité et l’automatisation sont la norme, c’est un choix de plus en plus stratégique.Bon coding, et que vos identifiants restent uniques !" }, { "title": "Redis 8 - Un retour attendu à l’open source avec l’AGPLv3", "url": "/posts/redis-redevient-open-source/", "categories": "outil-developpement", "tags": "", "date": "2025-05-04 19:00:00 -0400", "snippet": "Le 1er mai 2025, Redis a fait un virage majeur en revenant à ses racines open source. Redis 8 est désormais disponible sous la licence AGPLv3, approuvée par l’Open Source Initiative (OSI). Ce chang...", "content": "Le 1er mai 2025, Redis a fait un virage majeur en revenant à ses racines open source. Redis 8 est désormais disponible sous la licence AGPLv3, approuvée par l’Open Source Initiative (OSI). Ce changement intervient après une période de turbulences sur le plan des licences, que j’avais déjà abordée dans un précédent article.📜 Une parenthèse fermée : de BSD à SSPL, puis à AGPLEn mars 2024, Redis avait troqué sa licence permissive BSD pour un modèle dual : RSALv2 et SSPLv1. L’objectif était de restreindre l’exploitation commerciale sans contribution équitable, notamment par les grands fournisseurs cloud.Ce virage avait déplu à la communauté, provoquant l’apparition de forks comme Valkey, soutenu par la Linux Foundation. En réponse à ces tensions, Redis 8 est désormais aussi disponible sous AGPLv3, une licence réellement open source, tout en conservant les autres options.👨‍💻 Le retour de @antirez et les nouveautés de Redis 8Le retour de Salvatore Sanfilippo (@antirez), créateur de Redis, a été un moment clé. De retour chez Redis en novembre 2024 comme évangéliste développeur, il a participé à la réorientation du projet. Sous sa houlette, Redis 8 propose de vraies avancées techniques : Vector Sets : un nouveau type de données orienté pour l’IA et recherche vectorielle. Modules intégrés directement dans Redis Core : JSON, Time Series, Bloom, Cuckoo, Count-min sketch, Top-K, t-digest, etc. Améliorations de performance : plus de 30 optimisations, avec jusqu’à 87 % de gain de vitesse sur certaines commandes et un débit doublé.💡 Redis 8 s’aligne donc sur les besoins modernes des développeurs et des infrastructures.🌍 Une communauté rassuréeLe retour à une licence open source a été bien accueilli. Sanfilippo l’a résumé simplement : « Je suis heureux que Redis soit à nouveau un logiciel open source, sous les termes de la licence AGPLv3. »Ce geste témoigne d’une volonté de renouer avec les principes du logiciel libre, tout en soutenant une innovation continue.🎯 Ce qu’il faut retenir Redis revient à l’open source avec l’AGPLv3. Redis 8 introduit des fonctionnalités fortes pour l’IA, des modules clés et des vraies optimisations. C’est un geste d’ouverture et de réconciliation avec la communauté des développeurs.📚 Pour creuser le sujet : L’annonce officielle : redis.io/blog/agplv3 Mon analyse du changement de licence de Redis en 2024 : Redis change de licence – Pourquoi ça fait débat" }, { "title": "Interrupteurs de fonctionnalité", "url": "/posts/interrupteur-fonctionnalite/", "categories": "outil-developpement", "tags": "dotnet", "date": "2025-04-21 18:00:00 -0400", "snippet": "Qu’est-ce qu’un interrupteur de fonctionnalitéLes applications cloud, les microservices et les pratiques DevOps misent sur la vitesse et l’agilité. Les utilisateurs attendent de la réactivité, des ...", "content": "Qu’est-ce qu’un interrupteur de fonctionnalitéLes applications cloud, les microservices et les pratiques DevOps misent sur la vitesse et l’agilité. Les utilisateurs attendent de la réactivité, des nouveautés fréquentes et peu (voire aucun) temps d’arrêt.Les interrupteurs de fonctionnalité sont une technique moderne de déploiement permettant de découpler l’activation d’une fonctionnalité de son déploiement. Une simple modification de configuration suffit à activer une fonctionnalité pour certains utilisateurs ou environnements, sans redémarrer ni redéployer l’application (en théorie). ⚡ Ils permettent de dissocier le déploiement de la mise en production.Cela permet notamment de tester des fonctionnalités en production sur des groupes restreints d’utilisateurs ou d’activer graduellement une nouveauté.Mise en place dans une application .NETInstallation des packages NuGet Pour tout projet : dotnet add package Microsoft.FeatureManagement Pour les APIs : dotnet add package Microsoft.FeatureManagement.AspNetCore Configuration minimaleusing Microsoft.FeatureManagement;public class Program { // ... builder.Services.AddFeatureManagement();}Utilisation du service IFeatureManagerUne fois la configuration en place, vous pouvez interroger facilement un interrupteur de fonctionnalité comme illustré dans l’exemple suivant :public class MonService{ private readonly IFeatureManager _featureManager; public MonService(IFeatureManager featureManager) { _featureManager = featureManager; } public Task&lt;string&gt; ObtenirMessage() =&gt; _featureManager.IsEnabledAsync(\"NouvelleFonctionnalite\") ? \"Bienvenue avec la nouvelle fonctionnalité\" : \"Bienvenue\";}Sources de configurationFichier appsettings.json{ \"FeatureManagement\": { \"NouvelleFonctionnalite\": true }}Ou via une section personnalisée :{ \"MaSectionFlags\": { \"NouvelleFonctionnalite\": true }}services.AddFeatureManagement(Configuration.GetSection(\"MaSectionFlags\"));Variables d’environnementFeatureManagement__NouvelleFonctionnalite=true ✅ Deux underscores requis entre la section et la clé.Azure App Configuration NuGet : dotnet add package Microsoft.Extensions.Configuration.AzureAppConfiguration Configuration :config.AddAzureAppConfiguration(options =&gt;{ options.Connect(Environment.GetEnvironmentVariable(\"ConnectionString\")) .ConfigureRefresh(refresh =&gt; refresh.Register(\"Settings:Sentinel\", refreshAll: true) .SetCacheExpiration(TimeSpan.FromMinutes(1))) .UseFeatureFlags();});Cas d’utilisationActivation conditionnelle d’un contrôleur ou endpoint[FeatureGate(FeatureFlags.NouvelleFonctionnalite)][ApiController][Route(\"api/[controller]\")]public class ExempleController : ControllerBase{ [HttpGet] [FeatureGate(FeatureFlags.NouvelleFonctionnalite)] public IActionResult Get() =&gt; Ok(\"Accès autorisé\");}Composant Blazor (WebAssembly)@if (estFonctionnaliteActive){ &lt;div&gt;La fonctionnalité est active.&lt;/div&gt;}[Inject] public required IFeatureManager FeatureManager { get; set; }private bool estFonctionnaliteActive;protected override async Task OnInitializedAsync(){ estFonctionnaliteActive = await FeatureManager.IsEnabledAsync(nameof(FeatureFlags.NouvelleFonctionnalite)); // ...}Redirection conditionnelle (ex. : vers une page 404)&lt;NotFound&gt; &lt;LayoutView Layout=\"@typeof(MainLayout)\"&gt; &lt;PageNonTrouvee /&gt; &lt;/LayoutView&gt;&lt;/NotFound&gt;Autres possibilitésLes interrupteurs ne se limitent pas à un simple état activé ou désactivé. Grâce aux filtres, il est possible d’introduire des comportements plus dynamiques et contextuels. Interrupteurs avancés - Microsoft permet d’ajouter des filtres personnalisés comme une activation basée sur un utilisateur, une région, etc. Interrupteurs personnalisés - Vous pouvez créer vos propres filtres.Bonnes pratiquesUtiliser un enum pour éviter les chaînes magiquespublic enum FeatureFlags{ NouvelleFonctionnalite, FonctionnaliteB}await _featureManager.IsEnabledAsync(nameof(FeatureFlags.NouvelleFonctionnalite));Faire le ménage régulièrement des interrupteursLes interrupteurs de fonctionnalité sont par nature temporaires. Une fois qu’une fonctionnalité est pleinement déployée ou qu’un test A/B est terminé, il est essentiel de retirer les interrupteurs associés du code et de la configuration.Laisser des interrupteurs inactifs ou non utilisés peut nuire à la lisibilité, alourdir la maintenance, et introduire de l’incertitude dans les comportements attendus de l’application. 💡 Astuce : tenez une liste des interrupteurs actifs avec leur date de création et l’objectif visé. Cela vous aidera à planifier leur retrait à temps.Comportement personnalisé lorsqu’une action est désactivéePar défaut, un contrôleur ou action désactivé retourne un HTTP 404.Vous pouvez créer une classe personnalisée qui implémente IDisabledFeaturesHandler pour gérer un autre type de réponse. 👉 Voir la documentation.ConclusionLes interrupteurs de fonctionnalité sont un levier puissant pour améliorer l’agilité des équipes de développement, réduire les risques liés aux déploiements et faciliter l’expérimentation de nouvelles fonctionnalités. Leur intégration dans une application .NET est relativement simple grâce au support natif offert par Microsoft.FeatureManagement.Cependant, comme tout outil, leur utilisation doit être encadrée par des bonnes pratiques : éviter l’accumulation d’interrupteurs obsolètes, documenter leur usage, et réfléchir à une stratégie de configuration adaptée à l’échelle du projet (locale, via App Configuration, ou centralisée).Adopter les interrupteurs de fonctionnalité, c’est se donner les moyens de livrer plus rapidement, de manière plus sûre, et avec une meilleure maîtrise du changement. Commencez petit, expérimentez, et adaptez votre approche à vos besoins réels. L’essentiel est de rester intentionnel dans l’usage que vous en faites." }, { "title": "Introduction à K6", "url": "/posts/introduction-k6/", "categories": "outil-developpement", "tags": "k6, essais", "date": "2025-04-14 20:00:00 -0400", "snippet": "Qu’est-ce que k6 ?k6 est un outil open-source de test de charge et de performance, conçu pour aider les développeurs à évaluer la fiabilité de leurs systèmes, notamment les API, microservices et si...", "content": "Qu’est-ce que k6 ?k6 est un outil open-source de test de charge et de performance, conçu pour aider les développeurs à évaluer la fiabilité de leurs systèmes, notamment les API, microservices et sites web. Développé par Grafana Labs, k6 permet de simuler des comportements utilisateur réalistes et d’identifier les potentielles faiblesses avant le déploiement en production.Pourquoi utiliser k6 ? Si vous êtes habitué à Apache JMeter, k6 vous apportera une approche plus moderne et développeur-friendly avec des tests écrits en JavaScript. 🙌 Son CLI facilite son intégration dans les pipelines d’intégration continue. Capacité à générer des rapports détaillés sur les performances, aidant à identifier les goulets d’étranglement et les points à optimiser.Types d’essais de charge 🔼 Test de montée en charge (Ramp-up test) - Augmente progressivement le nombre d’utilisateurs pour voir à quel moment l’application commence à ralentir. 💥 Test de stress (Stress test) - Envoie plus de requêtes que la capacité normale pour voir comment l’application réagit sous la surcharge. ⏳ Test d’endurance (Soak test) - Vérifie si l’application reste stable après plusieurs heures/jours sous charge continue. ⚡ Test de pointe (Spike test) - Simule une augmentation soudaine du trafic pour voir si le système peut absorber les pics de charge.La liste ne s’arrête pas là ! Il existe également d’autres types de tests complémentaires, comme :  🧨 Tests de chaos et de résilience (Chaos and Resilience Testing) - Simulent des pannes ou des conditions extrêmes pour évaluer la capacité d’un système à résister aux défaillances et à se rétablir automatiquement. 🖧 Tests d’infrastructure (Infrastructure Testing) - Vérifient la performance et la fiabilité des composants sous-jacents tels que les serveurs, les bases de données, les réseaux et le stockage cloud. Ils aident à identifier les goulets d’étranglement et à optimiser les ressources.  🌐 Tests de performance du navigateur (Browser Performance Testing) - Mesurent la vitesse d’affichage et le temps de chargement des pages web du point de vue de l’utilisateur final. Ces tests permettent d’optimiser les performances côté client et d’améliorer l’expérience utilisateur. Outils populaires : Lighthouse et k6 Browser.  💡 En combinant ces différents types de tests, on s’assure d’une application robuste, performante et résiliente, prête à affronter toutes les conditions !Il est essentiel d’évaluer le retour sur investissement (ROI) avant de multiplier les tests : toutes les applications n’ont pas besoin de tests de charge, de résilience ou d’infrastructure avancés. L’important est d’adapter la stratégie de test aux risques et aux exigences de l’application pour éviter des efforts inutiles.Mise en place de k6k6 est multiplateforme et compatible avec Windows, Linux et macOS. Pour l’installer, utilisez le gestionnaire de paquets adapté à votre système d’exploitation.Sur Windows :winget install k6 --source winget 💡Notons également qu’il est possible d’exécuter k6 depuis un conteneur.Lancer un simple test de charge sur une API RESTFaire un test de charge sur une API REST (ex: https://example.com/api/products) pour simuler 10 utilisateurs virtuels pendant 30 secondes.Script test.js :import http from 'k6/http';import { sleep, check } from 'k6';export const options = { vus: 10, // 10 utilisateurs virtuels duration: '30s', // pendant 30 secondes};export default function () { const res = http.get('https://example.com/api/products'); check(res, { 'status est 200': (r) =&gt; r.status === 200, 'réponse contient produits': (r) =&gt; r.body.includes('product'), }); sleep(1);}Exécution du script :k6 run test.js🙌 Tu verras des statistiques en temps réel comme le nombre de requêtes réussies, les échecs, le temps de réponse moyen, etc.Comprendre les résultats d’exécution de k6Lorsqu’un test de charge est exécuté avec k6, un rapport détaillé est généré en console, affichant plusieurs indicateurs clés de performance. Ces métriques permettent d’évaluer la stabilité, la rapidité et la robustesse du système testé.   Résumé des métriques principales À la fin d’un test, k6 affiche un tableau de résultats contenant les indicateurs suivants :    http_reqs : Nombre total de requêtes HTTP effectuées pendant le test.  http_req_duration : Temps moyen de réponse des requêtes HTTP, généralement mesuré en millisecondes.  http_req_failed : Pourcentage de requêtes ayant échoué.  vus (Virtual Users) : Nombre d’utilisateurs simultanés simulés à un instant donné.  iterations : Nombre total d’itérations de script exécutées.Analyse des temps de réponseL’un des éléments les plus critiques est le http_req_duration, qui mesure le temps de réponse des requêtes.k6 fournit des valeurs utiles comme :  Moyenne (avg) : Temps de réponse moyen sur l’ensemble des requêtes.  Médiane (med) : Le temps de réponse qui sépare la moitié des requêtes les plus rapides des plus lentes.  95e percentile (p(95)) : 95% des requêtes ont eu un temps de réponse inférieur à cette valeur.  Maximum (max) : Le temps de réponse le plus élevé observé.   Ces données aident à identifier des problèmes comme des pics de latence ou des temps de réponse anormalement longs.Vérification des échecs et des erreurs L’indicateur http_req_failed permet de voir si certaines requêtes ont échoué (timeouts, erreurs serveur, etc.). Un taux élevé peut indiquer une saturation du backend ou une mauvaise configuration de l’application.   Interprétation des tendances et optimisations possibles  Si le temps de réponse médian est bas, mais le max est élevé, cela signifie que certaines requêtes subissent des latences importantes, peut-être dues à un goulot d’étranglement.  Un taux d’échec élevé peut indiquer un problème de scalabilité ou un manque de ressources côté serveur.  Si la charge CPU ou mémoire du serveur augmente rapidement, il pourrait être utile de mettre en place un autoscaling ou d’optimiser les requêtes et le caching. ConclusionDans un contexte où les performances applicatives sont de plus en plus scrutées, k6 s’impose comme un outil moderne, léger et puissant pour réaliser des essais de charge adaptés aux réalités d’aujourd’hui. Que ce soit pour valider la scalabilité d’une API, détecter des goulets d’étranglement, ou s’assurer que l’expérience utilisateur reste fluide en période de pointe, k6 offre une solution accessible aussi bien aux développeurs qu’aux équipes DevOps.Grâce à sa syntaxe en JavaScript, son intégration facile dans les pipelines CI/CD, et la richesse des métriques fournies, il devient un excellent allié pour intégrer les tests de performance dès les premières étapes du cycle de développement — un des piliers des pratiques modernes comme le Shift Left Testing.Il est cependant essentiel de rappeler que tous les projets ne nécessitent pas le même niveau de rigueur en matière de tests de performance. Adapter les types d’essais (stress, endurance, montée en charge…) aux enjeux métier, à l’infrastructure cible et aux attentes des utilisateurs permet d’optimiser l’effort tout en maximisant le retour sur investissement.En résumé, que vous souhaitiez prévenir les mauvaises surprises en production ou simplement renforcer la résilience de vos systèmes, k6 est un excellent point de départ pour professionnaliser vos essais de charge. Et comme toujours : testez tôt, testez souvent, et testez intelligemment." }, { "title": "Redis change de licence ! Quelles répercussions pour les développeurs et les entreprises ?", "url": "/posts/redis-change-licence/", "categories": "outil-developpement", "tags": "", "date": "2025-04-06 20:00:00 -0400", "snippet": "Depuis son lancement, Redis s’est imposé comme l’un des outils de base de données en mémoire les plus populaires. Que ce soit pour le caching, la gestion de files d’attente ou même l’implémentation...", "content": "Depuis son lancement, Redis s’est imposé comme l’un des outils de base de données en mémoire les plus populaires. Que ce soit pour le caching, la gestion de files d’attente ou même l’implémentation de structures de données complexes, Redis a été un choix de prédilection pour de nombreuses entreprises et développeurs. Cependant, un changement de licence soulève des questions : quelles seront les implications pour ceux qui utilisent Redis au quotidien ? Et comment les alternatives comme Garnet de Microsoft pourraient-elles s’intégrer dans cet écosystème en pleine mutation ?Changement de licence : Qu’est-ce qui a changé ?Redis, historiquement disponible sous licence BSD, a modifié les conditions d’utilisation de certains modules via la Redis Source Available License (RSAL). Ce changement vise à limiter l’utilisation des modules spécifiques à Redis dans des solutions commerciales fournies en tant que service (Software-as-a-Service). L’objectif principal derrière ce changement est de protéger Redis des grands acteurs du cloud, tels qu’Amazon Web Services, qui bénéficiaient des modules Redis sans contribuer en retour au projet ou à son développement (Microsoft soutient l’utilisation de Redis à travers plusieurs initiatives).Voici quelques points clés à retenir : RSAL s’applique uniquement à certains modules Redis (comme RedisSearch ou RedisJSON), mais pas au cœur de Redis, qui reste sous licence BSD. Les entreprises peuvent toujours utiliser ces modules gratuitement pour une utilisation interne ou dans des produits on-premise. Cependant, la vente de services SaaS basés sur ces modules nécessite désormais une licence commerciale.Répercussions pour les développeurs et les entreprisesPour les développeurs travaillant sur des projets open source ou des applications internes, le changement est relativement mineur. Toutefois, pour les entreprises qui intègrent Redis dans des services cloud commercialisés, cela peut entraîner des coûts supplémentaires et des obligations contractuelles. Les startups et les PME utilisant Redis dans des environnements cloud doivent maintenant évaluer attentivement leur conformité à la nouvelle licence.Impacts potentiels : Coûts accrus : Les entreprises SaaS pourraient voir leurs frais augmenter si elles choisissent d’intégrer des modules Redis sous RSAL dans leurs solutions. Alternative à considérer : Les projets peuvent chercher à remplacer certains modules Redis par des solutions open source alternatives ou à développer leurs propres implémentations. Écosystème cloud : Les grands fournisseurs cloud, comme AWS, peuvent se tourner vers des forks de Redis ou encourager l’adoption de solutions alternatives comme ElastiCache, leur version propriétaire du service Redis.Garnet : Une alternative prometteuse ?En parallèle de ce changement de licence, de nouveaux outils émergent, notamment Garnet de Microsoft, qui pourrait devenir une alternative sérieuse à Redis dans certains cas d’usage. Garnet est un projet conçu pour être une base de données distribuée ultra-rapide, optimisée pour les applications cloud natives, et offrant des fonctionnalités qui rivalisent avec Redis.Il se distingue par : Une intégration native avec Azure : Garnet est conçu pour tirer parti de l’infrastructure Azure, ce qui en fait un choix naturel pour les entreprises déjà investies dans cet écosystème. Une approche axée sur la performance : Tout comme Redis, Garnet est conçu pour des temps de réponse très bas, ce qui le rend idéal pour des applications nécessitant des données en temps réel. Flexibilité des licences : Microsoft pourrait proposer une politique de licence plus flexible, qui pourrait être attrayante pour les entreprises concernées par la RSAL de Redis. Compatibilité avec le contrat Redis : Garnet est entièrement compatible avec le contrat Redis, facilitant ainsi son adoption pour les entreprises qui utilisent déjà cette technologie.ConclusionLe changement de licence de Redis représente une évolution significative dans la gestion des outils open source dans le cloud. Si la RSAL peut offrir une meilleure protection pour Redis Labs contre les géants du cloud, elle pousse également les entreprises et développeurs à réévaluer leur dépendance à ces outils. Les alternatives comme Garnet de Microsoft, ainsi que d’autres bases de données en mémoire, vont probablement gagner en popularité à mesure que l’écosystème continue d’évoluer.Pour les développeurs et entreprises, l’essentiel est de suivre de près ces changements et d’évaluer les coûts et les bénéfices de chaque solution pour leur propre architecture. Le choix de rester avec Redis, d’explorer Garnet ou même d’adopter une autre solution dépendra largement des besoins spécifiques et des contraintes budgétaires de chaque projet.Ce changement de paradigme autour de Redis reflète un mouvement plus large dans l’écosystème open source où de plus en plus de projets cherchent à protéger leur modèle économique tout en continuant à offrir des solutions robustes pour les utilisateurs finaux.💡Découvrez en vidéo l’impact du changement de licence de Redis." }, { "title": "Amélioration des performances en utilisant JsonSerializerOptions en singleton", "url": "/posts/performances-jsonserializeroptions-singleton/", "categories": "", "tags": "dotnet", "date": "2025-03-20 20:00:00 -0400", "snippet": "IntroductionRécemment, j’ai donné une formation interne où je partageais différentes astuces pour améliorer les performances d’applications en .NET. Parmi ces astuces, une technique particulièremen...", "content": "IntroductionRécemment, j’ai donné une formation interne où je partageais différentes astuces pour améliorer les performances d’applications en .NET. Parmi ces astuces, une technique particulièrement efficace concernait l’utilisation de JsonSerializerOptions en singleton.Historiquement, la sérialisation d’objets en .NET a souvent été réalisée avec le NuGet Newtonsoft. Toutefois, depuis l’introduction de System.Text.Json en .NET Core 3.0, une alternative plus performante et mieux intégrée au runtime est devenue disponible. Bien que System.Text.Json soit généralement plus rapide que Newtonsoft, son efficacité peut être compromise par une mauvaise gestion des options de sérialisation (JsonSerializerOptions). Lorsque ces options sont recréées à chaque appel au lieu d’être réutilisées, cela peut entraîner une augmentation notable des allocations mémoire et une dégradation des performances.Dans cet article, nous allons démontrer l’impact de l’utilisation d’une instance singleton de JsonSerializerOptions par rapport à la création d’options à chaque appel. Nous allons utiliser BenchmarkDotNet pour comparer les deux méthodes et mettre en évidence les gains potentiels en matière de performances.ContexteL’analyseur CA1869 recommande d’utiliser une instance de JsonSerializerOptions en singleton pour optimiser les performances. Cela est particulièrement pertinent lorsqu’une configuration personnalisée est requise, comme l’utilisation de PropertyNamingPolicy ou PropertyNameCaseInsensitive.Configuration de l’analyse de performancesPour mesurer les performances, nous utilisons le code suivant qui compare deux méthodes : SerializeSansSingleton() : Crée une nouvelle instance de JsonSerializerOptions à chaque sérialisation. SerializeAvecSingleton() : Réutilise une instance singleton préconfigurée (JsonSerializerOptions.Web) pour chaque sérialisation.Voici le code complet :using BenchmarkDotNet.Attributes;using BenchmarkDotNet.Columns;using BenchmarkDotNet.Order;using System.Text.Json;namespace TestBenchmarkDotnet.TestJsonSerializerOptions;[MemoryDiagnoser][Orderer(SummaryOrderPolicy.FastestToSlowest)][RankColumn][HideColumns(Column.Error, Column.Median, Column.RatioSD, Column.StdDev)]public class BenchmarkJsonSerializerOptions{ private readonly TestModel _testModel = new() { Id = 1, Nom = \"Test\", Description = \"This is a test description\", EstActif = true }; [Params(1, 100)] public int Iteration { get; set; } [Benchmark(Baseline = true)] public string SerializeSansSingleton() { var json = \"\"; for (var i = 0; i &lt; Iteration; i++) { var options = new JsonSerializerOptions { PropertyNamingPolicy = JsonNamingPolicy.CamelCase, PropertyNameCaseInsensitive = true }; json = JsonSerializer.Serialize(_testModel, options); } return json; } [Benchmark] public string SerializeAvecSingleton() { var json = \"\"; for (var i = 0; i &lt; Iteration; i++) { json = JsonSerializer.Serialize(_testModel, JsonSerializerOptions.Web); } return json; }}public class TestModel{ public int Id { get; set; } public required string Nom { get; set; } public required string Description { get; set; } public bool EstActif { get; set; }}Résultats des benchmarksL’exécution des benchmarks a produit les résultats suivants :| Méthode | Itérations | Durée moyenne | Ratio | Allocations Gen0 | Mémoire allouée ||------------------------|------------|---------------|-------|------------------|-----------------|| SerializeAvecSingleton | 1 | 205.6 ns | 0.24 | 0.0219 | 184 B || SerializeSansSingleton | 1 | 861.3 ns | 1.00 | 0.0429 | 390 B || SerializeAvecSingleton | 100 | 21,592.5 ns | 0.32 | 2.1973 | 18400 B || SerializeSansSingleton | 100 | 67,384.9 ns | 1.00 | 4.1504 | 39035 B |Analyse des résultatsLes résultats montrent clairement qu’en utilisant une instance singleton de JsonSerializerOptions : La méthode est environ 4 fois plus rapide pour une seule itération. Les allocations mémoire sont réduites de plus de 50 %, ce qui est significatif pour des traitements en masse.L’approche utilisant JsonSerializerOptions.Web réutilise la configuration en singleton, ce qui permet d’éviter la création d’objets inutiles et améliore ainsi les performances de façon considérable.ConclusionL’utilisation d’une instance singleton pour JsonSerializerOptions est une bonne pratique qui améliore non seulement les performances en termes de temps d’exécution, mais réduit également les allocations mémoire. Cela est particulièrement bénéfique dans les scénarios où les opérations de sérialisation sont répétées de manière intensive.L’analyseur CA1869 dans .NET fournit des recommandations qui méritent d’être suivies pour éviter les pièges courants en matière de performances.En résumé, si votre application effectue des sérialisations fréquentes avec des configurations spécifiques, envisagez d’utiliser une instance singleton pour vos JsonSerializerOptions. 💡 Astuces supplémentaires : Pour vous assurer que l’analyseur CA1869 est actif dans votre projet, vous pouvez l’ajouter dans votre fichier .editorconfig comme suit dotnet_diagnostic.CA1869.severity = warning." }, { "title": "Veille technologique - Pourquoi et comment rester à jour", "url": "/posts/veille-technologique/", "categories": "", "tags": "", "date": "2025-02-26 18:00:00 -0500", "snippet": "La veille technologique est un processus continu de surveillance et d’analyse des évolutions dans un domaine technologique donné. Elle consiste à collecter, analyser et exploiter des informations s...", "content": "La veille technologique est un processus continu de surveillance et d’analyse des évolutions dans un domaine technologique donné. Elle consiste à collecter, analyser et exploiter des informations sur les nouvelles tendances, les innovations, les évolutions des standards et les meilleures pratiques afin d’anticiper les changements et d’adapter ses stratégies en conséquence.📌 Pourquoi faire de la veille technologique ? Rester à jour – La technologie évolue rapidement, surtout dans des domaines comme le cloud, l’intelligence artificielle ou le développement logiciel. Anticiper les évolutions – Identifier les tendances émergentes pour prendre de l’avance sur la concurrence. Améliorer la prise de décision – Adopter les bonnes technologies en connaissance de cause. Optimiser les performances – Tirer parti des nouvelles pratiques et outils pour améliorer la productivité et la qualité. Réduire les risques – Éviter d’investir dans des technologies obsolètes ou inadaptées.🔎 Comment faire une veille efficace ?✅ Suivre des sources fiables : Blogs spécialisés, sites d’éditeurs (Microsoft, AWS, Google Cloud), communautés GitHub, newsletters techniques, etc.✅ Participer aux événements : Conférences (Microsoft Ignite, .NET Conf), meetups, webinaires.✅ Tester les nouvelles technologies : Expérimenter avec des Proof of Concepts (PoC).✅ Échanger avec la communauté : Forums, LinkedIn, X/Twitter et Discord sont des mines d’informations.✅ Automatiser sa veille : Utiliser des outils comme Feedly, GitHub Trending et Twitter Lists.📡 Où je fais ma veille technologique ?🎥 Mes chaînes YouTube favoritesYouTube est une ressource incontournable pour apprendre et explorer de nouvelles technologies.Voici quelques créateurs de contenu que je suis : Nick Chapsas - Chaîne YouTube Milan Jovanović - Chaîne YouTube CodeOpinion (Derek Comartin) - Chaîne YouTube Shawn Wildermuth - Chaîne YouTube IAmTimCorey (Tim Corey) - Chaîne YouTube Zoran Horvat - Chaîne YouTube Fireship - Chaîne YouTube Patrick God - Chaîne YouTube Microsoft Visual Studio - Chaîne YouTube Microsoft Azure Developers - Chaîne YouTube Microsoft Developer - Chaîne YouTube Christopher Okhravi - Chaîne YouTube TechWorld with Nana - Chaîne YouTube👥 Experts et créateurs de contenu à suivreJe suis également plusieurs experts et créateurs de contenu sur des plateformes comme X, LinkedIn et d’autres réseaux sociaux. Ils partagent régulièrement des insights et des ressources sur le développement et les technologies Microsoft. Nick Chapsas : LinkedIn Milan Jovanović : LinkedIn Dave Callan : LinkedIn Oleg Kyrylchuk : LinkedIn Saeed Esmaeelinejad : LinkedIn Stefan Đokić : LinkedIn Nikola Knežević LinkedIn Mukesh Murugan : LinkedIn Nick Cosentino : LinkedIn Microsoft Visual Studio (Showcase) : LinkedIn GitHub (Entreprise) : LinkedIn Brigit Murtaugh : LinkedIn Rhea Patel : LinkedIn📚 Articles et blogs à suivreJ’utilise Feedly pour suivre et organiser mes lectures.Voici une liste de blogs et articles que je consulte régulièrement : Scott Hanselman’s Blog : Blog Thomas Claudius Huber : Blog Cezary Piątek Blog : Blog You’ve Been Haacked : Blog Jimmy Bogard : Blog Shawn Wildermuth’s Blog : Blog Martin Fowler : Blog Andrew Lock - .NET Escapades : Blog ploeh blog : Blog Flavio Copes : Blog Rick Strahl’s Web Log : Blog dotNetTips.com : Blog Dev Leader : Blog Jon Skeet’s coding : Blog CodeOpinion : Blog Visual Studio Blog : Blog Azure DevOps Blog : Blog Visual Studio Code - Code Editing. Redefined. : Blog Microsoft for Developers : Blog PowerShell Team : Blog Windows Blog : Blog Docker : Blog AWS News Blog : Blog Engineering@Microsoft : Blog AWS Architecture Blog : Blog Engineering at Meta : Blog Netflix TechBlog - Medium : Blog freeCodeCamp Programming Tutorials: Python, JavaScript, Git &amp; More : Blog Dropbox Tech Blog : Blog Engineering Blog : Blog Stack Overflow Blog : Blog The Airbnb Tech Blog - Medium : Blog The latest from GitHub’s engineering team - The GitHub Blog : Blog Pinterest Engineering Blog - Medium : Blog Stripe Blog : Blog Spotify Engineering : Blog Postman Blog : Blog Engineering at Slack : Blog Discord Inc. Press Releases : BlogEn conclusionCette liste n’est pas figée et évolue en fonction de mes intérêts et des tendances du marché. L’objectif est de rester à jour sur les meilleures pratiques et les innovations technologiques, notamment dans l’écosystème .NET et le cloud." }, { "title": "Mon parcours et conseils pour la relève", "url": "/posts/parcours-conseils-releve/", "categories": "", "tags": "", "date": "2025-02-13 18:58:00 -0500", "snippet": "Récemment, j’ai eu l’opportunité de participer à un webinaire destiné aux jeunes souhaitant découvrir les métiers des technologies de l’information. Cet échange a été l’occasion de répondre à plusi...", "content": "Récemment, j’ai eu l’opportunité de participer à un webinaire destiné aux jeunes souhaitant découvrir les métiers des technologies de l’information. Cet échange a été l’occasion de répondre à plusieurs questions sur mon parcours, mon rôle en tant qu’architecte logiciel, et les opportunités dans le domaine des TI. Voici un résumé des échanges qui, je l’espère, pourront inspirer celles et ceux qui envisagent une carrière en informatique.Pourquoi est-il important pour moi de participer à JeunesExplo ?En tant qu’architecte logiciel et responsable de l’axe « Pratiques logicielles et modernisation » au sein du Centre d’Excellence Azure chez Cofomo Québec, je crois fermement à l’importance de transmettre mon expérience et d’inspirer la nouvelle génération. Participer à JeunesExplo, c’est une occasion unique d’échanger avec des jeunes curieux et ambitieux, de leur montrer la diversité des carrières en TI, et de les encourager à explorer ce secteur en pleine transformation.L’informatique est omniprésente dans nos vies et impacte tous les secteurs : santé, finance, jeux vidéo, éducation, administration publique… Il est donc essentiel de faire découvrir ces opportunités et de démontrer que chacun peut y trouver sa place.Mon métier : Architecte logicielUne journée typique dans mon travailChaque journée commence par une mêlée quotidienne (daily scrum) avec mon équipe. C’est un moment clé où chacun partage sur quoi il travaille, évoque ses avancées et mentionne d’éventuels blocages nécessitant de l’aide. Cette synchronisation rapide permet d’assurer une bonne coordination et de déceler les obstacles dès le début de la journée.Ensuite, nous passons en revue les priorités pour nous assurer que nos objectifs restent bien alignés avec les attentes du projet et des parties prenantes.Mon rôle au quotidien implique : La conception d’architectures logicielles robustes et évolutives, en tenant compte des aspects de performance, de sécurité et de maintenabilité. La collaboration avec les développeurs pour résoudre des problèmes techniques, effectuer des revues de code et accompagner l’adoption de nouvelles technologies. Des tâches stratégiques, telles que la préparation de présentations, la coordination avec d’autres équipes ou encore l’analyse de performance, afin de garantir une expérience utilisateur optimale dans nos applications cloud.Cette approche me permet d’assurer à la fois un suivi opérationnel efficace et une vision globale pour l’évolution des projets.Les outils que j’utiliseMon travail au quotidien repose sur plusieurs outils, qui m’aident à concevoir des solutions performantes, collaborer avec mon équipe et assurer un haut niveau de qualité dans nos livrables.Voici quelques-uns de mes incontournables : Visual Studio – Mon IDE principal pour développer en .NET. Il offre un environnement riche pour écrire du code, exécuter des tests unitaires, profiler les performances et tirer parti de fonctionnalités avancées comme IntelliSense, l’intégration Git et Github Copilot. Azure DevOps – Utilisé pour la gestion des pipelines CI/CD, le suivi des tâches via les tableaux Kanban, et la collaboration sur le code avec les pull requests. Il nous permet d’automatiser les déploiements et d’assurer un développement fluide et structuré. BenchmarkDotNet – Indispensable pour mesurer et comparer les performances du code .NET. Je l’utilise notamment pour analyser l’impact des optimisations et valider les choix architecturaux avant leur mise en place. Windows Terminal – Mon terminal par défaut pour exécuter des commandes et scripts. Stryker.NET – Un outil que j’apprécie pour les essais de mutation, permettant d’améliorer la robustesse de nos tests unitaires et de s’assurer que notre code est réellement testé en profondeur.Quelle formation pour devenir architecte logiciel ?Le parcours typique passe par un baccalauréat en informatique ou en génie logiciel, mais ce n’est pas une voie obligatoire. J’ai moi-même suivi un parcours atypique, obtenant un diplôme d’études collégiales en informatique au Cégep de Matane.L’essentiel dans ce domaine, c’est l’auto-formation, la pratique sur des projets concrets, et la capacité à apprendre continuellement. Les certifications jouent un rôle clé dans plusieurs spécialisations, bien que leur importance varie selon le domaine. Certaines branches, comme la cybersécurité ou l’administration cloud, en font un véritable atout pour valider ses compétences, tandis que d’autres, comme le développement logiciel, mettent davantage l’accent sur l’expérience et les réalisations concrètes.Comment j’ai su que l’informatique était ma voie ?En secondaire 5, j’ai découvert la programmation grâce à un cours en Visual Basic 6. Très vite, j’ai été captivé par le fait de pouvoir écrire du code et voir un programme prendre vie sous mes yeux. Chaque nouvel exercice me motivait à en apprendre davantage et à expérimenter par moi-même.Vers la fin du cours, j’ai développé un tic-tac-toe où l’on pouvait jouer contre une intelligence artificielle basique. C’était un petit projet, mais il m’a permis de mieux comprendre la logique derrière le développement de logiciels et d’alimenter encore plus ma curiosité pour le domaine.Mon intérêt pour la matière s’est traduit par une belle reconnaissance : j’ai reçu le méritas en informatique cette année-là. Ce prix a renforcé ma confiance et confirmé que je voulais continuer dans cette voie.Les opportunités dans les TILes secteurs d’activités en informatique sont nombreux : Technologies de l’information : développement d’applications, cybersécurité, gestion d’infrastructures. Finance : plateformes de services bancaires, gestion des transactions. Santé : logiciels de suivi médical, gestion des dossiers patients. Éducation : outils pédagogiques numériques, plateformes d’apprentissage en ligne. Jeux vidéo : moteurs de jeu, intelligence artificielle. Secteur public : modernisation des services gouvernementaux.Les employeurs potentiels sont tout aussi variés : entreprises privées, startups, gouvernement, secteur autonome (freelance, consulting)…La diversité dans le secteur des TIBien que les hommes restent majoritaires, la mixité progresse grâce à des initiatives encourageant la participation des femmes en technologie. Des programmes de mentorat, des bourses et des événements dédiés contribuent à rendre le domaine plus inclusif.L’informatique gagne à être plus diversifiée, car la diversité des perspectives stimule l’innovation et la créativité.Conseils pour les futurs professionnels des TI Développe ta curiosité : l’informatique évolue vite, il faut aimer apprendre. Réseaute : cherche un mentor, participe à des événements, connecte-toi avec des professionnels sur LinkedIn. Travaille sur des projets personnels : crée des applications, contribue à des projets open source, participe à des hackathons. Familiarise-toi avec des technologies modernes : cloud, .NET, DevOps… Obtiens des certifications : elles sont un atout pour valider tes compétences.Avec détermination et persévérance, il est possible de réussir, même en suivant un chemin non conventionnel !ConclusionL’informatique est un domaine en constante évolution, offrant des opportunités variées et stimulantes dans de nombreux secteurs. Que ce soit par le développement logiciel, l’architecture des systèmes, la cybersécurité ou encore l’intelligence artificielle, chacun peut y trouver un chemin qui correspond à ses intérêts et ambitions.Mon parcours m’a appris que la curiosité, l’apprentissage continu et la persévérance sont des éléments clés pour réussir. Il n’existe pas de parcours unique : certains passent par l’université, d’autres, comme moi, empruntent des chemins différents. Ce qui compte, c’est la passion et l’envie d’évoluer.Si tu es intéressé par les technologies et que tu veux bâtir des solutions qui auront un impact, c’est un domaine fait pour toi. Lance-toi, explore, expérimente, et surtout, ne sous-estime jamais ta capacité à apprendre et à progresser." }, { "title": "Scripts utilitaires", "url": "/posts/scripts-utilitaires/", "categories": "outil-developpement", "tags": "dotnet", "date": "2025-02-03 19:00:00 -0500", "snippet": "Les scripts suivants sont conçus pour simplifier et automatiser diverses tâches courantes liées à l’administration d’une machine de développement.Ils couvrent un éventail d’opérations allant de la ...", "content": "Les scripts suivants sont conçus pour simplifier et automatiser diverses tâches courantes liées à l’administration d’une machine de développement.Ils couvrent un éventail d’opérations allant de la gestion de fichiers à la maintenance de dépôts Git, en passant par l’optimisation des performances de copie et le nettoyage de répertoires volumineux.Ces outils sont particulièrement utiles pour gérer des environnements de développement complexes et maintenir une machine propre et performante.Créer un fichier de taille exacteCe script permet de créer un fichier d’une taille exacte, ce qui peut être utile pour tester des systèmes de fichiers ou des transferts de données.Voici un exemple pour créer un fichier texte d’un 1 Mo :$file = New-Object -TypeName System.IO.FileStream -ArgumentList \"D:\\TestFile.txt\", \"Create\", \"ReadWrite\"$file.SetLength(1Mb)$file.Close()Ce fichier peut être utilisé pour des tests de performance ou pour vérifier le comportement de systèmes de fichiers avec des fichiers de taille spécifique. Notez toutefois que ce fichier n’aura pas de contenu valide ou de structure particulière, ce qui peut le rendre inapproprié pour certaines applications nécessitant des données significatives.Copier efficacement avec RobocopyRobocopy, un outil intégré à Windows, est souvent plus performant que l’Explorateur de fichiers pour copier des fichiers, surtout lorsqu’il s’agit de gros volumes de données. L’Explorateur de fichiers Windows ne tire pas pleinement parti de tous les cœurs du processeur, ce qui peut le rendre moins efficace pour les transferts de fichiers volumineux ou complexes. De plus, Robocopy utilise les ressources de manière plus optimale, offrant ainsi des vitesses de copie améliorées et une gestion des erreurs plus robuste, ce qui le rend particulièrement adapté pour les tâches de grande envergure.Voici un exemple d’utilisation :robocopy \"C:\\repertoireSource\" \"D:\\repertoireCible\" /E /r:0 /w:0Dans cet exemple : /E - copie les sous-répertoires, y compris les répertoires vides. /r:0 - fixe le nombre de tentatives de reprise à zéro, évitant ainsi les retards dus aux erreurs. /w:0 - définit le temps d’attente entre les tentatives à zéro, ce qui accélère encore le processus.Supprimer un répertoire avec un chemin trop longL’Explorateur de fichiers Windows peut rencontrer des problèmes lorsqu’il s’agit de supprimer des répertoires dont le chemin est trop long. En effet, Windows a une limite de 260 caractères pour les chemins de fichiers, ce qui peut rendre la suppression de tels répertoires compliquée directement depuis l’interface graphique.Pour contourner cette limitation, vous pouvez utiliser le script suivant avec Robocopy, qui permet de supprimer le contenu d’un répertoire même si le chemin est trop long :robocopy \"C:\\RepertoireVide\" \"C:\\RepertoireASupprimer\" /purgeCela peut être particulièrement pratique pour effectuer le ménage des répertoires node_modules, qui ont souvent des chemins de fichiers très longs en raison de la structure de leurs dépendances.Nettoyer vos référentiels Git de vos branches fusionnéesCe script automatise le nettoyage des branches fusionnées dans plusieurs référentiels Git. Il parcourt tous les répertoires du dossier courant, identifie ceux contenant un dépôt Git, synchronise les branches distantes, puis supprime les branches locales fusionnées, tout en préservant les branches principales comme master et main. C’est un outil pratique pour maintenir des référentiels propres et éviter l’encombrement avec des branches inutilisées.Exemple :.\\cleanbranch.ps1Supprimer les répertoires consommant beaucoup d’espaceIl est fréquent que certains répertoires tels que TestResults, StrykerOutput, node_modules, bin, et obj occupent une grande quantité d’espace disque. Il est donc utile d’effectuer un nettoyage régulier pour libérer de l’espace.Par défaut, le script cible ces répertoires qui sont connus pour consommer beaucoup d’espace. Vous pouvez soit confirmer la suppression de ces répertoires par défaut, soit sélectionner manuellement ceux que vous souhaitez supprimer. Une fois les répertoires choisis, le script s’occupe de les supprimer ainsi que tout leur contenu, en explorant également les sous-répertoires de manière récursive.Exemple :.\\cleanFolder.ps1Mettre à jour des dépôts GitCe script met à jour tous les dépôts Git dans un répertoire en exécutant git pull pour chacun. Il liste les sous-dossiers, affiche un message en vert pour chaque mise à jour, et effectue la synchronisation avec le dépôt distant.Exemple :.\\updateAll.ps1ConclusionEn intégrant ces scripts utilitaires à votre processus de travail, vous pouvez considérablement simplifier la gestion de votre machine de développement. Ils vous permettront d’optimiser les performances de votre environnement, de maintenir vos projets propres et organisés, et de gagner du temps en automatisant des processus répétitifs.Pour accéder à tous ces scripts, consultez le référentiel complet sur GitHub.💡Note : Utilisez Windows Terminal avec PowerShell Core pour éviter des problèmes d’encodage de caractères.N’hésitez pas à explorer et personnaliser ces outils en fonction de vos besoins spécifiques !" }, { "title": "Transition de Fluent Assertions vers une licence commerciale", "url": "/posts/changement-licence-fluentassertions/", "categories": "outil-developpement", "tags": "dotnet, fluentassertions", "date": "2025-01-15 11:00:00 -0500", "snippet": "Fluent Assertions, une des bibliothèques .NET les plus populaires pour écrire des assertions fluides et expressives dans les tests unitaires, a récemment franchi une étape importante en changeant d...", "content": "Fluent Assertions, une des bibliothèques .NET les plus populaires pour écrire des assertions fluides et expressives dans les tests unitaires, a récemment franchi une étape importante en changeant de modèle de licence. Depuis la version 8.0.0, la bibliothèque est soumise à une licence commerciale, ce qui signifie que les utilisateurs commerciaux doivent désormais acheter une licence pour continuer à l’utiliser.Une réaction communautaire : la naissance d’AwesomeAssertionsCe changement a suscité des réactions mitigées dans la communauté des développeurs. Beaucoup ont exprimé leur désir de continuer à utiliser une solution open source. En réponse à ces besoins, un projet communautaire nommé AwesomeAssertions a été lancé. Ce projet vise à offrir une alternative open source, sous licence Apache 2.0, permettant de bénéficier des fonctionnalités clés de Fluent Assertions sans les contraintes liées à une licence commerciale.Maintenir Fluent Assertions sous licence open sourcePour les développeurs qui souhaitent continuer à utiliser Fluent Assertions sans adopter la nouvelle licence commerciale, il est possible de verrouiller la version à la 7.0.0, la dernière version disponible sous l’ancienne licence open source.Voici comment procéder dans votre fichier de projet (.csproj) :&lt;PackageReference Include=\"FluentAssertions\" Version=\"[7.0.0]\" /&gt;Cela garantit que votre projet reste sur la version 7.0.0, évitant ainsi toute mise à jour involontaire vers une version soumise à la nouvelle licence.Alternatives disponiblesL’écosystème .NET propose plusieurs autres bibliothèques et solutions pour écrire des assertions dans les tests unitaires : Fluent Assertions 8.0.0 et versions ultérieures : Pour ceux qui n’ont pas de problème avec le modèle commercial, ces versions offrent des fonctionnalités avancées, avec un support prioritaire et des garanties additionnelles. AwesomeAssertions : Ce projet communautaire en cours de développement sous licence Apache 2.0 offre une alternative open source aux versions commerciales de Fluent Assertions. Shouldly : Une bibliothèque d’assertions connue pour ses messages d’erreur lisibles et informatifs en cas d’échec d’une assertion, tout en fournissant une syntaxe simple et intuitive. TUnit : Un framework de test moderne et performant pour .NET 8 et versions ultérieures. Bien qu’il soit principalement conçu comme un framework de test complet, il inclut des fonctionnalités d’assertion. Assertions natives : Utiliser des assertions telles que Assert.Equal ou Assert.True directement via le framework de test par défaut. En savoir plusPour ceux qui souhaitent mieux comprendre le contexte, Nick Chapsas a publié une vidéo analysant ce changement et ses implications.ConclusionLe passage de Fluent Assertions à une licence commerciale marque un tournant dans l’écosystème .NET. Bien que cela puisse créer des contraintes pour certains utilisateurs, des alternatives comme AwesomeAssertions ou Shouldly offrent des solutions adaptées. Chaque équipe doit évaluer ses besoins en termes de licence, de support et de fonctionnalités pour choisir la meilleure option.💡 Restez à l’affût, je devrais republier bientôt pour partager l’orientation que nous aurons choisie." }, { "title": "Aide-mémoire - Code propre", "url": "/posts/aide-memoire-code-propre/", "categories": "bonne-pratique", "tags": "", "date": "2025-01-14 19:00:00 -0500", "snippet": "L’objectif de tout développeur est de produire du code qui soit non seulement fonctionnel, mais aussi propre, facile à lire et à maintenir. Un code propre permet non seulement d’améliorer la collab...", "content": "L’objectif de tout développeur est de produire du code qui soit non seulement fonctionnel, mais aussi propre, facile à lire et à maintenir. Un code propre permet non seulement d’améliorer la collaboration entre les membres de l’équipe, mais aussi d’assurer la pérennité du projet à long terme. Chaque choix de conception que vous faites peut avoir un impact significatif sur la maintenabilité de votre code, c’est pourquoi il est crucial de bien comprendre ces principes pour élever la qualité de vos projets.Le code propreLe code est propre s’il peut être compris facilement par toutes les personnes qui auront à travailler avec celui-ci. Un code propre peut être lu et amélioré aisément par un autre développeur que son auteur d’origine. Avec la compréhensibilité viennent la lisibilité, la possibilité de changement, l’extensibilité et la maintenabilité.💡Note : L’importance est d’en prendre connaissance plus comme des orientations communes que comme des règles à respecter à tout prix.Règles générales Suivez les conventions de codage C# Suivez les conventions générales d’affectation de noms Pensez SOLID Pensez KISS (Keep it simple stupid). Plus simple est toujours mieux. Réduisez au maximum la complexité. Suivez-la règle du boy-scouts (The Boy Scout Rule). Toujours laisser un endroit dans un état meilleur que celui où vous l’avez trouvé. Cherchez toujours la cause première d’un problème.Règles de conception Conservez les données configurables à des niveaux élevés. Préférez le polymorphisme à if/else ou switch/case. Séparez le code multithread. Empêchez la surconfigurabilité (over-configurability). Utilisez l’injection de dépendance. Suivez-la loi de Déméter. Une classe ne doit connaître que ses dépendances directes.Conseils de compréhension Être cohérent. Si vous faites quelque chose d’une certaine manière, suivez ce patron par la suite. Utilisez des variables explicatives. Encapsulez les conditions aux cas limites (boundary conditions). Les conditions aux cas limites sont difficiles à suivre. Mettez le traitement pour eux en un seul endroit. Préférez les objets de valeur (value objects) dédiés au type primitif. Évitez les dépendances logiques. N’écrivez pas de méthodes qui fonctionnent correctement en fonction de quelque chose d’autre dans la même classe. Évitez les conditions négatives.Conventions de nommage Choisissez des noms descriptifs et sans ambiguïté. Utilisez le français, sauf si les directives de la compagnie indiquent de coder en anglais. Il n’est cependant pas nécessaire de traduire des termes techniques spécifiques au framework ou un patron de conception tel qu’un “repository”. Faire une distinction significative. Utilisez des noms prononçables. Utilisez des noms consultables. Remplacez les nombres magiques par des constantes nommées. Évitez les encodages. N’ajoutez pas de préfixes ni d’informations de type. Évitez de mettre les acronymes tels que “XML” en majuscule, favorisez le format “Xml”.Règles pour les fonctions Petites. Font qu’une chose. Utilisez des noms descriptifs. Préférez moins d’arguments. N’ont pas d’effets secondaires. N’utilisez pas d’arguments de type flags. Divisez la méthode en plusieurs méthodes indépendantes qui peuvent être appelées depuis le client sans l’indicateur.Règles pour les commentaires Essayez toujours de vous expliquer en code. Ne soyez pas redondant. N’ajoutez pas de bruit évident. N’utilisez pas d’accolade fermante pour commenter. Ne commentez pas le code. Retirez-le simplement. Utilisez comme explication l’intention. Utilisez comme clarification du code. Utilisez comme avertissement des conséquences.Structure du code source Séparez les concepts verticalement. Le code associé doit apparaître verticalement dense. Déclarez les variables proches de leur utilisation. Les fonctions dépendantes doivent être proches. Des fonctions similaires doivent être proches. Placez les fonctions dans le sens descendant. Gardez les lignes courtes. N’utilisez pas l’alignement horizontal. Utilisez un saut de ligne pour associer des éléments liés et dissocier des éléments faiblement liés. Ne cassez pas l’indentation.Objets et structures de données Masquez la structure interne. Préférez les structures de données. Évitez les structures hybrides (moitié objet et moitié données). Devrait être petit. Fait qu’une chose. Petit nombre de variables. La classe de base ne doit rien savoir de ses dérivés. Vaut mieux avoir plusieurs fonctions que de passer du code dans une fonction pour sélectionner un comportement. Préférez les méthodes non statiques aux méthodes statiques.Essais Une assertion logique par test Lisible Rapide Indépendant RépétableCode smells Rigidité - Le logiciel est difficile à changer. Un petit changement provoque une cascade de changements. Fragilité - Le logiciel tombe en panne à de nombreux endroits en raison d’un seul changement. Immobilité - Vous ne pouvez pas réutiliser des parties du code dans d’autres - projets en raison des risques impliqués et des efforts importants. Complexité inutile. Répétition inutile. Opacité. Le code est difficile à comprendre.💡Note : Au besoin, référez-vous à cet article complet pour plus de détails.ConclusionEn conclusion, adopter des pratiques de code propre n’est pas seulement une bonne pratique, mais une nécessité dans le développement logiciel moderne. En intégrant les principes évoqués dans cet article, vous favoriserez non seulement la lisibilité et la maintenabilité de votre code, mais vous contribuerez également à l’efficacité de votre équipe. Un code bien structuré permet de réduire les erreurs et de faciliter les mises à jour futures, tout en rendant l’intégration de nouveaux développeurs plus fluide. En fin de compte, investir dans la propreté du code est un investissement dans la durabilité de vos projets, garantissant ainsi leur succès à long terme. Rappelez-vous que la quête d’un code propre est un processus continu qui demande réflexion, pratique et engagement.Références https://www.perforce.com/blog/sca/what-code-quality-overview-how-improve-code-quality https://gist.github.com/wojteklu/73c6914cc446146b8b533c0988cf8d29" }, { "title": "Introduction aux Testcontainers", "url": "/posts/introduction-testcontainers/", "categories": "outil-developpement", "tags": "essais, testcontainers", "date": "2024-12-12 19:00:00 -0500", "snippet": "Les systèmes logiciels modernes s’attaquent à des problèmes complexes en utilisant une multitude de technologies et d’outils. Rarement un système logiciel fonctionne de manière isolée; il interagit...", "content": "Les systèmes logiciels modernes s’attaquent à des problèmes complexes en utilisant une multitude de technologies et d’outils. Rarement un système logiciel fonctionne de manière isolée; il interagit généralement avec des bases de données, des systèmes de messagerie, des fournisseurs de cache, et de nombreux autres services tiers. Dans ce marché hautement concurrentiel, le temps de mise en marché est crucial. Les entreprises souhaitent lancer leur produit rapidement, obtenir des retours et itérer en conséquence. Pour atteindre cette agilité, il est essentiel d’avoir un processus solide d’intégration et de déploiement continus (CI/CD). Un élément clé de ce processus sont les tests automatisés, qui garantissent le bon fonctionnement de l’application.Les tests unitaires permettent de vérifier la logique métier et les détails d’implémentation en isolant les services externes, mais la majeure partie du code applicatif réside dans l’intégration avec ces services. Pour avoir une confiance totale dans notre application, il est essentiel d’écrire des tests d’intégration en plus des tests unitaires, afin d’assurer la fonctionnalité complète de l’application.Historiquement, les tests d’intégration sont considérés comme difficiles en raison des défis liés à la maintenance d’un environnement de test d’intégration. La mise en place de tests d’intégration avec une infrastructure préconfigurée pose plusieurs problèmes, notamment la nécessité de garantir que l’infrastructure est opérationnelle et que les données sont préconfigurées dans un état désiré. De plus, l’exécution de plusieurs pipelines de construction (build) en parallèle peut interférer avec d’autres données de test, entraînant des tests instables.Face à ces défis, certains développeurs se tournent vers des services en mémoire ou des variations embarquées des services requis pour les tests d’intégration. Par exemple, un développeur pourrait utiliser une base de données en mémoire en remplacement de Microsoft SQL Server. Bien que cela soit une amélioration par rapport à l’absence de tests d’intégration, l’utilisation de simulations ou de versions en mémoire présente ses propres problèmes. Les services en mémoire peuvent ne pas avoir toutes les fonctionnalités des services en production !C’est dans ce contexte que les Testcontainers entre en jeu, offrant une solution innovante pour les tests d’intégration avec de véritables services, rendant cette tâche aussi simple que l’écriture de tests unitaires. En utilisant les Testcontainers, les développeurs peuvent créer des environnements de test reproduisant fidèlement leurs systèmes de production, tout en bénéficiant d’une rétroaction rapide et efficace sur leurs modifications.Qu’est-ce qu’un TestcontainerTestcontainers est une bibliothèque de test qui offre des API pour faciliter la mise en place de tests d’intégration avec des services réels, encapsulés dans des conteneurs Docker. Grâce à Testcontainers, il est possible d’écrire des tests qui interagissent avec le même type de services que ceux utilisés en production, éliminant ainsi la nécessité d’utiliser des simulations ou des services en mémoire.Un test d’intégration typique basé sur Testcontainers fonctionne de la manière suivante : Avant les tests :  Vous démarrez vos services requis (bases de données, systèmes de messagerie, etc.) en utilisant l’API de Testcontainers pour lancer des conteneurs Docker. Configuration : Vous configurez ou mettez à jour la configuration de votre application pour qu’elle utilise ces services conteneurisés. Pendant les tests : Vos tests s’exécutent en utilisant ces services conteneurisés, ce qui garantit un environnement de test qui reflète fidèlement la production. Après les tests : Testcontainers s’occupe de détruire ces conteneurs, que les tests aient réussi ou échoué.La seule exigence pour exécuter des tests basés sur Testcontainers est d’avoir un environnement d’exécution de conteneurs compatible avec l’API Docker. Si vous avez Docker Desktop installé et en cours d’exécution, tout est prêt. Pour plus d’informations sur les environnements Docker pris en charge par Testcontainers, vous pouvez consulter la documentation officielle.Qu’est-ce que les Testcontainers règlent ?Les Testcontainers résolvent plusieurs problèmes liés aux tests d’intégration en permettant aux développeurs de tester leur application avec de véritables services, ce qui augmente la confiance dans les modifications apportées au code.Voici quelques points clés sur ce que Testcontainers améliore : Infrastructure de test préconfigurée : Avec Testcontainers, il n’est pas nécessaire d’avoir une infrastructure de test d’intégration préalablement configurée. L’API de Testcontainers fournit automatiquement les services nécessaires avant l’exécution des tests. Cela signifie que la définition de l’infrastructure se trouve directement à côté du code de test, facilitant ainsi la maintenance et la compréhension. Isolation des données : Testcontainers élimine les problèmes de conflit de données, même lorsque plusieurs pipelines de construction (build) s’exécutent en parallèle. Chaque pipeline fonctionne avec un ensemble de services isolés, ce qui prévient les interférences entre les tests. Exécution depuis l’IDE : Les développeurs peuvent exécuter leurs tests d’intégration directement depuis leur IDE, tout comme pour les tests unitaires, sans avoir à pousser les modifications et attendre que l’intégration continue (CI) exécute les tests. Cela améliore le flux de travail et accélère le cycle de rétroaction. Nettoyage automatique : Après l’exécution des tests, Testcontainers s’occupe automatiquement de la suppression des conteneurs, ce qui simplifie la gestion des ressources et réduit le risque d’accumulation de services inutilisés sur le système. Compatibilité avec plusieurs langages : Testcontainers peut être utilisé avec de nombreux langages de programmation populaires, notamment .NET, Java, Go, NodeJS, Rust et Python, avec davantage de supports de langages à venir.En résumé, Testcontainers fournit une solution pratique et efficace pour les tests d’intégration, permettant aux équipes de développement de gagner en confiance et en agilité dans leur processus de développement.DémonstrationConclusionEn conclusion, nous avons examiné les défis inhérents aux tests d’intégration, en mettant en lumière les limitations de l’utilisation de mocks ou de services en mémoire, qui peuvent entraîner des incohérences entre les environnements de test et de production. Testcontainers offre une solution puissante à ces problèmes, permettant aux développeurs de réaliser des tests d’intégration avec de véritables services dans des conteneurs Docker isolés. Cela renforce non seulement la fiabilité des tests, mais streamline également le processus de test, permettant un retour d’information plus rapide et une plus grande confiance dans les modifications de code.Pour des informations supplémentaires et une documentation détaillée sur la mise en œuvre de Testcontainers dans vos projets, n’hésitez pas à visiter Testcontainers." }, { "title": "Optimisation des Expressions Régulières avec les Générateurs de Sources dans .NET", "url": "/posts/regex-generateurs-sources-fluentvalidation/", "categories": "", "tags": "dotnet, fluentvalidation, source-generator", "date": "2024-12-01 19:00:00 -0500", "snippet": "Avec .NET, les générateurs de sources pour les expressions régulières (Regex Source Generators) permettent d’optimiser la création et l’exécution des expressions régulières en générant du code sour...", "content": "Avec .NET, les générateurs de sources pour les expressions régulières (Regex Source Generators) permettent d’optimiser la création et l’exécution des expressions régulières en générant du code source au moment de la compilation. Ces générateurs, introduits à partir de .NET 7, utilisent l’attribut [GeneratedRegex] pour définir des expressions régulières de manière déclarative.L’objectif est d’améliorer les performances en éliminant les étapes coûteuses de compilation des expressions régulières au moment de l’exécution. Au lieu de créer une instance de Regex dynamique, le générateur produit un code précompilé, offrant ainsi une solution plus rapide et adaptée aux scénarios exigeants. 💡 Pour plus de détails, consultez la documentation officielle : Regular Expression Source Generators.FluentValidation et les Générateurs de SourcesOn peut légitimement se demander si FluentValidation utilise les générateurs de sources pour optimiser l’instruction Matches(\"^[a-zA-Z0-9]*$\");. Pour explorer cette question, j’ai conçu un benchmark comparant deux implémentations de validation : l’une utilisant des expressions régulières classiques et l’autre tirant parti des générateurs de sources avec [GeneratedRegex].Benchmark ComparatifVoici le code du benchmark :using BenchmarkDotNet.Attributes;using BenchmarkDotNet.Columns;using BenchmarkDotNet.Order;using FluentValidation;using System.Text.RegularExpressions;namespace TestBenchmarkDotnet.TestRegexFluentValidation;[MemoryDiagnoser][Orderer(SummaryOrderPolicy.FastestToSlowest)][RankColumn][HideColumns(Column.Error, Column.Median, Column.RatioSD, Column.StdDev)]public partial class BenchmarkRegexFluentValidation{ private ValidatorSansGeneratedRegex _validatorSansGeneratedRegex; private ValidatorAvecGeneratedRegex _validatorAvecGeneratedRegex; private TestModel _instance; [GlobalSetup] public void GlobalSetup() { _validatorSansGeneratedRegex = new(); _validatorAvecGeneratedRegex = new(); _instance = new() { Value = \"TestString123\" }; } [Benchmark(Baseline = true)] public void ValidationSansGeneratedRegex() =&gt; _ = _validatorSansGeneratedRegex.Validate(_instance).IsValid; [Benchmark] public void ValidationAvecGeneratedRegex() =&gt; _ = _validatorAvecGeneratedRegex.Validate(_instance).IsValid; public class TestModel { public required string Value { get; set; } } public class ValidatorSansGeneratedRegex : AbstractValidator&lt;TestModel&gt; { public ValidatorSansGeneratedRegex() { RuleFor(x =&gt; x.Value).Matches(\"^[a-zA-Z0-9]*$\"); } } public partial class ValidatorAvecGeneratedRegex : AbstractValidator&lt;TestModel&gt; { public ValidatorAvecGeneratedRegex() { RuleFor(x =&gt; x.Value).Matches(MyRegex()); } // L'option `RegexOptions.Compiled` est ignorée par le générateur et donc non nécessaire. [GeneratedRegex(@\"^[a-zA-Z0-9]*$\")] private static partial Regex MyRegex(); }}Analyse des résultats| Method | Mean | Ratio | Rank | Gen0 | Allocated | Alloc Ratio ||----------------------------- |---------:|------:|-----:|-------:|----------:|------------:|| ValidationAvecGeneratedRegex | 157.3 ns | 0.67 | 1 | 0.0715 | 600 B | 1.00 || ValidationSansGeneratedRegex | 234.7 ns | 1.00 | 2 | 0.0715 | 600 B | 1.00 | Temps d’exécution :L’utilisation des générateurs de sources réduit le temps d’exécution de la validation d’environ 33 %, passant de 234,7 ns à 157,3 ns. Cette amélioration est due à l’optimisation apportée par le code précompilé généré au moment de la compilation. Allocations mémoire :Les allocations mémoire restent identiques dans les deux cas (600 B). Cela montre que l’amélioration des performances est principalement due à la réduction des calculs nécessaires pour traiter les expressions régulières. Classement :Le benchmark classe clairement la validation avec générateurs de sources comme la méthode la plus rapide. Ces résultats démontrent que l’intégration des générateurs de sources peut apporter des bénéfices significatifs en termes de performance, particulièrement pour des validations répétées ou critiques en production.ConclusionLes générateurs de sources pour les expressions régulières représentent une avancée importante dans l’écosystème .NET. En utilisant l’attribut [GeneratedRegex], les développeurs peuvent bénéficier d’améliorations notables en termes de vitesse d’exécution, tout en conservant une syntaxe déclarative et lisible.Bien que FluentValidation ne supporte pas directement les générateurs de sources dans ses règles comme Matches, l’approche manuelle démontrée ici peut être utilisée pour combiner les avantages des deux outils.Les développeurs souhaitant optimiser leurs applications .NET devraient envisager d’adopter cette technique, notamment dans les scénarios où des expressions régulières complexes ou souvent utilisées jouent un rôle clé." }, { "title": "Retour sur Microsoft Ignite 2024", "url": "/posts/microsoft-ignite-2024/", "categories": "", "tags": "dotnet, conference", "date": "2024-12-01 19:00:00 -0500", "snippet": "L’événement phare de Microsoft a mis l’accent sur des thématiques majeures : l’intelligence artificielle (IA), la cybersécurité et la collaboration. L’édition de cette année a présenté des outils e...", "content": "L’événement phare de Microsoft a mis l’accent sur des thématiques majeures : l’intelligence artificielle (IA), la cybersécurité et la collaboration. L’édition de cette année a présenté des outils et solutions destinés à transformer les entreprises dans un monde numérique en rapide évolution.1. Azure AI FoundryUne plateforme intégrée pour concevoir, entraîner et déployer des modèles d’IA personnalisés : Permet aux entreprises de personnaliser des modèles comme GPT pour leurs propres données. Cas pratique : Une banque européenne a augmenté ses ventes croisées de 25 % grâce à l’analyse de données client en temps réel.2. Copilot EverywhereUne extension de l’IA dans les outils quotidiens comme Microsoft Teams, Power Platform et des plateformes tierces : Résumés automatiques, génération d’applications low-code et intégration avec des solutions comme Salesforce. Impact mesurable : Une entreprise a réduit de 30 % le temps consacré à des tâches administratives.3. Zero Day QuestUn programme de détection proactive des vulnérabilités pour renforcer la cybersécurité mondiale : Collaboration avec des chercheurs en sécurité, des universités et des gouvernements pour détecter les menaces. Résultat : Réduction des incidents liés aux menaces internes de 40 % dans une société d’assurance.4. Focus sur l’inclusivité et l’éthique Accessibilité accrue dans les outils collaboratifs. Garde-fous intégrés dans l’IA pour garantir des résultats fiables.Microsoft a réaffirmé sa mission : rendre l’IA accessible tout en assurant la sécurité et la résilience nécessaires pour prospérer dans ce monde numérique.SQL Server 2025SQL Server 2025 marque une avancée significative pour intégrer l’IA au cœur des données d’entreprise : Stockage vectoriel natif et gestion des modèles IA directement dans le moteur SQL pour réduire les latences et simplifier l’orchestration. Sécurité renforcée avec cryptage en temps réel et conformité stricte aux normes (GDPR, HIPAA). SQL Server Management Studio (SSMS) 21 Preview : Migration vers Visual Studio 2022, support 64 bits, thème sombre et Copilot intégré pour simplifier la rédaction de requêtes T-SQL. Ces innovations offrent des gains en performance, une meilleure gestion des coûts et une modernisation adaptée aux besoins des entreprises modernes.Azure DevOps et GitHubMicrosoft a clarifié sa vision sur l’avenir d’Azure DevOps et GitHub, en renforçant leur intégration pour répondre à une large diversité de besoins : GitHub Advanced Security : Disponible dans Azure DevOps pour détecter les secrets dans le code et surveiller les dépendances. GitHub Copilot : Accessibilité dans Azure DevOps pour des suggestions de code contextuelles et un gain de productivité accru. Azure Boards et GitHub : Intégration renforcée permettant une gestion fluide des éléments de travail. Azure Pipelines : Des pipelines CI/CD adaptatifs pour les projets GitHub.Microsoft ne cherche pas à opposer Azure DevOps et GitHub, mais plutôt à renforcer leur complémentarité.Résumé : GitHub et Azure DevOps ne sont pas des concurrents, mais des alliés, offrant une double offre stratégique pour simplifier les processus, sécuriser les applications et améliorer la performance.Azure Kubernetes Service (AKS) et Azure Managed RedisMicrosoft a mis en avant les améliorations de Azure Kubernetes Service (AKS) et Azure Managed Redis, essentiels pour les applications modernes et l’IA générative.1. Azure Kubernetes Service (AKS) Fleet Manager : Orchestration centralisée des clusters Kubernetes sur plusieurs environnements. Auto-provisionnement des nœuds : Allocation dynamique des ressources pour optimiser les coûts et les performances. Trusted Launch : Sécurité renforcée avec vérification de l’intégrité des workloads critiques.2. Azure Managed Redis Optimisé pour les charges d’IA générative (GenAI) avec des latences réduites et une gestion avancée des données en mémoire. Idéal pour stocker des embeddings, des données contextuelles et des résultats de modèles génératifs. API simplifiées pour une intégration rapide dans les pipelines d’IA.Avec ces outils, Microsoft simplifie la gestion des environnements complexes tout en garantissant scalabilité et sécurité.GitHub CopilotGitHub Copilot révolutionne le développement avec des fonctionnalités clés : Complétion contextuelle avancée et refactoring automatisé pour simplifier le travail des développeurs. Suggestions de sécurité et génération de documentation intelligente pour améliorer la qualité et la fiabilité du code. Possibilité d’ajouter des instructions spécifiques pour guider Copilot.Impact mesurable : Les entreprises comme Accenture rapportent une réduction de 30 % du temps consacré aux tâches répétitives grâce à Copilot, permettant aux équipes de se concentrer sur l’innovation.ConclusionMicrosoft Ignite 2024 a confirmé son engagement envers une transformation numérique responsable et performante. Entre IA, cybersécurité, et complémentarité entre Azure DevOps et GitHub, l’avenir est clair : des outils puissants, sécurisés, et accessibles pour toutes les entreprises, quelle que soit leur taille ou leur secteur." }, { "title": "Le mois clé des innovations chez Microsoft", "url": "/posts/innovations-microsoft-2024/", "categories": "", "tags": "conference", "date": "2024-12-01 19:00:00 -0500", "snippet": "Installez-vous confortablement, peut-être avec une bonne tasse de café, car ce résumé regorge d’informations passionnantes et détaillées pour explorer les dernières innovations.Le mois de novembre ...", "content": "Installez-vous confortablement, peut-être avec une bonne tasse de café, car ce résumé regorge d’informations passionnantes et détaillées pour explorer les dernières innovations.Le mois de novembre est désormais une période clé pour les annonces technologiques, marquée par deux événements majeurs organisés par Microsoft : .NET Conf et Microsoft Ignite..NET Conf 2024.NET Conf 2024 a eu lieu du 12 au 14 novembre, est une conférence annuelle dédiée à l’écosystème .NET, où les développeurs découvrent les dernières innovations et mises à jour autour de .NET.C’est un événement gratuit, entièrement en ligne, qui propose des sessions techniques, des démonstrations, et des présentations d’experts.Points clés de .NET Conf : Focus sur .NET 9 : Mise en avant des performances et des outils de développement. Multiplateforme : Sessions couvrant le développement web, mobile, cloud, et plus encore. Accessibilité : Un incontournable pour ceux qui souhaitent rester à la pointe des meilleures pratiques. 🎥 Astuce : Vous pouvez retrouver toutes les sessions de .NET Conf 2024 sur la playlist officiellee.Microsoft Ignite 2024Microsoft Ignite, quant à lui, a eu lieu du 18 au 22 novembre à Chicago. Il s’agit d’une conférence majeure pour les professionnels TI et les décideurs technologiques, couvrant une gamme plus large de technologies Microsoft, incluant Azure, Microsoft 365, et la cybersécurité.Points clés de Microsoft Ignite : Découverte des innovations majeures dans le cloud, la productivité et la transformation numérique. Présentation des dernières avancées dans Azure et Microsoft 365. Un lieu stratégique pour comprendre les enjeux technologiques futurs. 🎥 Astuce : Retrouvez toutes les sessions sur la playlist officielle .Synergie des événementsCes deux événements, organisés en novembre, créent une synergie stratégique : ils permettent à Microsoft de synchroniser les lancements de produits comme .NET, Visual Studio, et Azure, tout en touchant des audiences complémentaires.Bien que certains participants trouvent cette concentration d’événements intense, elle permet de faire des annonces clés au moment opportun pour les décisions budgétaires de l’année suivante.En combinant ces événements, Microsoft transforme novembre en un mois de découvertes technologiques, générant un enthousiasme considérable dans la communauté des développeurs et professionnels TI.Cycle de versionnement en .NETAvant d’analyser les nouveautés annoncées, il est crucial de bien comprendre le cycle de versionnement en .NET, car cela permet de situer chaque version dans son contexte de support et d’innovation, et d’orienter ses choix en conséquence.Depuis la sortie de .NET 6, Microsoft a introduit un cycle de versionnement régulier qui alterne entre des versions LTS (Long-Term Support) et STS (Standard-Term Support). Ce schéma vise à offrir un équilibre entre : La stabilité pour les applications à long terme. La possibilité d’accéder rapidement aux nouvelles fonctionnalités.Détails sur les versions : Versions LTS : Publiées tous les deux ans, elles bénéficient de trois ans de support.Idéales pour les applications en production nécessitant de la stabilité. Versions STS : Publiées entre deux versions LTS, elles incluent les dernières avancées en termes de fonctionnalités et de performances.Support limité à 18 mois, encourageant les migrations rapides.Ce modèle assure un rythme d’innovation rapide tout en offrant aux entreprises la flexibilité de choisir la version qui correspond le mieux à leurs besoins.Devriez-vous toujours cibler la version la plus récente du framework ?Non, il n’est pas toujours nécessaire d’utiliser la version la plus récente du framework. Cela dépend de plusieurs facteurs liés à la stabilité, au support, et aux nouvelles fonctionnalités.Éléments clés à prendre en compte : Stabilité et support (LTS vs STS) : Pour les applications nécessitant stabilité et support prolongé, privilégiez les versions LTS. Si vous souhaitez bénéficier des dernières avancées, une version STS peut être envisagée, malgré son support plus court. Compatibilité des dépendances : Assurez-vous que les bibliothèques ou packages de votre projet sont compatibles avant toute mise à jour. Nouvelles fonctionnalités : Si une version introduit des optimisations ou améliorations cruciales pour votre projet, cela peut justifier une migration. Effort de migration : Migrer vers une nouvelle version demande souvent des ajustements dans le code et des tests supplémentaires. Si la stabilité est prioritaire, une mise à jour peut être moins urgente. ConclusionEn conclusion, il est recommandé de choisir la version qui correspond le mieux aux besoins de votre projet, en privilégiant les LTS pour la stabilité, mais sans exclure les STS si les nouvelles fonctionnalités apportent un réel bénéfice.Microsoft transforme le mois de novembre en une période stratégique pour les découvertes technologiques, offrant une vision claire des innovations à venir dans .NET, Azure, et bien d’autres outils.Pour en savoir plus, découvrez mon article dédié à .NET Conf 2024 et celui sur Microsoft Ignite 2024." }, { "title": "Retour sur .NET Conf 2024", "url": "/posts/dotnet-conf-2024/", "categories": "", "tags": "dotnet, conference", "date": "2024-12-01 19:00:00 -0500", "snippet": ".NET 9 regorge de nouveautés pensées pour améliorer la performance, la lisibilité et offrir des outils modernes aux développeurs. Après une année à explorer les préversions, j’ai pris le temps de s...", "content": ".NET 9 regorge de nouveautés pensées pour améliorer la performance, la lisibilité et offrir des outils modernes aux développeurs. Après une année à explorer les préversions, j’ai pris le temps de synthétiser les points forts qui pourraient transformer votre façon de coder.Pour une approche visuelle, je recommande la vidéo récente de Nick Chapsas, qui propose un excellent aperçu. Et pour des détails techniques approfondis, les articles de Stephen Toub sont une ressource incontournable. ⚠️ Attention : Il est impossible de couvrir en détail toutes les nouveautés de .NET 9 dans un seul article. Cependant, pour une vue d’ensemble complète, je vous encourage à consulter les ressources officielles et les analyses approfondies mentionnées ici.1. LINQ : Toujours plus intuitifNouvelle méthode Index().NET 9 introduit Index(), qui rend le traitement des indices plus élégant et naturel :var resultat = maListe.Index().Select(x =&gt; $\"{x.Value} à l'index {x.Index}\");Dites adieu à l’utilisation d’un index manuel !Autres nouveautés : CountBy() et AggregateBy() CountBy() : Simplifie le comptage des occurrences par clé. AggregateBy() : Facilite les agrégations sur des groupes de données.2. UUID v7 : Identifiants uniques temporellement ordonnésAvec UUID v7, Microsoft répond aux besoins des systèmes modernes nécessitant un tri temporel efficace.Ces identifiants : Incluent un horodatage en millisecondes. Restent conformes aux standards tout en améliorant la compatibilité avec des scénarios actuels. S’utilisent facilement avec Guid.CreateVersion7().Une solution idéale pour les journaux, les bases de données ou toute application axée sur l’ordre chronologique.3. Gestion optimisée des chaînes avec Span et SearchValuesSplit sans allocationsDésormais, vous pouvez découper des chaînes en utilisant Span, réduisant ainsi les allocations mémoire :var fruits = \"pomme,banane,fraise\";foreach (var fruit in fruits.AsSpan().Split(',')){ Console.WriteLine(fruit.ToString());}Recherches avancéesGrâce à SearchValues : ContainsAny : Identifiez rapidement si une valeur fait partie de la collection. IndexOfAny : Localisez efficacement les éléments recherchés.4. Nouvelle gestion de la synchronisation avec System.Threading.Lock.NET 9 introduit un mécanisme de verrouillage dédié, plus lisible et fiable que les approches traditionnelles :private readonly Lock _lock = new Lock();lock (_lock){ // Section critique} Lisibilité accrue : le code exprime clairement son intention. Réduction des erreurs grâce à une meilleure gestion des synchronisations complexes.5. Performances : Des gains concretsMicrosoft continue d’optimiser .NET, et cette version ne fait pas exception : Réduction des allocations mémoire avec des améliorations sur LINQ et Span. Compatibilité renforcée avec AOT (Ahead-of-Time), un atout pour les Minimal APIs et les environnements à ressources limitées.6. Swagger : Où est-il passé ?Une surprise pour plusieurs : Swagger n’est plus activé par défaut. Si cela vous perturbe, pas de panique ! Patrick God vous guide dans l’ajout de Swagger à vos projets.Pour en savoir plus 👉 Issue GitHubConclusionAvec .NET 9, Microsoft consolide sa vision d’un framework qui allie performance, lisibilité et innovation. Que vous soyez un adepte des dernières technologies ou simplement curieux d’améliorer vos pratiques, cette version offre de nombreux outils pour faire passer vos projets au niveau supérieur." }, { "title": "La performance de Mapperly", "url": "/posts/performance-mapperly/", "categories": "outil-developpement", "tags": "dotnet, source-generator, mapperly", "date": "2024-11-25 16:00:00 -0500", "snippet": "La semaine dernière, j’ai eu la chance de participer à Microsoft Ignite à Chicago. Cet événement riche en annonces et en échanges m’a inspiré plusieurs sujets que je partagerai sur mon blog dans le...", "content": "La semaine dernière, j’ai eu la chance de participer à Microsoft Ignite à Chicago. Cet événement riche en annonces et en échanges m’a inspiré plusieurs sujets que je partagerai sur mon blog dans les jours à venir !Dans un article précédent, j’ai mentionné les avantages de l’utilisation des Source Generators. J’avais également présenté un exemple avec Mapperly, un outil qui utilise les Source Generators pour effectuer le mapping entre deux entités, évitant ainsi la réflexion.Curieux de comparer les performances entre AutoMapper, un outil fréquemment utilisé dans mes mandats, et Mapperly, j’ai décidé de réaliser quelques benchmarks !Mise en place de l’environnementJ’ai commencé par créer un projet .NET 9 et j’ai ajouté la structure de base pour utiliser BenchmarkDotNet.J’ai défini une classe Personne et un DTO PersonneDto :public class Personne{ public required string Prenom { get; set; } public required string Nom { get; set; } public required int Age { get; set; } public required string Adresse { get; set; } public required string Ville { get; set; } public required string Province { get; set; }}public class PersonneDto{ public required string Prenom { get; set; } public required string Nom { get; set; } public required string Adresse { get; set; } public required string Ville { get; set; } public required string Province { get; set; }}Pour Mapperly, j’ai créé le mapper suivant :[Mapper]public partial class PersonneMapper{ public partial PersonneDto PersonneToPersonneDto(Personne personne);}Configuration des benchmarksJ’ai configuré mes benchmarks pour itérer sur 1, 100 et 1 000 fois :[MemoryDiagnoser][Orderer(SummaryOrderPolicy.FastestToSlowest)][RankColumn][HideColumns(Column.Error, Column.Median, Column.RatioSD, Column.StdDev)]public class BenchmarkMapperlyAutomapper{ private IMapper _automapper; private PersonneMapper _mapperly; private Personne _personne; [GlobalSetup] public void GlobalSetup() { _automapper = new MapperConfiguration(cfg =&gt; cfg.CreateMap&lt;Personne, PersonneDto&gt;()).CreateMapper(); _mapperly = new PersonneMapper(); _personne = new Fixture().Create&lt;Personne&gt;(); } [Params(1, 100, 1_000)] public int Iteration { get; set; } [Benchmark(Baseline = true)] public List&lt;PersonneDto&gt; AvecAutomapper() { var personnes = new List&lt;PersonneDto&gt;(); for (var i = 0; i &lt; Iteration; i++) { var personneDto = _automapper.Map&lt;PersonneDto&gt;(_personne); personnes.Add(personneDto); } return personnes; } [Benchmark] public List&lt;PersonneDto&gt; AvecMapperly() { var personnes = new List&lt;PersonneDto&gt;(); for (var i = 0; i &lt; Iteration; i++) { var personneDto = _mapperly.PersonneToPersonneDto(_personne); personnes.Add(personneDto); } return personnes; }}Résultats des benchmarksVoici les résultats obtenus :| Method | Iteration | Mean | Ratio | Rank | Gen0 | Gen1 | Allocated | Alloc Ratio ||--------------- |---------- |-------------:|------:|-----:|-------:|-------:|----------:|------------:|| AvecMapperly | 1 | 25.61 ns | 0.32 | 1 | 0.0172 | - | 144 B | 1.00 || AvecAutomapper | 1 | 80.79 ns | 1.00 | 2 | 0.0172 | - | 144 B | 1.00 || | | | | | | | | || AvecMapperly | 100 | 1,506.71 ns | 0.22 | 1 | 0.9308 | 0.0229 | 7792 B | 1.00 || AvecAutomapper | 100 | 6,938.45 ns | 1.00 | 2 | 0.9308 | 0.0153 | 7792 B | 1.00 || | | | | | | | | || AvecMapperly | 1000 | 14,079.26 ns | 0.20 | 1 | 8.6670 | 1.7242 | 72600 B | 1.00 || AvecAutomapper | 1000 | 70,748.49 ns | 1.01 | 2 | 8.6670 | 1.7090 | 72600 B | 1.00 |ConclusionLes résultats montrent clairement que Mapperly est beaucoup plus performant que AutoMapper. L’utilisation des Source Generators plutôt que de la réflexion porte ses fruits !La grande question reste : comment l’outil se comporte-t-il avec des structures plus complexes ? Restez à l’affut pour un prochain article !Pour en savoir plus sur Mapperly, consultez la documentation officielle." }, { "title": "L'importance d'un registre de décisions", "url": "/posts/registre-decisions/", "categories": "", "tags": "", "date": "2024-11-06 19:00:00 -0500", "snippet": "PréambuleJe ne pensais pas publier un article aujourd’hui, mais je me suis laissé inspirer ! La majorité ne le sait pas, mais j’ai changé de mandat depuis peu. Je me retrouve maintenant dans un min...", "content": "PréambuleJe ne pensais pas publier un article aujourd’hui, mais je me suis laissé inspirer ! La majorité ne le sait pas, mais j’ai changé de mandat depuis peu. Je me retrouve maintenant dans un ministère où les ressources et les pratiques ne sont pas toutes en place pour affronter des projets d’envergure informatique. Dans ce genre de contexte, on réalise rapidement que certaines choses que l’on prend pour acquises sont loin d’être la norme ! Ce n’est pas par manque de volonté ; plusieurs personnes dans l’organisation occupent des rôles multiples, remplissant parfois les tâches d’une autre ressource en plus de leur propre mission.Cette situation me rappelle une conférence de Roy Osherove, Team Leadership in the Age of Agile, où il explique qu’une équipe constamment en mode “survie” peine à surmonter les problèmes quotidiens. Impossible donc, pour une équipe dans cette situation, d’améliorer ses processus par elle-même ; c’est pourquoi il mentionne souvent qu’une intervention externe est indispensable pour aider l’équipe à sortir de cette impasse.Pour donner un peu de contexte, lors de mon dernier mandat, j’ai eu la chance d’être impliqué activement dans un groupe d’experts qui avait pour mission de définir des orientations, de développer et maintenir des outils communs, et surtout, de prévenir les problèmes avant même qu’ils n’apparaissent. Nous avons instauré, au fil du temps, divers processus, documentations et outils pour alléger les tâches des équipes de développement.C’est dans cette même optique que j’ai voulu continuer cette pratique dans mon nouveau mandat en recommandant la mise en place d’un registre de décisions !Mais pourquoi, allez-vous me demander ? C’est précisément ce que je veux explorer dans cet article 😉.Qu’est-ce qu’un registre de décisions ?Un registre de décisions est un document centralisé (parfois même un outil) où sont consignées toutes les décisions importantes prises dans le cadre d’un projet, qu’elles soient techniques, stratégiques ou organisationnelles. Chaque décision y est décrite avec son contexte, les options envisagées, les raisons du choix final, ainsi que la date et les participants à la prise de décision.Il permet de garder une trace claire de chaque orientation prise pour diverses raisons : justifications futures, onboarding des nouveaux membres de l’équipe ou, tout simplement, éviter de revenir sans cesse sur des sujets déjà tranchés.Ce registre n’a pas besoin d’être complexe, mais il doit être à jour et facilement accessible par l’équipe pour être utile. On peut utiliser un simple document partagé ou opter pour des outils de gestion plus avancés si l’ampleur du projet le justifie. Dans mon précédent mandat, par exemple, notre registre était intégré dans un wiki dans Azure DevOps.Les gainsLa mise en place d’un registre de décisions offre plusieurs avantages significatifs : Transparence et clarté : Toutes les décisions sont visibles et documentées, ce qui réduit les interprétations ou les malentendus. Gain de temps : Plus besoin de revisiter sans cesse les mêmes discussions ou de redébattre des choix déjà faits. L’équipe peut consulter le registre pour comprendre pourquoi une décision a été prise et se concentrer sur les prochaines étapes. Responsabilité et engagement : En documentant chaque décision avec les noms des participants, chacun s’investit davantage dans le processus, sachant que son avis est pris en compte et consigné. Traçabilité : Dans un contexte de révision, de retour en arrière ou même d’audit, le registre sert de référence pour comprendre la logique derrière chaque étape du projet. Support à la continuité : En cas de changement de personnel, le registre facilite la transition et l’onboarding des nouveaux arrivants en leur offrant un historique clair. Un exemplePrenons un exemple du monde .NET. Vous avez peut-être remarqué que certaines méthodes portent le suffixe Async.La question s’est posée : est-ce systématique de l’ajouter ?Voici une décision prise pour clarifier cette pratique :2024-11-06 - Nomenclature des méthodes asynchrones Contexte : Devons-nous systématiquement ajouter le suffixe Async aux méthodes asynchrones ? Décision : Comme toutes nos méthodes sont asynchrones, le suffixe Async devient redondant, donc il n’est pas recommandé de l’ajouter systématiquement. Nous suivons l’orientation préconisée par MediatR. Cependant, si une interface doit proposer des versions synchrone et asynchrone d’une même méthode, le suffixe Async est recommandé pour identifier clairement la méthode asynchrone. Conséquences : Aucun impact notable dans ce cas-ci.ConclusionUn registre de décisions est bien plus qu’un simple document administratif. Dans les environnements complexes et les projets d’envergure, il devient un outil essentiel de transparence, de cohérence et d’efficacité pour toute l’équipe. Il permet non seulement de gagner du temps, mais également de créer un espace de confiance où chacun peut retrouver les raisons et le contexte des choix passés, sans devoir constamment revenir en arrière.En documentant nos décisions, nous créons un cadre de référence clair et solide pour éviter les erreurs de répétition, faciliter l’intégration des nouveaux membres et renforcer la continuité dans les projets. En fin de compte, ce petit investissement de temps pour maintenir un registre peut faire toute la différence dans la gestion d’un projet réussi.Alors, pourquoi ne pas intégrer un registre de décisions dès aujourd’hui dans vos pratiques ?" }, { "title": "Introduction à BenchmarkDotNet", "url": "/posts/introduction-benchmarkdotnet/", "categories": "outil-developpement", "tags": "dotnet", "date": "2024-10-31 19:00:00 -0400", "snippet": "BenchmarkDotNet est une bibliothèque populaire pour effectuer des benchmarks en .NET. Elle permet de mesurer précisément les performances de vos applications et d’identifier les points à optimiser....", "content": "BenchmarkDotNet est une bibliothèque populaire pour effectuer des benchmarks en .NET. Elle permet de mesurer précisément les performances de vos applications et d’identifier les points à optimiser.Voici comment commencer : Installation : Tout d’abord, installez BenchmarkDotNet via NuGet dans votre projet : dotnet add package BenchmarkDotNet Création d’un benchmark : Créez une classe pour votre benchmark et ajoutez-y une méthode marquée avec l’attribut [Benchmark]. Configuration : Vous pouvez configurer votre benchmark en utilisant des attributs comme [Params] pour spécifier différents paramètres de test. Exemple d’utilisationSupposons que vous vouliez mesurer les performances de deux méthodes différentes pour obtenir la valeur texte d’un enum.Voici comment vous pourriez procéder :using BenchmarkDotNet.Attributes;using BenchmarkDotNet.Order;namespace TestBenchmarkDotnet.TestEnumToString;[MemoryDiagnoser][Orderer(SummaryOrderPolicy.FastestToSlowest)][RankColumn]public class BenchmarkEnumToString{ [Params(1, 100, 1_000)] public int Iteration { get; set; } [Benchmark(Baseline = true)] public List&lt;string&gt; EnumToString() { var valeursEnum = new List&lt;string&gt;(); for (var i = 0; i &lt; Iteration; i++) { valeursEnum.Add(Joueur.ValeurEnum1.ToString()); } return valeursEnum; } [Benchmark] public List&lt;string&gt; EnumNameof() { var valeursEnum = new List&lt;string&gt;(); for (var i = 0; i &lt; Iteration; i++) { valeursEnum.Add(nameof(Joueur.ValeurEnum1)); } return valeursEnum; } private enum Joueur { ValeurEnum1, ValeurEnum2 }}// ------------- au niveau du program.csusing BenchmarkDotNet.Running;using TestBenchmarkDotnet.TestEnumToString;BenchmarkRunner.Run&lt;BenchmarkEnumToString&gt;();Démarrer l’exécution d’un benchmarkPour exécuter un benchmark, il est essentiel d’exécuter vos benchmarks en mode Release.Voici comment procéder : Exécution des benchmarks : Vous pouvez exécuter vos benchmarks en ligne de commande. dotnet run -c Release 💡 Assurez-vous d’exécuter la commande où se trouve votre fichier .csproj. Analyse des résultats : Après l’exécution, BenchmarkDotNet générera un rapport détaillé dans la console avec des métriques telles que le temps d’exécution moyen, l’écart-type et l’allocation mémoire. Voici ce que ces métriques signifient : Temps d’exécution moyen (Mean) : C’est la moyenne des temps d’exécution mesurés pour chaque itération du benchmark. Mesuré en nanosecondes (ns), microsecondes (us), ou millisecondes (ms). Écart-type (StdDev) : Il mesure la variation ou la dispersion des temps d’exécution mesurés autour de la moyenne. Une valeur faible indique une grande stabilité dans les mesures. Allocation mémoire (Allocated) : C’est la quantité totale de mémoire allouée pendant l’exécution du benchmark. Mesurée en octets (B), kilooctets (KB), ou mégaoctets (MB), selon l’ampleur de l’allocation. Résultat de l’exécution| Method | Iteration | Mean | Error | StdDev | Median | Ratio | RatioSD | Rank | Gen0 | Gen1 | Allocated | Alloc Ratio ||------------- |---------- |------------:|-----------:|-----------:|------------:|------:|--------:|-----:|-------:|-------:|----------:|------------:|| EnumNameof | 1 | 13.71 ns | 0.117 ns | 0.109 ns | 13.69 ns | 0.59 | 0.02 | 1 | 0.0105 | - | 88 B | 0.79 || EnumToString | 1 | 22.44 ns | 0.496 ns | 1.002 ns | 22.32 ns | 1.00 | 0.00 | 2 | 0.0134 | - | 112 B | 1.00 || | | | | | | | | | | | | || EnumNameof | 100 | 375.49 ns | 7.477 ns | 18.621 ns | 367.17 ns | 0.35 | 0.02 | 1 | 0.2618 | 0.0010 | 2192 B | 0.48 || EnumToString | 100 | 1,065.24 ns | 21.091 ns | 44.488 ns | 1,064.77 ns | 1.00 | 0.00 | 2 | 0.5474 | 0.0019 | 4592 B | 1.00 || | | | | | | | | | | | | || EnumNameof | 1000 | 3,190.26 ns | 110.750 ns | 319.539 ns | 3,023.35 ns | 0.35 | 0.03 | 1 | 1.9836 | 0.0572 | 16600 B | 0.41 || EnumToString | 1000 | 9,346.52 ns | 186.882 ns | 360.058 ns | 9,284.09 ns | 1.00 | 0.00 | 2 | 4.8523 | 0.1373 | 40602 B | 1.00 |On constate qu’il est plus rapide d’utiliser nameof et également moins coûteux en termes d’allocation mémoire.Points à considérer dans les benchmarksLorsque vous analysez les résultats de vos benchmarks avec BenchmarkDotNet, voici quelques points clés à garder à l’esprit : Métriques : Les métriques comme le temps d’exécution moyen, l’écart-type et l’allocation mémoire sont cruciales pour évaluer les performances et l’efficacité de votre code. Comparaison : Utilisez les résultats pour comparer différentes implémentations ou configurations de votre code.ConclusionBenchmarkDotNet est un outil précieux pour évaluer les performances de vos applications .NET de manière précise et reproductible. En suivant ces étapes, vous pouvez commencer à utiliser efficacement BenchmarkDotNet pour optimiser vos applications." }, { "title": "Introduction aux Minimal APIs", "url": "/posts/introduction-minimal-apis/", "categories": "outil-developpement", "tags": "dotnet, aspnet-core", "date": "2024-10-10 20:00:00 -0400", "snippet": "L’utilisation des Minimal APIs est une approche simplifiée pour créer des API HTTP rapides avec ASP.NET Core. Elles permettent de créer des points de terminaison REST entièrement fonctionnels avec ...", "content": "L’utilisation des Minimal APIs est une approche simplifiée pour créer des API HTTP rapides avec ASP.NET Core. Elles permettent de créer des points de terminaison REST entièrement fonctionnels avec un minimum de code et de configuration. Vous pouvez ignorer la génération automatique classique et éviter les contrôleurs en déclarant directement des routes et des actions d’API.Par exemple, le code suivant crée une API à la racine de l’application web qui retourne simplement le texte “Hello World!” :var app = WebApplication.Create(args)app.MapGet(\"/\", () =&gt; \"Hello World!\");app.Run();Cela suffit pour démarrer, mais il y a bien plus à découvrir. Les API minimales offrent également la configuration et la personnalisation nécessaires pour évoluer vers plusieurs API, gérer des routes complexes, appliquer des règles d’autorisation, et contrôler le contenu des réponses.Comment créer un premier minimal ApiCe tutoriel décrit les principes fondamentaux liés à la création d’un Minimal API avec ASP.NET Core.👨‍💻 Le code du tutoriel est disponible ici. Si vous ne le saviez pas, il est possible d’ouvrir un référentiel GitHub directement dans une version web de Visual Studio Code en ajoutant “1s” après “github” dans l’URL.Comment structurer ces endpointsQue remarquez-vous à la suite de la lecture du tutoriel ? Les Minimal APIs offrent une grande flexibilité sur la manière de les structurer. On peut définir des endpoints directement dans le fichier Program.cs, contrairement à l’utilisation classique des contrôleurs. C’est très pratique pour réaliser des prototypes ou même des projets de faible envergure.Mais que faire si notre projet devient plus complexe ? Comment gérer les essais unitaires et structurer le code pour faciliter sa lisibilité et son entretien ?Voici quelques recommandations importantes à garder en tête : Pour être testable unitairement, une méthode doit être exposée et non simplement définie en tant qu’endpoint (exemple). Il est possible de définir des méthodes statiques dans le Program.cs, mais il est recommandé de déplacer les points de terminaison à l’extérieur pour une meilleure organisation. Utilisez le type TypedResults au lieu du type générique Results pour un meilleur contrôle des réponses API.Le code associé au tutoriel illustre ces pratiques à travers la structure de la classe TodoEndpointsV2.D’autres lectures pour aller plus loinSi vous souhaitez approfondir vos connaissances sur les Minimal APIs, voici quelques ressources que je vous recommande fortement : Gestionnaires de routage Liaison de paramètres Création de réponses Filtres Tests unitaires et d’intégrationQuand utiliser les Minimal APIs ?Les Minimal APIs brillent dans les cas où la simplicité, la rapidité, et la légèreté sont des priorités. Elles sont particulièrement adaptées pour : Les microservices : Leur faible complexité en fait un excellent choix pour les services isolés, où les fonctionnalités sont limitées et où les performances sont cruciales. Les prototypes ou petites applications : Si vous avez besoin de mettre en place rapidement un service sans trop de structure, les Minimal APIs offrent une solution agile. Les fonctions serverless : Leur nature légère permet de réduire les temps de démarrage et d’améliorer les performances, particulièrement pour des environnements comme Azure Functions ou AWS Lambda. Compatibilité avec l’AOT (Ahead-of-Time) compilation : Contrairement aux contrôleurs classiques, les Minimal APIs sont compatibles avec la compilation AOT, ce qui est un atout important si vous avez des exigences de performance et de déploiement rapide. Les Web APIs classiques, reposant sur la réflexion, ne peuvent pas bénéficier de cette optimisation.Les Minimal APIs et les Web APIs classiques ont toutes deux leur utilité, en fonction des besoins du projet. Pour des applications simples ou des microservices, les Minimal APIs offrent rapidité, compatibilité AOT, et efficacité. Pour des applications plus complexes nécessitant une structure bien définie, les Web APIs classiques restent le choix idéal.ConclusionL’utilisation des API minimales dans ASP.NET Core offre une manière moderne et efficace de créer des services web légers et performants. Grâce à leur simplicité, elles permettent aux développeurs de se concentrer sur l’essentiel, en réduisant la complexité du code et en accélérant le développement. Bien qu’elles soient idéales pour des scénarios simples et des microservices, elles offrent également la flexibilité nécessaire pour évoluer vers des architectures plus complexes si besoin. En adoptant cette approche, vous bénéficiez d’une solution élégante, facile à maintenir, tout en profitant des puissantes fonctionnalités d’ASP.NET Core.💡 Pour approfondir le débat entre Minimal APIs et Web APIs classiques, je vous recommande vivement de visionner la vidéo de Nick Chapsas, où il partage son avis éclairé sur le sujet.Références Vue d’ensemble des API minimales Tutoriel : Créer une API minimale avec ASP.NET Core Gestionnaires de routage dans les applications API minimales Liaison de paramètres dans les applications API minimales Créer des réponses dans les applications API minimales Filtres dans les applications d’API minimales Tester les applications API minimales" }, { "title": "Les Générateurs de source", "url": "/posts/source-generator-dotnet/", "categories": "", "tags": "dotnet, source-generator", "date": "2024-09-29 18:00:00 -0400", "snippet": "Les Source Generators (Générateurs de source) sont des outils intégrés au compilateur qui génèrent automatiquement du code source lors de la compilation, permettant aux développeurs d’automatiser l...", "content": "Les Source Generators (Générateurs de source) sont des outils intégrés au compilateur qui génèrent automatiquement du code source lors de la compilation, permettant aux développeurs d’automatiser la création de portions de code répétitives ou complexes tout en gardant le code généré visible et modifiable. Introduits avec .NET 5, ils sont rapidement devenus un élément clé de l’écosystème .NET. Contrairement à des techniques comme la réflexion, qui génèrent du code dynamiquement à l’exécution, les Source Generators produisent du code statique, optimisé et vérifié lors de la compilation.La révolution sur le quotidienL’arrivée des Source Generators a considérablement transformé la manière de travailler au quotidien, surtout pour les développeurs confrontés à du boilerplate code ou des scénarios nécessitant une génération de code personnalisée.Voici comment ils révolutionnent le développement : Réduction du code récurrent : Les générateurs de code prennent en charge la génération de classes de mapping, DTOs, modèles de validation, ou tout autre code répétitif, permettant ainsi de se concentrer sur des tâches à plus forte valeur ajoutée. Meilleures performances : Le code étant généré à la compilation, il est plus performant que les approches reposant sur des mécanismes comme la réflexion, qui sont dynamiques et coûteux en termes de performance. Centralisation de la logique : Les règles de génération peuvent être définies une fois et appliquées de manière uniforme dans tout le projet, améliorant la maintenabilité et réduisant les erreurs. Compatibilité AOT : L’un des avantages cruciaux des Source Generators est qu’ils ne reposent pas sur la réflexion, une technique incompatible avec le mode de compilation Ahead-Of-Time (AOT). AOT est utilisé pour optimiser les applications .NET destinées à des environnements contraints en ressources, comme les applications mobiles et les conteneurs. En remplaçant la réflexion par du code généré statiquement, les Source Generators rendent le code compatible avec AOT et permettent ainsi une exécution plus rapide et plus légère.Exemple avec MapperlyMapperly est un exemple parfait d’outil tirant parti des Source Generators pour automatiser les tâches de mapping entre objets, tout en éliminant la dépendance à la réflexion.Imaginons que vous ayez une classe Personne et un DTO PersonneDto, et que vous souhaitiez les mapper :public class Personne{ public string Prenom { get; set; } public string Nom { get; set; } public DateTime DateNaissance { get; set; }}public class PersonneDto{ public string Prenom { get; set; } public string Nom { get; set; }}Avec Mapperly, il vous suffit de créer un mapper sous la forme d’une classe partielle et d’appliquer l’attribut [Mapper].[Mapper]public partial class PersonneMapper{ public partial PersonneDto MapToDto(Personne personne);}À la compilation, Mapperly génère automatiquement le code nécessaire pour mapper les propriétés correspondantes :public partial class PersonneMapper{ public partial PersonneDto MapToDto(Personne personne) { return new PersonneDto { Prenom = personne.Prenom, Nom = personne.Nom }; }}Ici, Mapperly remplace la réflexion, souvent utilisée dans des solutions comme AutoMapper, par du code statique et optimisé. Ce qui, en plus de rendre le mapping plus rapide, le rend compatible avec le mode AOT, où la réflexion n’est pas une option possible.Outils à surveiller pour exploiter pleinement les Source GeneratorsL’écosystème .NET regorge d’outils qui exploitent les Source Generators pour simplifier et automatiser certaines tâches courantes du développement.Voici une sélection d’outils à surveiller qui pourraient transformer vos pratiques de développement : Mapperly - Mapperly, comme mentionné précédemment, est un outil qui génère du code de mapping entre objets de manière statique. Contrairement à AutoMapper, qui utilise la réflexion, Mapperly repose sur des Source Generators, ce qui le rend beaucoup plus performant et compatible avec des environnements AOT. TUnit - TUnit est une bibliothèque de tests unitaires qui utilise les Source Generators pour générer des méthodes de test à la volée. Cela permet de réduire considérablement le code à écrire pour chaque test, en particulier pour les scénarios répétitifs où seule la donnée testée varie. CoreWCF - CoreWCF est un port de Windows Communication Foundation (WCF) pour .NET Core. Il utilise les Source Generators pour générer automatiquement des contrats de service et des proxy, facilitant la migration des applications WCF existantes vers des technologies modernes comme .NET 6+. Enum.Source.Generator - Cet outil permet de générer du code à partir des énumérations définies dans vos projets. Plutôt que d’écrire manuellement des méthodes pour obtenir des valeurs ou des descriptions associées à chaque enum, Enum.Source.Generator génère automatiquement ce code pour vous, réduisant ainsi les risques d’erreurs et la charge de maintenance. Dunet - Dunet est une bibliothèque pour faciliter la création de types discriminés (discriminated unions), une fonctionnalité très puissante que l’on retrouve dans des langages comme F#. Dunet génère automatiquement le code nécessaire pour gérer ces types discriminés en .NET, simplifiant grandement la gestion des cas particuliers dans les applications.ConclusionLes Source Generators représentent une avancée majeure, permettant aux développeurs d’écrire moins de code répétitif et d’améliorer les performances globales de leurs applications. En éliminant la nécessité de la réflexion dans certains scénarios critiques, comme le mapping d’objets ou la génération de proxy, ils permettent également une meilleure compatibilité avec les environnements AOT. Des outils comme Mapperly, TUnit, CoreWCF, Enum.Source.Generator et Dunet montrent bien le potentiel transformateur de ces outils.Restez attentif à ces outils qui permettent de tirer pleinement parti des capacités de .NET !" }, { "title": "Les avantages d'utiliser un fichier .editorconfig en .NET", "url": "/posts/avantages-editorconfig/", "categories": "outil-developpement", "tags": "dotnet", "date": "2024-08-19 19:58:00 -0400", "snippet": "IntroductionSi vous travaillez sur des projets en .NET (ou même dans d’autres langages), vous avez peut-être déjà entendu parler du fichier .editorconfig. C’est un fichier qui peut rendre la vie de...", "content": "IntroductionSi vous travaillez sur des projets en .NET (ou même dans d’autres langages), vous avez peut-être déjà entendu parler du fichier .editorconfig. C’est un fichier qui peut rendre la vie de votre équipe de développement beaucoup plus facile. Il permet de définir et de maintenir des règles de codage cohérentes, peu importe l’éditeur ou l’IDE utilisé.Voici pourquoi utiliser un fichier .editorconfig est une bonne idée, avec quelques exemples pratiques spécifiques à .NET.Pourquoi utiliser un fichier .editorconfig? Code uniforme Constance : Un fichier .editorconfig permet de définir des règles de formatage et de style que tout le monde doit suivre. Fini les disputes sur les tabulations versus les espaces! Conformité: Tout le monde respecte les mêmes règles, ce qui réduit les erreurs et rend le code plus propre. Travail d’équipe simplifié Harmonisation : Que votre équipe soit à Québec, Montréal ou New York, tout le monde suit les mêmes conventions. Le code reste uniforme peu importe qui l’écrit. Moins de conflits : Avec un formatage standardisé, il y a moins de conflits quand vous fusionnez des branches dans Git. Compatible avec vos outils Fonctionne partout : La plupart des éditeurs de code et IDE comme Visual Studio et Visual Studio Code supportent les .editorconfig. Pas besoin de plugins supplémentaires, mais une extension peut être nécessaire. Adaptable : Vous pouvez ajuster les règles de formatage selon les besoins spécifiques de votre projet ou organisation. Impliquer l’équipe dans la définition des règlesIl est crucial que toute l’équipe soit d’accord sur les règles de codage définies dans le fichier .editorconfig.Voici un processus simple pour y arriver : Réunion d’équipe : Organisez une réunion pour discuter des règles de codage à adopter. Assurez-vous que tout le monde puisse exprimer ses préférences et ses besoins. Brainstorming : Encouragez les membres de l’équipe à proposer des règles et des conventions qu’ils aimeraient suivre. Notez toutes les suggestions. Consensus : Discutez des différentes propositions et essayez de trouver un consensus. L’objectif est de choisir des règles que tout le monde pourra suivre facilement. Documenter les règles : Une fois que les règles sont décidées, documentez-les dans le fichier .editorconfig et partagez-le avec toute l’équipe. Revue périodique : Planifiez des revues périodiques des règles pour s’assurer qu’elles restent pertinentes et adaptées aux besoins de l’équipe.Installer l’extension requise pour Visual Studio CodePour vous assurer que Visual Studio Code utilise correctement le fichier .editorconfig, vous devez installer l’extension EditorConfig for VS Code. Cette extension garantit que les paramètres de votre fichier .editorconfig sont respectés.Pour installer l’extension : Ouvrez Visual Studio Code. Allez dans l’onglet “Extensions” ou appuyez sur Ctrl+Shift+X. Recherchez “EditorConfig”. Cliquez sur “Installer” pour l’extension “EditorConfig for VS Code”.Exemples de configurationVoici quelques exemples de configurations pratiques pour un projet .NET. Indenter avec des espaces # Utiliser des espaces pour l'indentation, avec une largeur de 4 espaces[*.cs]indent_style = spaceindent_size = 4 Fin de ligne uniforme # Utiliser LF (Line Feed) pour les fins de ligne[*.cs]end_of_line = lf Encodage des fichiers # Utiliser UTF-8 pour tous les fichiers[*.cs]charset = utf-8 Espaces autour des opérateurs # Ajouter des espaces autour des opérateurs[*.cs]dotnet_style_operator_placement_when_wrapping = before Règles spécifiques pour les fichiers de YAML # Utiliser des espaces pour l'indentation avec une largeur de 2 espaces pour les fichiers YAML[*.yaml]indent_style = spaceindent_size = 2 Avec cette configuration dans votre fichier .editorconfig, tous les fichiers YAML dans votre projet respecteront la convention de deux espaces par indentation. Cela permet d’assurer une cohérence dans le formatage du code YAML, quel que soit l’éditeur utilisé par votre équipe.Ressources additionnellesPour des informations plus détaillées sur les analyseurs et les bonnes pratiques en .NET, vous pouvez consulter les articles de Dave McCarter (dotnetdave) sur son blog DotNet Tips and Tricks. Dave publie régulièrement des articles pertinents sur l’utilisation des analyseurs et d’autres outils pour améliorer la qualité du code et la productivité des développeurs.ConclusionUtiliser un fichier .editorconfig dans vos projets .NET a plein d’avantages. Cela aide à garder un code propre et cohérent, facilite le travail en équipe, et évite les conflits de formatage. En plus, c’est super facile à mettre en place et ça marche avec la plupart des éditeurs et IDE. Bref, si vous voulez rendre la vie de votre équipe de développeurs plus simple et le code plus propre, essayez le fichier .editorconfig. N’oubliez pas de le mettre à la racine de votre référentiel, d’impliquer toute l’équipe dans la définition des règles et d’installer l’extension “EditorConfig for VS Code”.Vous allez me remercier!" }, { "title": "Découvrez la nouvelle version de MSTest", "url": "/posts/mise-a-jour-mstest/", "categories": "outil-developpement", "tags": "dotnet, essais", "date": "2024-06-26 19:00:00 -0400", "snippet": "Microsoft vient tout juste de sortir une nouvelle version de MSTest!Quoi de neuf ? MSTest.Analyzers : Plein de corrections de bugs et d’améliorations pour rendre vos analyses de code encore plus e...", "content": "Microsoft vient tout juste de sortir une nouvelle version de MSTest!Quoi de neuf ? MSTest.Analyzers : Plein de corrections de bugs et d’améliorations pour rendre vos analyses de code encore plus efficaces. MSTest.Sdk : De nouvelles fonctionnalités et des améliorations pour une meilleure performance et intégration des tests. MSTest.Runner : Vous pouvez maintenant tester vos applications WinUI avec MSTest. Génial, non ?Pour plus de détails, consultez l’article officiel ici.Les règles à ajouterSi vous voulez tirer le meilleur parti des nouveaux analyseurs, pensez à ajouter ces règles à votre fichier editorconfig :Règles de structures : MSTEST0004 - PublicTypeShouldBeTestClassAnalyzer MSTEST0006 - AvoidExpectedExceptionAttributeAnalyzer MSTEST0015 - TestMethodShouldNotBeIgnored MSTEST0016 - TestClassShouldHaveTestMethod MSTEST0019 - PreferTestInitializeOverConstructorAnalyzer MSTEST0021 - PreferDisposeOverTestCleanupAnalyzer MSTEST0025 - PreferAssertFailOverAlwaysFalseConditionsAnalyzerRègles d’usages : MSTEST0002 - TestClassShouldBeValidAnalyzer MSTEST0003 - TestMethodShouldBeValidAnalyzer MSTEST0005 - TestContextShouldBeValidAnalyzer MSTEST0007 - UseAttributeOnTestMethodAnalyzer MSTEST0008 - TestInitializeShouldBeValidAnalyzer MSTEST0009 - TestCleanupShouldBeValidAnalyzer MSTEST0010 - ClassInitializeShouldBeValidAnalyzer MSTEST0011 - ClassCleanupShouldBeValidAnalyzer MSTEST0012 - AssemblyInitializeShouldBeValidAnalyzer MSTEST0013 - AssemblyCleanupShouldBeValidAnalyzer MSTEST0014 - DataRowShouldBeValidAnalyzer MSTEST0017 - AssertionArgsShouldBePassedInCorrectOrder MSTEST0023 - DoNotNegateBooleanAssertionAnalyzer MSTEST0024 - DoNotStoreStaticTestContextAnalyzer MSTEST0026 - AssertionArgsShouldAvoidConditionalAccessRuleIdConfiguration du timeout pour le cleanup des classesIl est également possible de configurer un timeout pour le cleanup des classes. Pour cela, il suffit d’ajouter la configuration suivante dans votre fichier .runsettings :&lt;RunSettings&gt; &lt;MSTest&gt; &lt;ClassCleanupTimeout&gt;1000&lt;/ClassCleanupTimeout&gt; &lt;/MSTest&gt;&lt;/RunSettings&gt;Pour en savoir plus sur ces règles, jetez un œil à la documentation officielle ici." }, { "title": "Annonce de la mise en ligne", "url": "/posts/annonce-mise-en-ligne/", "categories": "", "tags": "", "date": "2024-05-12 21:00:00 -0400", "snippet": "Oyez, Oyez! Je voulais vous informer que je suis actuellement très engagé dans l’achèvement de la structure de mon site web.Il me reste encore quelques ajustements à faire pour bien maîtriser la pl...", "content": "Oyez, Oyez! Je voulais vous informer que je suis actuellement très engagé dans l’achèvement de la structure de mon site web.Il me reste encore quelques ajustements à faire pour bien maîtriser la plateforme que j’utilise, mais cela avance plus rapidement que prévu.En attendant sa mise en ligne, je vous encourage vivement à me faire parvenir vos suggestions!Restez informés en vous abonnant au flux RSS!" } ]
