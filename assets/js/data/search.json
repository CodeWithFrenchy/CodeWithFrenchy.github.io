[ { "title": "Optimiser la performance des logs dans un projet .NET", "url": "/posts/optimisation-performances-logs/", "categories": "", "tags": "dotnet", "date": "2026-01-26 18:00:00 -0500", "snippet": "IntroductionLa journalisation est un Ã©lÃ©ment central de toute application .NET : elle permet de diagnostiquer des erreurs, de suivre des traitements et dâ€™auditer les actions. La plupart des dÃ©velop...", "content": "IntroductionLa journalisation est un Ã©lÃ©ment central de toute application .NET : elle permet de diagnostiquer des erreurs, de suivre des traitements et dâ€™auditer les actions. La plupart des dÃ©veloppeurs utilisent ILogger&lt;T&gt; et les mÃ©thodes dâ€™extension comme LogInformation() ou LogError(). Pourtant ces mÃ©thodes pratiques cachent un coÃ»t non nÃ©gligeable : Ã  chaque appel, le message doit Ãªtre analysÃ©, les paramÃ¨tres sont convertis en object et les types â€œvaleursâ€ sont boxÃ©s. Ce mÃ©canisme crÃ©e des allocations mÃ©moire et du travail pour le GC, surtout lorsque le niveau de journalisation ne permet pas dâ€™Ã©mettre le message. Heureusement, depuis plusieurs versions de .NET un pattern de journalisation Ã  hautes performances existe : les templates de loggers (LoggerMessage).Pourquoi les mÃ©thodes ILogger classiques sont lentesLes extensions de ILogger&lt;T&gt; acceptent des paramÃ¨tres sous forme dâ€™arguments variadiques (params object?), ce qui entraÃ®ne plusieurs problÃ¨mes : Boxing des types valeurs : chaque fois quâ€™une structure (int, bool, etc.) est passÃ©e, elle est enveloppÃ©e dans un objet, crÃ©ant une allocation. Analyse rÃ©pÃ©tÃ©e du template : la chaÃ®ne contenant les placeholders (â€œProcess {Item} at {Time}â€) doit Ãªtre analysÃ©e Ã  chaque appel pour extraire les noms des champs. MÃªme si le niveau de log nâ€™est pas activÃ©, lâ€™analyse est effectuÃ©e. Interpolation de chaÃ®nes : lorsque lâ€™on utilise la syntaxe $\"User {userId} logged in\", une nouvelle chaÃ®ne est crÃ©Ã©e et tous les arguments sont convertis en chaÃ®nes avant lâ€™appel.Ces coÃ»ts, parfois faibles Ã  lâ€™unitÃ©, deviennent importants dans des scÃ©narios Ã  fort trafic ou pour des services trÃ¨s sollicitÃ©s. Microsoft a introduit des solutions pour contourner ces problÃ¨mes : les mÃ©thodes prÃ©â€‘gÃ©nÃ©rÃ©es avec LoggerMessage.Historique : LoggerMessage.Define et lâ€™arrivÃ©e du sourceâ€‘generatorNaissance des templates de loggersDÃ¨s .NET Core 2.1, lâ€™Ã©quipe ASP.NET Core a ajoutÃ© la classe LoggerMessage. Cette classe permet de dÃ©finir des dÃ©lÃ©guÃ©s de journalisation qui sont mis en cache. Chaque dÃ©lÃ©guÃ© associe un niveau de log, un EventId et un modÃ¨le de message. Ã€ lâ€™exÃ©cution, on nâ€™a plus besoin dâ€™analyser le modÃ¨le ni de boxer les valeurs : on invoque simplement la mÃ©thode compilÃ©e. Microsoft explique que ces dÃ©lÃ©guÃ©s Â« crÃ©ent des Ã©vÃ©nements de log avec moins dâ€™allocations par rapport aux mÃ©thodes dâ€™extension Â».Le pattern consiste Ã  : DÃ©clarer un champ Action&lt;ILogger, T1, T2, Exception&gt; Ã  lâ€™aide de LoggerMessage.Define(). On prÃ©cise le niveau de log, lâ€™ID de lâ€™Ã©vÃ©nement et le modÃ¨le de message. CrÃ©er une mÃ©thode dâ€™extension qui appelle ce dÃ©lÃ©guÃ© en passant les valeurs typÃ©es.Exemple :private static readonly Action&lt;ILogger, string, int, Exception?&gt; _logProcessingItem = LoggerMessage.Define&lt;string, int&gt;( LogLevel.Information, new EventId(1001, nameof(LogProcessingItem)), \"Traitement de lâ€™Ã©lÃ©ment {Item} en {Milliseconds}Â ms\");public static void LogProcessingItem(this ILogger logger, string item, int milliseconds){ _logProcessingItem(logger, item, milliseconds, null);}Ã€ partir de .NET 6, Microsoft a simplifiÃ© cette approche grÃ¢ce au sourceâ€‘generator. En dÃ©corant une mÃ©thode partielle avec lâ€™attribut LoggerMessageAttribute, le compilateur gÃ©nÃ¨re automatiquement le dÃ©lÃ©guÃ© et la mÃ©thode dâ€™extension. Cette approche Ã©vite dâ€™Ã©crire du code rÃ©pÃ©titif. Le guide officiel prÃ©cise que les mÃ©thodes doivent Ãªtre partial, retourner void et ne pas Ãªtre gÃ©nÃ©riques.Exemple :public static partial class Logs{ [LoggerMessage( EventId = 1002, Level = LogLevel.Warning, Message = \"Erreur lors du traitement de lâ€™utilisateur {UserId}\")] public static partial void LogUserProcessingError( ILogger logger, string userId, Exception ex);}// AppelÂ :Logs.LogUserProcessingError(_logger, userId, exception);Le compilateur gÃ©nÃ¨re une implÃ©mentation qui appelle un dÃ©lÃ©guÃ© cachÃ© et offre ainsi les mÃªmes bÃ©nÃ©fices de performance quâ€™avec LoggerMessage.Define.Quand ces techniques sontâ€‘elles apparues ? .NET Core 2.1 (2018) : apparition de LoggerMessage.Define. Lâ€™API permet dÃ©jÃ  de mettre en cache la structure du message et de rÃ©duire les allocations. .NET 6 (novembre 2021) : introduction de LoggerMessageAttribute, un gÃ©nÃ©rateur de code qui rÃ©duit la verbositÃ© du modÃ¨le. Cette version marque aussi lâ€™arrivÃ©e de la rÃ¨gle dâ€™analyse CA1848 (Utilisez les dÃ©lÃ©guÃ©s LoggerMessage) activable en niveau dâ€™analyse recommended. .NET 7 / 8 et 9 : la sourceâ€‘gÃ©nÃ©ration sâ€™amÃ©liore et les analyzers incitent toujours plus Ã  adopter ces patterns. Lâ€™analyseur CA1848 nâ€™est cependant pas activÃ© par dÃ©faut ; il faut choisir un niveau dâ€™analyse pour lâ€™activer.BÃ©nÃ©fices mesurÃ©s : benchmarksPour savoir si lâ€™effort vaut la peine, plusieurs dÃ©veloppeurs ont mesurÃ© les diffÃ©rences de performance. Lâ€™article Donâ€™t box your logs de Daniel Genezini compare trois techniques : lâ€™appel direct Ã  ILogger.LogInformation(), lâ€™utilisation dâ€™un dÃ©lÃ©guÃ© avec LoggerMessage.Define() et lâ€™utilisation de LoggerMessageAttribute.Voici un rÃ©sumÃ© des rÃ©sultats : MÃ©thode Temps moyen par appel Allocations Description Extension ILogger ~43-155 ns Allocations (~56 B) Les paramÃ¨tres sont convertis en object, la chaÃ®ne est analysÃ©e et les types valeurs sont boxÃ©s. LoggerMessage.Define ~12-20 ns Aucune allocation Le dÃ©lÃ©guÃ© est gÃ©nÃ©rÃ© au dÃ©marrage et mis en cache ; les paramÃ¨tres restent typÃ©s. LoggerMessageAttribute ~9 ns Aucune allocation Le compilateur gÃ©nÃ¨re le code de log au build ; la mÃ©thode est aussi courte que possible. On constate que les templates de loggers multiplient les performances par 4 Ã  10 et suppriment complÃ¨tement les allocations, ce qui rÃ©duit la pression sur le GC. Dans des microâ€‘services oÃ¹ des milliers de logs sont produits par seconde, ces gains sont prÃ©cieux.Exemples dâ€™utilisation1. Enregistrer le traitement dâ€™une commandeImaginons un service qui traite des commandes et veut journaliser le dÃ©but et la fin de chaque traitement. Avec LoggerMessageAttribute, on crÃ©e une classe statique de logs :public static partial class CommandLogs{ [LoggerMessage( EventId = 2001, Level = LogLevel.Information, Message = \"DÃ©but du traitement de la commande {OrderId}\")] public static partial void StartProcessing( ILogger logger, string orderId); [LoggerMessage( EventId = 2002, Level = LogLevel.Information, Message = \"Fin du traitement de la commande {OrderId} en {ElapsedMs}Â ms\")] public static partial void EndProcessing( ILogger logger, string orderId, double elapsedMs);}// Dans votre serviceÂ :var sw = Stopwatch.StartNew();CommandLogs.StartProcessing(_logger, orderId);// exÃ©cution â€¦CommandLogs.EndProcessing(_logger, orderId, sw.Elapsed.TotalMilliseconds);Les mÃ©thodes gÃ©nÃ©rÃ©es sont statiques et ne crÃ©ent aucune allocation lorsque le niveau Information est dÃ©sactivÃ©.EnforÃ§ant la rÃ¨gle CA1848 dans .editorconfigPour encourager lâ€™Ã©quipe Ã  adopter les templates de loggers, il est utile dâ€™ajouter la rÃ¨gle CA1848 dans un fichier .editorconfig. Ce fichier permet de configurer la sÃ©vÃ©ritÃ© des analyzers pour un projet ou une solution.Voici un exemple :# Applique Ã  tous les fichiers C#[*.cs]# Utiliser un niveau dâ€™analyse afin dâ€™activer CA1848 et dâ€™autres rÃ¨gles# AnalyseLevel peut Ãªtre dÃ©fini sur `latest` ou `latest-recommended` selon la version de .NETdotnet_analyzer_diagnostic.severity = warning# SpÃ©cifie que la rÃ¨gle CA1848 est considÃ©rÃ©e comme une erreur# Les dÃ©veloppeurs devront corriger les appels `ILogger` classiquesdotnet_diagnostic.CA1848.severity = errorLa documentation sur la configuration des analyzers prÃ©cise que le prÃ©fixe dotnet_diagnostic.&lt;rule ID&gt;.severity permet de dÃ©finir la sÃ©vÃ©ritÃ© dâ€™une rÃ¨gle individuelle. Les valeurs possibles sont error (fait Ã©chouer la compilation), warning, suggestion, silent, none ou default. Ce paramÃ©trage sâ€™applique Ã  lâ€™ensemble du projet, mais on peut ajuster la portÃ©e en ajoutant plusieurs blocs *.cs, MyApp/Program.cs ou */Tests selon les besoins.Conseils pour adopter les templates de loggers Centralisez vos messages : crÃ©ez une classe statique par fonctionnalitÃ© ou domaine (OrderLogs, UserLogsâ€¦), ce qui facilite la recherche et la cohÃ©rence des messages. Utilisez des EventId uniques : associez chaque Ã©vÃ©nement Ã  un identifiant qui permettra de filtrer et de corrÃ©ler les logs. Les gÃ©nÃ©rateurs acceptent la propriÃ©tÃ© EventId et un nom. Soyez parcimonieux avec les arguments : limitez le nombre dâ€™arguments et privilÃ©giez des types simples ou des objets qui se sÃ©rialisent bien. Ã‰vitez de passer des entitÃ©s entiÃ¨res ; prÃ©fÃ©rez des identifiants ou des DTO. Gardez la compatibilitÃ© : pour les bibliothÃ¨ques NuGet ciblant plusieurs versions de .NET, fournissez une implÃ©mentation conditionnelle avec LoggerMessage.Define et, si possible, une variante sourceâ€‘gÃ©nÃ©rÃ©e via #if. Mettez en place des tests et des benchmarks : utilisez BenchmarkDotNet pour vÃ©rifier que votre adoption nâ€™impacte pas la performance globale. Les mesures prÃ©sentÃ©es plus haut montrent un gain substantiel, mais chaque application est unique.ConclusionLâ€™apparition des templates de loggers marque une Ã©volution importante pour la journalisation en .NET. Contrairement aux mÃ©thodes dâ€™extension ILogger&lt;T&gt;, ces templates Ã©liminent les allocations liÃ©es au boxing, cachent lâ€™analyse du modÃ¨le et amÃ©liorent sensiblement les temps dâ€™exÃ©cution. La premiÃ¨re approche, disponible depuis .NET Core 2.1 avec LoggerMessage.Define, offre dÃ©jÃ  un gain apprÃ©ciable, tandis que le sourceâ€‘generator introduit avec .NET 6 simplifie lâ€™usage et apporte un boost supplÃ©mentaire. Des benchmarks rÃ©alisÃ©s par la communautÃ© montrent un temps divisÃ© par dix entre lâ€™appel classique et lâ€™appel gÃ©nÃ©rÃ©, et surtout aucune allocation mÃ©moire supplÃ©mentaire.En adoptant ces patterns, les dÃ©veloppeurs .NET amÃ©liorent les performances de leurs services sans sacrifier la lisibilitÃ© du code. Lâ€™ajout de la rÃ¨gle CA1848 dans votre .editorconfig aidera votre Ã©quipe Ã  dÃ©tecter et corriger les appels non optimisÃ©s. Enfin, centraliser les messages dans des classes dÃ©diÃ©es et utiliser des EventId cohÃ©rents facilite lâ€™analyse des logs et renforce la maintenabilitÃ© des applications." }, { "title": "Retour d'expÃ©rience post-livraisonÂ - bonnes pratiques et amÃ©liorations", "url": "/posts/retour-experience-post-livraison/", "categories": "", "tags": "", "date": "2026-01-12 18:00:00 -0500", "snippet": "PrÃ©ambuleUne livraison majeure marque souvent la fin dâ€™un cycle intense : dÃ©lais serrÃ©s, arbitrages techniques, pression sur la qualitÃ© et sur lâ€™Ã©quipe. Une fois la version en production, la tentat...", "content": "PrÃ©ambuleUne livraison majeure marque souvent la fin dâ€™un cycle intense : dÃ©lais serrÃ©s, arbitrages techniques, pression sur la qualitÃ© et sur lâ€™Ã©quipe. Une fois la version en production, la tentation est grande de tourner rapidement la page pour se projeter vers la prochaine fonctionnalitÃ© ou le prochain projet. Pourtant, ce moment charniÃ¨re est loin dâ€™Ãªtre anodin.La phase post-livraison constitue une opportunitÃ© prÃ©cieuse pour prendre du recul, analyser ce qui sâ€™est rÃ©ellement passÃ© et capitaliser sur lâ€™expÃ©rience acquise. Câ€™est durant cette pÃ©riode que se rÃ©vÃ¨lent les forces du produit, mais aussi les fragilitÃ©s des processus, de lâ€™outillage et de lâ€™organisation. Ignorer cette Ã©tape revient souvent Ã  reproduire les mÃªmes problÃ¨mes dâ€™une livraison Ã  lâ€™autre.Cet article propose un retour dâ€™expÃ©rience structurÃ© sur les bonnes pratiques Ã  adopter aprÃ¨s une livraison importante : priorisation des actions immÃ©diates, amÃ©lioration des pipelines DevOps, renforcement de la qualitÃ© logicielle et de lâ€™observabilitÃ©, ajustements organisationnels et gestion de la documentation dans un contexte multi-produit. Lâ€™objectif nâ€™est pas de viser la perfection, mais de faire de chaque livraison un point dâ€™appui pour progresser collectivement et livrer plus sereinement par la suite.Bonnes pratiques post-livraison pour les Ã©quipes de dÃ©veloppementPrioriser les actions juste aprÃ¨s une livraison majeureAprÃ¨s une livraison majeure, il est important de prendre du recul avant de foncer vers de nouvelles fonctionnalitÃ©s. Dans lâ€™immÃ©diat, lâ€™Ã©quipe doit cÃ©lÃ©brer le travail accompli et analyser le projet : une rÃ©union de retour dâ€™expÃ©rience (post-mortem) permet dâ€™identifier ce qui a bien fonctionnÃ© et ce qui pourrait Ãªtre amÃ©liorÃ© pour les prochains cycles. Câ€™est lâ€™occasion de faire le bilan calmement avant de passer Ã  autre chose. ConcrÃ¨tement, il faut dâ€™abord recueillir les retours : feedback des utilisateurs, mÃ©triques de production, rapports de bugs. Ces informations orientent la suite en priorisant les corrections critiques et les amÃ©liorations Ã  apporter. Par exemple, la phase post-livraison doit se concentrer sur le support et lâ€™amÃ©lioration continue : analyser les bugs ou problÃ¨mes dÃ©tectÃ©s en production, les corriger en prioritÃ©, et utiliser ces retours pour ajuster le backlog des prochaines itÃ©rations. En parallÃ¨le, on veille Ã  mettre Ã  jour la documentation utilisateur et technique, et Ã  adresser les demandes de support client liÃ©es Ã  la nouvelle version.AprÃ¨s une grosse sortie, certaines Ã©quipes planifient mÃªme un sprint de stabilisation ou de maintenance proactive. Lâ€™idÃ©e est de consacrer du temps aux tÃ¢ches qualitatives souvent mises de cÃ´tÃ© pendant la course Ã  la livraison : amÃ©lioration du code, refactoring lÃ©ger, rÃ©duction de la dette technique, renforcement des tests et de la supervision. Il est recommandÃ© dâ€™organiser une rÃ©trospective dâ€™Ã©quipe dÃ©diÃ©e Ã  la livraison qui vient de sâ€™achever. Cette rÃ©tro permet dâ€™aligner tout le monde sur les enseignements du projet, de pointer ce qui a manquÃ© ou pris du retard, et de proposer des ajustements de processus. Par exemple, lâ€™Ã©quipe peut dÃ©cider de raffiner sa Definition of Done si des critÃ¨res de qualitÃ© ont Ã©tÃ© nÃ©gligÃ©s. En Agile, la Definition of Done est une description formelle de lâ€™Ã©tat attendu dâ€™une fonctionnalitÃ© lorsquâ€™elle satisfait Ã  tous les critÃ¨res de qualitÃ© requis. Lâ€™analyser en rÃ©tro aide Ã  voir pourquoi certains items nâ€™Ã©taient pas vraiment â€œdoneâ€ et comment Ã©viter cela. AmÃ©liorer la Definition of Done rÃ©duit les retouches et les dÃ©fauts en exigeant dÃ¨s le dÃ©part un niveau de qualitÃ© plus Ã©levÃ©. En somme, une fois les urgences traitÃ©es, lâ€™Ã©quipe doit faire le point sur ses processus internes pour gagner en maturitÃ© : quâ€™est-ce qui a freinÃ© ou gÃ©nÃ©rÃ© des bugs et comment y remÃ©dier ? Cette dÃ©marche dâ€™amÃ©lioration continue est vitale pour aborder sereinement les dÃ©veloppements suivants.Renforcer les pipelines DevOps et lâ€™automatisation aprÃ¨s coupLes grands dÃ©ploiements mettent souvent Ã  lâ€™Ã©preuve le pipeline CI/CD et les pratiques DevOps de lâ€™Ã©quipe. Juste aprÃ¨s la livraison, câ€™est le moment dâ€™identifier les points faibles et dâ€™apporter des amÃ©liorations aux pipelines, aux scripts de build/dÃ©ploiement, et aux validations automatiques. Lâ€™objectif est de fiabiliser et accÃ©lÃ©rer la prochaine livraison. Par exemple, si des tÃ¢ches manuelles ont Ã©tÃ© nÃ©cessaires durant le dÃ©ploiement, il convient de les automatiser. Adopter ou amÃ©liorer une chaÃ®ne CI/CD solide permet dâ€™Ã©liminer les opÃ©rations rÃ©pÃ©titives (compilation, packaging, dÃ©ploiement manuel, etc.) en les exÃ©cutant de faÃ§on automatique, ce qui libÃ¨re du temps aux dÃ©veloppeurs pour se concentrer sur le code et lâ€™innovation. En pratique, on pourra ajouter des Ã©tapes de validation supplÃ©mentaires dans le pipeline (analyses de code statique, tests de sÃ©curitÃ©, tests de performance automatisÃ©s) afin de dÃ©tecter les problÃ¨mes plus tÃ´t. Raccourcir et fiabiliser les builds fait aussi partie des axes dâ€™amÃ©lioration courants : par exemple en parallÃ©lisant certains tests ou en optimisant lâ€™infrastructure CI. â€œShift-leftâ€ est le principe clÃ© dâ€™intÃ©grer les contrÃ´les qualitÃ© le plus tÃ´t possible dans le cycle de dÃ©veloppement.Investir dans lâ€™outillage DevOps aprÃ¨s une livraison majeure apporte un retour sur investissement direct pour la suite. Des pratiques comme lâ€™intÃ©gration continue renforcÃ©e garantissent que chaque modification de code soumise est automatiquement testÃ©e et validÃ©e, dÃ©tectant plus vite les bugs de rÃ©gression et les conflits dâ€™intÃ©gration. Cela amÃ©liore la qualitÃ© du code et la stabilitÃ© de la branche principale avant mÃªme quâ€™on nâ€™approche la prochaine release. De mÃªme, si le dÃ©ploiement en production a Ã©tÃ© difficile, câ€™est peut-Ãªtre le signe quâ€™il faut amÃ©liorer le processus de dÃ©ploiement continu : scripts dâ€™infrastructure (Infrastructure as Code), stratÃ©gies de feature toggles pour dÃ©ployer sans activer immÃ©diatement, etc. Par ailleurs, une bonne pratique post-livraison est de mesurer les indicateurs du pipeline (durÃ©e des builds, taux de succÃ¨s des tests, temps de dÃ©ploiement) et de fixer des objectifs dâ€™amÃ©lioration. Lâ€™Ã©quipe DevOps peut profiter de ce moment pour revoir la configuration des environnements de test et de prÃ©production, afin quâ€™ils collent davantage Ã  la production et Ã©viter les â€œsurprisesâ€ en dÃ©ploiement. Enfin, il est recommandÃ© de documenter les Ã©tapes du pipeline et dâ€™inclure cette documentation au wiki interne pour faciliter lâ€™onboarding des nouveaux et partager les connaissances sur la livraison continue.Consolider la qualitÃ© logicielle et lâ€™observabilitÃ©AprÃ¨s la frÃ©nÃ©sie dâ€™une sortie en production, revenir aux fondamentaux de la qualitÃ© logicielle est une Ã©tape importante. Cela passe par un renforcement de la batterie de tests : on peut Ã©crire des tests additionnels pour les zones du code qui ont prÃ©sentÃ© des bugs ou qui sont insuffisamment couvertes. Augmenter la couverture de tests (unitaires, intÃ©gration, end-to-end) fiabilisera les prochaines Ã©volutions du logiciel. Il est aussi utile dâ€™analyser les rapports de couverture de code afin de cibler les parties critiques non testÃ©es et dâ€™y remÃ©dier. La phase post-release offre lâ€™occasion de rÃ©aliser des tests de performance approfondis sur la version livrÃ©e, pour sâ€™assurer quâ€™elle tient la charge et quâ€™aucune rÃ©gression de performance nâ€™a Ã©tÃ© introduite. Si des problÃ¨mes de performance en production ont Ã©tÃ© constatÃ©s, ils doivent Ãªtre reproduits en environnement de test et analysÃ©s. On pourra ensuite ajouter des tests de performance automatisÃ©s au pipeline pour prÃ©venir le retour de telles rÃ©gressions Ã  lâ€™avenir.Lâ€™observabilitÃ© du logiciel en production est un autre pilier Ã  consolider. AprÃ¨s un dÃ©ploiement, il est important de mettre en place (ou amÃ©liorer) le monitoring en continu : mÃ©triques systÃ¨me, surveillance des erreurs applicatives, logs centralisÃ©s, etc. Une bonne pratique DevOps est dâ€™intÃ©grer Ã©troitement tests et monitoring, de sorte quâ€™on puisse dÃ©tecter et diagnostiquer rapidement tout Ã©cart de comportement une fois lâ€™application en ligne. Par exemple, ajouter des alertes sur les taux dâ€™erreur ou les temps de rÃ©ponse permet de rÃ©agir avant que les utilisateurs ne soient impactÃ©s. Lâ€™Ã©quipe devrait revoir ses tableaux de bord (dashboards) de suivi post-livraison : sont-ils suffisamment dÃ©taillÃ©s ? manquent-ils de certains indicateurs mÃ©tiers ou techniques pertinents ? Il est Ã©galement temps de vÃ©rifier les outils de traÃ§abilitÃ© et dâ€™observabilitÃ© (par exemple, lâ€™ajout de traces distribuÃ©es si lâ€™architecture est microservices, pour suivre le parcours des requÃªtes). Lâ€™amÃ©lioration de lâ€™observabilitÃ© facilite les prochaines phases de support et de diagnostic en cas dâ€™incident. En somme, consolider la qualitÃ© logicielle aprÃ¨s une release majeure signifie Ã©lever le niveau dâ€™exigence : plus de tests automatisÃ©s et plus de visibilitÃ© sur le fonctionnement interne du produit. Ces efforts proactifs rÃ©duisent les risques de dÃ©fauts futurs et amÃ©liorent la fiabilitÃ© globale de lâ€™application sur le long terme.Ajuster lâ€™organisation dâ€™Ã©quipe et la communication interneUn dÃ©ploiement important est aussi un test pour lâ€™organisation de lâ€™Ã©quipe â€“ câ€™est le moment dâ€™identifier les amÃ©liorations possibles dans la faÃ§on de collaborer. Dâ€™abord, il convient de clarifier la â€œDefinition of Doneâ€ (DoD) de lâ€™Ã©quipe si des divergences ont Ã©tÃ© constatÃ©es. Une DoD bien dÃ©finie (incluant par exemple â€œrevue de code et approuvÃ©e, tests passÃ©s Ã  100%, dÃ©ployer sur un environnement de staging, documentation mise Ã  jourâ€) aligne tout le monde sur les mÃªmes critÃ¨res de qualitÃ©. Si pendant le rush de livraison certains Ã©lÃ©ments sont passÃ©s Ã  la trappe (tests bÃ¢clÃ©s, doc non Ã©criteâ€¦), la rÃ©tro permet de le souligner et de mettre Ã  jour la DoD en consÃ©quence. Cette clarification Ã©vite le rÃ©travail inutile et rÃ©duit les dÃ©fauts en sâ€™assurant que toute fonctionnalitÃ© livrÃ©e respecte un socle de qualitÃ© dÃ©fini Ã  lâ€™avance. Ensuite, le leadership technique doit sâ€™exercer pour guider lâ€™Ã©quipe dans la pÃ©riode de post-release : câ€™est le moment de fÃ©liciter les efforts fournis (â€œcelebrate successâ€), mais aussi de diffuser les apprentissages Ã  lâ€™ensemble de lâ€™organisation. Par exemple, si un membre de lâ€™Ã©quipe a rÃ©solu un problÃ¨me Ã©pineux durant la livraison, il pourrait faire un petit retour dâ€™expÃ©rience en rÃ©union pour partager la leÃ§on apprise.Du point de vue de la communication interne, il est primordial de maintenir un climat de confiance et de transparence, surtout aprÃ¨s les Ã©ventuelles tensions dâ€™une grosse livraison. Les managers et Scrum Masters doivent encourager lâ€™Ã©quipe Ã  exprimer les difficultÃ©s rencontrÃ©es et les suggestions dâ€™amÃ©lioration. Il peut Ãªtre utile dâ€™organiser un dÃ©briefing informel oÃ¹ chacun partage son ressenti sur la release (points positifs et irritants). Pour renforcer la cohÃ©sion dâ€™Ã©quipe, on valorise lâ€™entraide qui a eu lieu et on identifie oÃ¹ des silos dâ€™information ont pu freiner le projet. Par exemple, si un dÃ©veloppeur Ã©tait le seul Ã  maÃ®triser un module critique, on planifiera du pair programming ou une session de transfert de connaissances pour Ã©viter ce point de vulnÃ©rabilitÃ© Ã  lâ€™avenir. La communication agile repose sur lâ€™idÃ©e quâ€™aucune question nâ€™est bÃªte : chaque membre doit se sentir Ã  lâ€™aise de demander de lâ€™aide ou des Ã©claircissements. Encourager systÃ©matiquement les questions amÃ©liore la clartÃ© et peut mener Ã  des dÃ©couvertes ou Ã  des amÃ©liorations de processus insoupÃ§onnÃ©es. Enfin, repensez les rituels dâ€™Ã©quipe si nÃ©cessaire : par exemple, instaurer un point quotidien un peu plus long pendant les phases de stabilisation, ou au contraire rÃ©duire la frÃ©quence de certaines rÃ©unions pour laisser du temps Ã  lâ€™apprentissage de nouvelles technologies (beaucoup dâ€™Ã©quipes accordent du temps de veille technologique aprÃ¨s une release, pour monter en compÃ©tences sur des outils repÃ©rÃ©s, mais nâ€™ayant pas eu le temps dâ€™approfondir). En somme, la pÃ©riode de post-livraison est propice Ã  ajuster lâ€™organisation du travail, que ce soit via une DoD enrichie, une meilleure communication ou un leadership servant Ã  la fois la performance et le bien-Ãªtre de lâ€™Ã©quipe.Documentation technique dans un contexte multi-produitLorsque plusieurs produits ou projets cohabitent au sein de lâ€™entreprise, la gestion de la documentation technique devient plus complexe. Il faut Ã  la fois maintenir la spÃ©cificitÃ© de chaque produit et capitaliser sur des ressources communes (guides, normes, etc.). Voici quelques bonnes pratiques pour structurer et pÃ©renniser une documentation technique multi-produits.Structurer un wiki efficace par produitIl est conseillÃ© dâ€™organiser le wiki interne par produit ou par domaine afin de compartimenter lâ€™information et Ã©viter la confusion. ConcrÃ¨tement, on peut crÃ©er des espaces ou sections dÃ©diÃ©s pour chaque produit (ou chaque Ã©quipe) dans lâ€™outil de documentation. Cela Ã©vite que les pages techniques dâ€™un produit se mÃ©langent avec celles dâ€™un autre, et permet Ã  chaque Ã©quipe de retrouver facilement son pÃ©rimÃ¨tre de documentation. Comme le souligne un guide de bonnes pratiques, si votre entreprise dÃ©veloppe plusieurs produits ou services, il peut Ãªtre judicieux de prÃ©voir des espaces de documentation sÃ©parÃ©s pour chaque produit, de faÃ§on Ã  prÃ©venir lâ€™encombrement et la navigation confuse dans le wiki. En segmentant ainsi par contexte, on amÃ©liore la pertinence de la documentation visible par chaque collaborateur. Par exemple, un nouveau dÃ©veloppeur sur le produit A consultera le wiki du produit A sans tomber sur des pages du produit B qui ne le concernent pas.Bien entendu, ces espaces sÃ©parÃ©s doivent sâ€™accompagner dâ€™une navigation transversale soignÃ©e et dâ€™un moteur de recherche performant. Il faut que le wiki reste facile Ã  parcourir malgrÃ© la quantitÃ© dâ€™informations. Un bon moteur de recherche interne est important pour que chacun trouve rapidement les infos nÃ©cessaires, sans devoir connaÃ®tre lâ€™emplacement exact dans lâ€™arborescence. Pensez Ã©galement Ã  dÃ©finir des gabarits de pages communs (par exemple un modÃ¨le pour les documents dâ€™architecture, un modÃ¨le pour les README de service, etc.) afin dâ€™harmoniser la prÃ©sentation entre les diffÃ©rents produits. Chaque espace produit peut suivre une structure similaire : par exemple â€œPrÃ©sentation gÃ©nÃ©rale â€“ Architecture â€“ Guide de dÃ©marrage â€“ FAQ â€“ Runbookâ€. Cette cohÃ©rence aide les lecteurs Ã  sâ€™y retrouver dâ€™un produit Ã  lâ€™autre. Enfin, une fois lâ€™ossature du wiki multi-produit en place, communiquez auprÃ¨s des Ã©quipes sur comment lâ€™utiliser (quelle info va oÃ¹). Clarifiez aussi les droits dâ€™Ã©dition : dans un wiki, tout le monde peut contribuer, mais on peut restreindre certaines sections sensibles (exemple : pages de normes globales) Ã  la relecture par des rÃ©fÃ©rents.Centraliser les normes de dÃ©veloppement et guides transversesDans un contexte multi-produit, il est essentiel de partager les bonnes pratiques communes et de ne pas dupliquer lâ€™information gÃ©nÃ©rique. Pour cela, on crÃ©e une section centrale dans le wiki (ou un espace dÃ©diÃ©) pour regrouper tous les contenus transverses utiles Ã  lâ€™ensemble des Ã©quipes. Par exemple, on y retrouvera les normes de dÃ©veloppement (guidelines de code, conventions de nommage, formatage du code, processus de revue de code), les guides dâ€™architecture (patrons recommandÃ©s, exigences de sÃ©curitÃ©, politiques de gestion des donnÃ©es), ainsi que les procÃ©dures communes (comment dÃ©ployer en prod, comment ouvrir une demande de changement, etc.). Ce hub documentaire sert de source unique de vÃ©ritÃ© pour les sujets globaux. Ainsi, chaque Ã©quipe produit sait quâ€™elle doit se rÃ©fÃ©rer Ã  cet endroit pour tout ce qui dÃ©passe le cadre spÃ©cifique de son application.Un aspect particuliÃ¨rement important est la documentation pour les nouveaux arrivants (onboarding). Un nouveau dÃ©veloppeur ou architecte qui rejoint lâ€™organisation doit pouvoir trouver rapidement un guide dâ€™onboarding qui rassemble : la prÃ©sentation des produits de lâ€™entreprise, lâ€™architecture globale du systÃ¨me dâ€™information, les outils et accÃ¨s Ã  configurer, le glossaire interne, etc. Le wiki est lâ€™endroit idÃ©al pour hÃ©berger ces guides dâ€™accueil. Il peut Ãªtre utile de crÃ©er un parcours de lecture conseillÃ© aux nouveaux (exemple : â€œGuide du nouvel arrivant â€“ Lire dâ€™abord la section â€œArchitecture globaleâ€, puis â€œStandards de codeâ€, puis se pencher sur le wiki du produit affectÃ©â€¦â€). Un wiki bien structurÃ© peut ainsi faire office de base de connaissances pour accÃ©lÃ©rer lâ€™intÃ©gration. Comme lâ€™explique un article, si le wiki est pensÃ© pour lâ€™onboarding, il contiendra typiquement les documents RH, le matÃ©riel de formation, et des documents de bonnes pratiques Ã  destination des nouveaux venus. Mieux encore, ce rÃ©fÃ©rentiel commun de normes et pratiques doit Ãªtre vivant : il faut encourager les experts de chaque domaine Ã  le tenir Ã  jour. Par exemple, lâ€™expert de sÃ©curitÃ© mettra Ã  jour la section sÃ©curitÃ© dÃ¨s quâ€™une nouvelle rÃ¨gle apparaÃ®t, le rÃ©fÃ©rent qualitÃ© logicielle enrichira les standards de tests au fil du temps, etc.Pour faciliter la centralisation, certaines organisations optent pour des outils dÃ©diÃ©s ou des portails dÃ©veloppeurs. Mais un simple SharePoint ou Wiki Azure DevOps bien organisÃ© peut suffire. Lâ€™important est de garantir que â€œtout le monde se rÃ©fÃ¨re aux mÃªmes donnÃ©esâ€ et que la mise Ã  jour rÃ©guliÃ¨re de ce rÃ©fÃ©rentiel soit inscrite dans le processus. On peut par exemple instituer une revue semestrielle des normes : lâ€™architecte principal et les tech leads parcourent le wiki central et actualisent ce qui doit lâ€™Ãªtre (nouvelles versions de framework, nouvelles conventions suite Ã  une rÃ©tro, etc.). En somme, centraliser la documentation commune Ã©vite la dispersion de lâ€™information et assure une cohÃ©rence entre les diffÃ©rents produits de lâ€™Ã©cosystÃ¨me.Maintenir la documentation Ã  jour de maniÃ¨re collaborativeUn dÃ©fi majeur de la documentation technique est sa mise Ã  jour dans le temps, surtout dans un environnement multi-projet oÃ¹ les changements sont nombreux. Pour Ã©viter que le wiki devienne obsolÃ¨te, plusieurs approches et outils peuvent Ãªtre mis en place. Dâ€™une part, il est recommandÃ© de dÃ©signer clairement des responsables ou rÃ©fÃ©rents pour la documentation (â€œwiki gardenersâ€). Leur rÃ´le est de â€œjardinerâ€ le wiki, câ€™est-Ã -dire de supprimer les pages dÃ©passÃ©es, corriger les liens morts, consolider les doublons et en gÃ©nÃ©ral garder la base de connaissances en bon Ã©tat. Cette responsabilitÃ© peut tourner entre les membres de lâ€™Ã©quipe (par exemple, chaque sprint, une personne diffÃ©rente relit et actualise une partie du wiki) ou Ãªtre formalisÃ©e dans un rÃ´le (certains nomment un documentation owner par produit). Lâ€™important est que la tÃ¢che de maintenance documentaire soit reconnue et planifiÃ©e, et non laissÃ©e au hasard. Une bonne practique est dâ€™insÃ©rer la mise Ã  jour de la doc dans le workflow de dÃ©veloppement : par exemple, inclure dans la Definition of Done quâ€™une fonctionnalitÃ© majeure implique la mise Ã  jour de la page wiki correspondante. De mÃªme, lors des revues de code, les reviewers peuvent ajouter un rappel : â€œAs-tu mis Ã  jour la doc ?â€ si nÃ©cessaire.Au niveau des outils, une astuce pour faciliter la collaboration est dâ€™utiliser des plateformes de wiki offrant des fonctions de suivi des modifications et de commentaires. Un wiki interne doit idÃ©alement permettre Ã  chacun de proposer des changements (Ã©ventuellement via des â€œsuggestionsâ€ ou pull requests sur la doc si on la stocke en Markdown dans un repo). Le versioning des pages et la notification des modifications aux parties prenantes aident Ã  impliquer tout le monde dans la vie du wiki. Par exemple, si la page â€œGuide API Produit Xâ€ est modifiÃ©e, les dÃ©veloppeurs du produit X reÃ§oivent une notification (via email ou Teams) et peuvent rÃ©agir en cas dâ€™erreur. Pour maintenir la doc fiable, il faut Ã©galement encourager une culture oÃ¹ consulter et amÃ©liorer la documentation fait partie du quotidien. Lorsquâ€™un dÃ©veloppeur cherche une info et ne la trouve pas ou la trouve pÃ©rimÃ©e, il devrait se sentir responsable de la crÃ©er ou la corriger pour les suivants. Un moyen efficace est de traiter le wiki comme du code : utiliser un contrÃ´le de source, faire relire les changements significatifs, etc. Certaines organisations intÃ¨grent mÃªme la doc technique dans le mÃªme repo que le code (Docs as Code), ce qui permet de versionner doc et code simultanÃ©ment et de valider les mises Ã  jour de doc via revue de code.Enfin, dans un contexte multi-projets, la gouvernance de la documentation peut Ãªtre un sujet en soi. Il peut Ãªtre utile de tenir un comitÃ© documentation (informel) rÃ©unissant des reprÃ©sentants de chaque Ã©quipe produit, qui se retrouvent ponctuellement pour partager les bonnes pratiques de documentation, les outils, et sâ€™aligner sur la structure. Cela permet de conserver une certaine homogÃ©nÃ©itÃ© et de propager les amÃ©liorations dâ€™une Ã©quipe Ã  lâ€™autre. En rÃ©sumÃ©, garder une documentation Ã  jour requiert organisation et vigilance collective : en dÃ©signant des â€œjardiniersâ€ du wiki, en intÃ©grant la doc aux processus de dÃ©veloppement et en tirant profit des outils collaboratifs, on pÃ©rennise un wiki utile et vivant pour tous.ConclusionLa pÃ©riode post-livraison est souvent perÃ§ue comme une transition rapide vers la prochaine Ã©chÃ©ance. Pourtant, câ€™est prÃ©cisÃ©ment Ã  ce moment-lÃ  que se joue une grande partie de la maturitÃ© technique et organisationnelle dâ€™une Ã©quipe. Prendre le temps dâ€™analyser la livraison, de renforcer les pipelines DevOps, dâ€™amÃ©liorer la qualitÃ© logicielle et de consolider lâ€™observabilitÃ© permet de transformer une release en apprentissage durable, plutÃ´t quâ€™une simple Ã©tape franchie.Au-delÃ  des aspects techniques, lâ€™aprÃ¨s-livraison est aussi un moment humain et collectif. Clarifier la Definition of Done, ajuster les modes de collaboration, partager les retours dâ€™expÃ©rience et renforcer la communication interne contribuent Ã  crÃ©er un environnement plus sain, plus alignÃ© et plus rÃ©silient. Ces ajustements, parfois modestes en apparence, ont un impact direct sur la capacitÃ© de lâ€™Ã©quipe Ã  livrer plus sereinement par la suite.Enfin, dans un contexte multi-produit, la documentation joue un rÃ´le clÃ© pour capitaliser sur lâ€™expÃ©rience acquise. Un wiki structurÃ©, centralisant les normes communes tout en respectant les spÃ©cificitÃ©s de chaque produit, devient un actif stratÃ©gique : il accÃ©lÃ¨re lâ€™onboarding, rÃ©duit les silos et garantit une cohÃ©rence Ã  lâ€™Ã©chelle de lâ€™organisation. Ã€ condition, bien sÃ»r, quâ€™il soit entretenu de maniÃ¨re collaborative et intÃ©grÃ© aux pratiques quotidiennes.En somme, rÃ©ussir lâ€™aprÃ¨s-livraison, ce nâ€™est pas seulement corriger des bugs ou fermer des tickets. Câ€™est faire le choix conscient dâ€™investir dans la qualitÃ©, lâ€™automatisation, la communication et le partage de connaissances. Un investissement qui ne se voit pas toujours immÃ©diatement, mais qui paye largement sur le long terme, en permettant aux Ã©quipes de livrer mieux, plus vite et avec davantage de confiance." }, { "title": "Plan de formation - .NET Framework vers .NET moderne", "url": "/posts/plan-formation-dotnet/", "categories": "", "tags": "dotnet", "date": "2025-12-15 17:00:00 -0500", "snippet": " ğŸ’¡ Cet article marque le dernier de lâ€™annÃ©e 2025 : je prends maintenant un congÃ© bien mÃ©ritÃ© pour la pÃ©riode des fÃªtes et je vous invite Ã  en faire autant. On se retrouve le 12 janvier prochain po...", "content": " ğŸ’¡ Cet article marque le dernier de lâ€™annÃ©e 2025 : je prends maintenant un congÃ© bien mÃ©ritÃ© pour la pÃ©riode des fÃªtes et je vous invite Ã  en faire autant. On se retrouve le 12 janvier prochain pour le prochain article.PrÃ©ambulePasser de .NET Framework Ã  .NET moderne reprÃ©sente une Ã©tape importante pour tout dÃ©veloppeur souhaitant rester Ã  jour dans lâ€™Ã©cosystÃ¨me Microsoft. Ce plan de formation a Ã©tÃ© conÃ§u pour accompagner cette transition de maniÃ¨re progressive et concrÃ¨te.Lâ€™objectif est dâ€™aider les dÃ©veloppeurs et architectes Ã  : Comprendre les diffÃ©rences fondamentales entre .NET Framework et .NET (Core) ; Explorer les nouvelles versions de .NET, leurs cycles de support (LTS / STS) et leurs amÃ©liorations ; DÃ©couvrir les nouveaux outils et pratiques (configuration moderne, DI intÃ©grÃ©e, hosting model, etc.) ; Se familiariser avec les technologies clÃ©s de lâ€™Ã©cosystÃ¨me actuel : ASP.NET Core, Entity Framework Core, TestContainers, Aspire, GitHub Copilot, et plus encore.Quâ€™est-ce que .NET ? Microsoft .NET est une plateforme de dÃ©veloppement offrant un Ã©cosystÃ¨me riche pour crÃ©er des applications de tous types (web, bureau, mobiles, services, etc.). Historiquement, on distinguait .NET Framework (lancÃ© en 2002) et .NET Core (lancÃ© en 2016). Aujourdâ€™hui, le terme Â« .NET Â» dÃ©signe principalement la version moderne issue de .NET Core, unifiÃ©e Ã  partir de .NET 5 en 2020. Ã€ lâ€™inverse, .NET Framework 4.x est lâ€™ancienne version Â« legacy Â», dÃ©sormais figÃ©e (la 4.8 Ã©tant la derniÃ¨re).Quelles sont les grandes diffÃ©rences entre les deux ? Multiplateforme vs Windows uniquement : .NET moderne est cross-platform : une application .NET peut sâ€™exÃ©cuter sur Windows, Linux ou macOS avec les mÃªmes performances. Le dÃ©veloppement nâ€™est plus limitÃ© Ã  Windows : on peut coder sous Visual Studio Code ou dâ€™autres Ã©diteurs sur Linux/macOS, en plus de Visual Studio sous Windows. En comparaison, .NET Framework Ã©tait uniquement compatible Windows. Open source et innovation : Le nouveau .NET est open source et Ã©volue rapidement. La communautÃ© peut contribuer aux amÃ©liorations du runtime et des bibliothÃ¨ques via GitHub. .NET Framework, lui, Ã©tait en grande partie fermÃ© et Ã©volue trÃ¨s peu depuis 2019. Performance et scalabilitÃ© : .NET (Core) a Ã©tÃ© repensÃ© pour Ãªtre beaucoup plus performant. Le runtime optimisÃ© et lâ€™environnement dâ€™exÃ©cution (CLR) font dâ€™ASP.NET Core lâ€™un des frameworks web les plus rapides selon TechEmpower. Ã€ lâ€™inverse, .NET Framework est plus lourd et moins optimisÃ© pour les scÃ©narios modernes (microservices, conteneurs, etc.). ModularitÃ© et dÃ©ploiement : .NET Core introduit une architecture modulaire (via NuGet) et permet le dÃ©ploiement autoâ€contenu (les runtimes nÃ©cessaires peuvent Ãªtre embarquÃ©s avec lâ€™application). .NET Framework, en revanche, est installÃ© au niveau du systÃ¨me et partagÃ© par toutes les applications, ce qui rend les mises Ã  jour dÃ©licates. De plus, .NET moderne autorise dâ€™installer plusieurs versions en parallÃ¨le sur la mÃªme machine (side-by-side), chaque application peut cibler sa version de runtime sans conflit. Avec .NET Framework, une seule version 4.x est active Ã  la fois sur Windows, limitant la flexibilitÃ©. APIs et fonctionnalitÃ©s disponibles : .NET Core a rattrapÃ© et mÃªme dÃ©passÃ© .NET Framework sur la plupart des fonctionnalitÃ©s. Certains anciens modules (Web Forms, WCF, Workflowâ€¦) ne sont disponibles que sous .NET Framework, mais ils ont souvent des alternatives modernes (par ex. gRPC Ã  la place de WCF). Le nouveau .NET inclut Ã©galement de nouveaux frameworks (ASP.NET Core, Blazor, MAUI pour le multiplateforme, etc.) non prÃ©sents dans lâ€™ancien.En rÃ©sumÃ©, .NET Â« Core Â» unifiÃ© est lâ€™avenir de lâ€™Ã©cosystÃ¨me .NET, idÃ©al pour tout nouveau projet, tandis que .NET Framework sert uniquement Ã  maintenir dâ€™anciennes applications sur Windows.Ã€ Ã©couter :Ã‰volution des versions .NET 6 Ã  .NET 10Pour chaque version majeure de .NET (de la version 6 Ã  la version 10), vous trouverez ci-dessous au moins une vidÃ©o explicative des nouveautÃ©s ainsi que lâ€™article dÃ©taillÃ© de Stephen Toub sur les amÃ©liorations techniques (notamment de performance) de cette version..NET 6 (2021 - 2024) VidÃ©os : Whatâ€™s New in .NET 6 and C# 10? - .NET 6 deep dive et Whatâ€™s new in C# 10 Article : Performance Improvements in .NET 6 - Performance Improvements in .NET 6 - .NET Blog.NET 7 (2022 - 2024) VidÃ©os : Whatâ€™s New in .NET 7 C# 11? - .NET 7 Overview et Whatâ€™s New in C# 11 Article : Performance Improvements in .NET 7 - Performance Improvements in .NET 7 - .NET Blog.NET 8 (2023 - 2026) VidÃ©os : Whatâ€™s New in .NET 8 C# 12? - Whatâ€™s new in .NET 8 et Every New Feature Added in C# 1. Article : Performance Improvements in .NET 8 - Performance Improvements in .NET 8 - .NET Blog..NET 9 (2024 - 2026) VidÃ©os : Whatâ€™s New in .NET 9 C# 13? - Whatâ€™s new in .NET 9 &amp; C# 13 et Whatâ€™s New in .NET 9. Article : Performance Improvements in .NET 9 - Performance Improvements in .NET 9 et Performance Improvements in .NET 9 - .NET Blog..NET 10 (2025 - 2028) VidÃ©o : Whatâ€™s New in .NET 10 C# 14? - csproj is GONE! â€˜dotnet run app.csâ€™ is Here Article : Performance Improvements in .NET 10 - Performance Improvements in .NET 10 - .NET BlogLe cycle de support de LTS versus STS avec .NET CoreDepuis .NET Core 3.1, Microsoft suit un cycle de publication fixe pour .NET avec une alternance entre versions LTS (Long-Term Support) et STS (Standard-Term Support).LTS - Long-Term Support Support officiel de 3 ans IdÃ©al pour les applications critiques ou Ã  cycle de vie long Mises Ã  jour de sÃ©curitÃ© et corrections sans changements de rupture Exemples : .NET 6 (LTS), .NET 8 (LTS), .NET 10 (LTS)STS - Standard-Term Support Support Ã©tendu Ã  24 mois depuis .NET 9 (contre 18 mois auparavant) AccÃ¨s anticipÃ© aux nouveautÃ©s du runtime et du langage AdaptÃ© aux projets agiles pouvant adopter un rythme de mise Ã  jour rÃ©gulier Exemples : .NET 7 (18 mois), .NET 9 (24 mois)VidÃ©o explicative recommandÃ©e :DiffÃ©rences entre les outils de lâ€™Ã©cosystÃ¨me .Net Framework Ã  .NET CoreLe passage de .NET Framework Ã  .NET Core sâ€™accompagne de nombreux changements dâ€™outils, de conventions et de choix technologiques par dÃ©faut.Voici un aperÃ§u des Ã©volutions les plus notables :Remplacements technologiques frÃ©quents Outil / Approche (Framework) Ã‰volution / Remplacement en .NET Core+ Commentaire Autofac (DI) DI natif intÃ©grÃ© Ã  ASP.NET Core Autofac est toujours compatible, mais plus nÃ©cessaire pour la plupart des cas Web.config, app.config appsettings.json + configuration par injection Plus flexible, hiÃ©rarchique, supporte IOptions System.Web SupprimÃ©, remplacÃ© par ASP.NET Core Hosting &amp; Middleware Architecture lÃ©gÃ¨re, cross-platform Global.asax RemplacÃ© par Program.cs et Startup.cs ou minimal hosting DÃ©marrage centralisÃ© simplifiÃ© IIS exclusivement Cross-platform (Kestrel, Nginx, Apache, etc.) Kestrel est souvent utilisÃ© en production csproj verbeux Nouveau format SDK-style (&lt;Project Sdk=\"...\"&gt;) Plus simple, support multi-targeting MSBuild-based NuGet restore dotnet restore intÃ©grÃ© au build Plus rapide, CLI unifiÃ©e Project.json (obsolÃ¨te depuis Core 1.x) FusionnÃ© dans le nouveau format csproj moderne Stabilisation du modÃ¨le de projet Changements frÃ©quents dans lâ€™Ã©cosystÃ¨me .NET Core et modernesMÃªme entre les versions de .NET Core, certains outils ou comportements Ã©voluent : Swagger / Swashbuckle : Ã©tait activÃ© par dÃ©faut dans certains templates .NET 5/.NET 6 (notamment dans les APIs), ce nâ€™est plus le cas en .NET 9/10. Il faut lâ€™ajouter et le configurer manuellement. Hosting Model : depuis .NET 6, le modÃ¨le minimal de dÃ©marrage (Program.cs simplifiÃ©) est devenu la norme, remplaÃ§ant la combinaison Startup.cs + Program.cs. HTTP Logging natif : ajoutÃ© dans .NET 6, souvent oubliÃ©. Prise en charge des tests avec dotnet test : meilleure intÃ©gration CLI, y compris pour les tests en parallÃ¨le. Nouvelle maniÃ¨re de gÃ©rer les secrets utilisateurs : dotnet user-secrets introduit depuis .NET Core 2, mais plus encouragÃ© avec les secrets Azure pour les apps cloud.Changement de licences dans lâ€™Ã©cosystÃ¨me .NETUn point important Ã  connaÃ®tre : Microsoft et dâ€™autres Ã©diteurs ont modifiÃ© les licences de nombreux packages dans lâ€™Ã©cosystÃ¨me .NET, ce qui a un impact sur : Lâ€™utilisation commerciale de certains outils open source La compatibilitÃ© avec certaines entreprises ou environnements sensibles Les pratiques de contribution ou de forkArticle recommandÃ© : https://codewithfrenchy.com/posts/changement-de-licence-ecosysteme-dotnet/DÃ©veloppement web avec ASP.NET CoreASP.NET Core est la plateforme web principale de .NET moderne. Elle permet de crÃ©er des applications performantes, testables, sÃ©curisÃ©es et facilement dÃ©ployables, grÃ¢ce Ã  une architecture modulaire, multiplateforme et cloud-ready.VidÃ©o recommandÃ©e :AccÃ¨s aux donnÃ©es avec Entity Framework CoreEntity Framework Core (EF Core) est lâ€™ORM moderne de Microsoft pour .NET. Il remplace EF6 dans lâ€™Ã©cosystÃ¨me .NET Core et sâ€™intÃ¨gre naturellement Ã  lâ€™injection de dÃ©pendances, Ã  la configuration moderne et aux API REST.VidÃ©o recommandÃ©e :Migration dâ€™une application ASP.NET MVC (Framework) vers ASP.NET CoreMigrer une application .NET Framework Ã  .NET Core est un projet structurant. Il ne sâ€™agit pas dâ€™un simple â€œportageâ€, mais bien dâ€™une reconstruction partielle dans un Ã©cosystÃ¨me diffÃ©rent, plus modulaire, plus performant et multiplateforme.Playlist recommandÃ©e : https://www.youtube.com/watch?v=zHgYDZK3MrA&amp;list=PLdo4fOcmZ0oWiK8r9OkJM3MUUL7_bOT9zDÃ©ploiement local et dans le cloud (Azure App Services)DÃ©ployer une application ASP.NET Core en 2025 peut se faire trÃ¨s facilement, aussi bien localement quâ€™en environnement cloud. Comprendre les options disponibles et les outils recommandÃ©s est essentiel pour industrialiser son projet.VidÃ©o recommandÃ©e :BonusStructure moderne des solutionsPasser Ã  .NET 6 jusquâ€™Ã  .NET 10 implique dâ€™adopter de nouvelles conventions de structure et dâ€™organisation du code. Une architecture propre facilite la maintenabilitÃ©, les tests, la scalabilitÃ© et les bonnes pratiques DevOps.VidÃ©o recommandÃ©e :RoadmapLâ€™Ã©cosystÃ¨me .NET Ã©volue rapidement, notamment avec .NET 8/9/10, les microservices, le cloud et la montÃ©e en puissance de la performance et de la productivitÃ©. En 2025, un dÃ©veloppeur .NET moderne devrait idÃ©alement maÃ®triser plusieurs axes, Ã  la fois techniques et mÃ©thodologiques.VidÃ©o recommandÃ©e :Moderniser une application .NET avec GitHub CopilotGitHub Copilot, basÃ© sur lâ€™IA, peut assister les dÃ©veloppeurs dans la modernisation dâ€™une application existante en lâ€™accÃ©lÃ©rant.VidÃ©o recommandÃ©e :Faciliter les essaisDans les versions rÃ©centes de .NET, Microsoft a introduit plusieurs outils pour faciliter les tests automatisÃ©s sans avoir besoin de dÃ©pendances tierces.Deux nouveautÃ©s trÃ¨s utiles sont : TimeProvider / FakeTimeProvider ILogger&lt;T&gt; / FakeLogger&lt;T&gt;Articles recommandÃ©s : https://grantwinney.com/how-to-use-timeprovider-and-faketimeprovider/ et https://www.freecodecamp.org/news/how-to-use-fakelogger-to-make-testing-easier-in-netTester contre des conteneurs rÃ©els avec les TestContainersTestContainers est une bibliothÃ¨que .NET qui permet de lancer des services Docker (bases de donnÃ©es, RabbitMQ, Redis, etc.) dans un environnement de test, Ã  la volÃ©e. Elle garantit des tests rÃ©alistes, isolÃ©s et reproductibles.Câ€™est une solution idÃ©ale pour remplacer : Les scripts SQL manuels dâ€™initialisation Les bases de donnÃ©es en dur sur la machine du dÃ©veloppeur Les mocks peu fiables de services externesArticle recommandÃ© : https://codewithfrenchy.com/posts/introduction-testcontainers/Aspire : orchestrer des applicationsAspire est une stack dâ€™orchestration proposÃ©e par Microsoft, conÃ§ue pour faciliter le dÃ©veloppement, la configuration, lâ€™observation et le dÃ©ploiement dâ€™applications cloud natives composÃ©es de plusieurs projets ou services.Câ€™est une rÃ©ponse directe aux dÃ©fis rencontrÃ©s avec les microservices, les BFF, les APIs, les workers, etc.Playlist : Migrating From Docker Compose to Aspire (my experience) - YouTube" }, { "title": "Azure DevOps vs GitHub - convergence en cours et perspectives", "url": "/posts/azure-devops-vs-github/", "categories": "", "tags": "", "date": "2025-12-01 16:00:00 -0500", "snippet": "PrÃ©ambuleâ€œFaut-il migrer vers GitHub et abandonner Azure DevOps ?â€ Cette question mâ€™est revenue Ã  plusieurs reprises ces derniers temps, que ce soit lors de discussions entre collÃ¨gues ou de rencon...", "content": "PrÃ©ambuleâ€œFaut-il migrer vers GitHub et abandonner Azure DevOps ?â€ Cette question mâ€™est revenue Ã  plusieurs reprises ces derniers temps, que ce soit lors de discussions entre collÃ¨gues ou de rencontres avec des clients. Depuis le rachat de GitHub par Microsoft en 2018, la cohabitation de ces deux plateformes suscite en effet de nombreuses interrogations. Certains y voient des solutions redondantes, dâ€™autres redoutent la disparition de lâ€™une au profit de lâ€™autre. En tant quâ€™architecte logiciel ayant fait cÃ´toyer les deux outils au quotidien, je vous propose dans cet article de partager ma vision de ce â€œduel fraternelâ€. Nous verrons que plus quâ€™un duel, il sâ€™agit dâ€™une convergence progressive, et quâ€™il nâ€™y a aucune urgence Ã  basculer prÃ©cipitamment dâ€™un cÃ´tÃ© ou de lâ€™autre.Deux plateformes, deux approches diffÃ©rentesAzure DevOps et GitHub ont des origines et des forces distinctes, ce qui explique quâ€™ils continuent dâ€™exister cÃ´te Ã  cÃ´te. Azure DevOps est lâ€™hÃ©ritier de TFS/VSTS, orientÃ© cycle de vie complet dâ€™une application de la planification Ã  la livraison. Il propose une suite intÃ©grÃ©e pour les Ã©quipes agiles : gestion de projet (Azure Boards avec backlogs, sprints, Kanban), rÃ©fÃ©rentiels Git (Azure Repos), pipelines CI/CD (Azure Pipelines), gestion de packages (Artifacts) et tests (Test Plans). Lâ€™outil a Ã©tÃ© pensÃ© pour les entreprises et Ã©quipes structurÃ©es, avec un haut niveau de contrÃ´le (droits fins sur les repos, gestion des branches, approbations de dÃ©ploiement, etc.). En somme, Azure DevOps brille par sa capacitÃ© Ã  organiser le travail et Ã  encadrer des processus complexes.De son cÃ´tÃ©, GitHub sâ€™est imposÃ© comme la plateforme de rÃ©fÃ©rence pour le code et la collaboration entre dÃ©veloppeurs. Historiquement axÃ© sur lâ€™open source, GitHub offre une expÃ©rience Git trÃ¨s fluide et centrÃ©e sur les dÃ©pÃ´ts, les pull requests et la revue de code. Son atout principal rÃ©side dans sa simplicitÃ© dâ€™adoption et sa popularitÃ© dans la communautÃ© des dÃ©veloppeurs. Ces derniÃ¨res annÃ©es, GitHub a Ã©tendu ses fonctionnalitÃ©s pour sÃ©duire aussi les organisations : introduction de GitHub Actions pour le CI/CD, de GitHub Packages, de GitHub Projects/Issues pour la gestion de tÃ¢ches, ou encore dâ€™outils de sÃ©curitÃ© comme Dependabot et Advanced Security. NÃ©anmoins, en 2025, GitHub reste avant tout perÃ§u comme un outil de gestion de code et dâ€™automatisation, moins complet quâ€™Azure DevOps sur lâ€™aspect planification de projet ou gestion de portefeuille. Azure DevOps et GitHub ne sont donc pas exactement interchangeables : le premier excelle dans la structure et la traÃ§abilitÃ© de projets internalisÃ©s, le second dans la collaboration ouverte et le dÃ©veloppement pilotÃ© par la communautÃ©. Beaucoup dâ€™Ã©quipes choisissent dâ€™ailleurs lâ€™un ou lâ€™autre selon leurs besoins spÃ©cifiques (par exemple, gouvernance stricte vs. ouverture, mÃ©thodologie Scrum/Kanban vs. workflow Git simple, etc).Une intÃ©gration de plus en plus Ã©troiteSi Azure DevOps et GitHub ont des positions diffÃ©rentes, Microsoft a clairement affichÃ© son intention de rapprocher les deux plateformes plutÃ´t que de les opposer. Ã€ court et moyen terme, on observe dâ€™ailleurs une intÃ©gration croissante entre les deux produits. Plusieurs fonctionnalitÃ©s autrefois exclusives Ã  lâ€™un sont dÃ©sormais accessibles aux utilisateurs de lâ€™autre, et des passerelles officielles simplifient la collaboration mixte. IntÃ©gration des outils de planification et de code : Il est tout Ã  fait possible aujourdâ€™hui dâ€™utiliser Azure Boards pour suivre des travaux liÃ©s Ã  un dÃ©pÃ´t GitHub, ou de dÃ©clencher des Azure Pipelines Ã  partir dâ€™un code hÃ©bergÃ© sur GitHub. Microsoft a publiÃ© des connecteurs natifs pour lier commits, pull requests et issues GitHub avec les Ã©lÃ©ments de travail Azure DevOps. Ã€ lâ€™inverse, GitHub propose des GitHub Actions qui peuvent dÃ©ployer sur Azure, et son module GitHub Projects sâ€™inspire partiellement des tableaux Azure Boards (mÃªme sâ€™il reste moins avancÃ©). Cette intÃ©gration permet aux Ã©quipes de combiner les points forts de chaque plateforme en fonction de leurs besoins. FonctionnalitÃ©s DevSecOps mutualisÃ©es : Microsoft a commencÃ© Ã  rendre disponibles sur Azure DevOps certains outils initialement dÃ©veloppÃ©s pour GitHub. Par exemple, GitHub Advanced Security (scans de code, dÃ©tection de secrets, analyse de dÃ©pendances) est dÃ©sormais disponible dans Azure DevOps pour analyser les repositories Azure Repos. ConcrÃ¨tement, un projet Azure DevOps peut activer ces analyses de sÃ©curitÃ© avancÃ©es dans ses dÃ©pÃ´ts Git, lÃ  oÃ¹ il fallait auparavant passer par GitHub. De mÃªme, lâ€™assistant intelligent GitHub Copilot sâ€™invite dans Azure DevOps (extensions IDE liÃ©es Ã  Azure Repos, ou prochainement via le service Azure DevOps MCP permettant Ã  Copilot dâ€™interroger les donnÃ©es Azure DevOps). Ces exemples illustrent la volontÃ© dâ€™offrir aux utilisateurs dâ€™Azure DevOps les avancÃ©es technologiques de GitHub, sans quâ€™ils aient Ã  changer de plateforme du jour au lendemain. Licences et offres convergentes : Un pas important vers la convergence a Ã©tÃ© fait sur le plan des licences. Depuis fÃ©vrier 2025, les clients GitHub Enterprise disposent automatiquement des droits dâ€™utilisation basiques dâ€™Azure DevOps. Autrement dit, une entreprise abonnÃ©e Ã  GitHub Enterprise peut utiliser Azure Boards et Azure Pipelines sans coÃ»t supplÃ©mentaire. Cela vise Ã  Ã©liminer les freins financiers Ã  une approche hybride. Plus de 200 000 utilisateurs profitent dÃ©jÃ  de cette intÃ©gration, accÃ©dant Ã  Azure DevOps via leur licence GitHub. Inversement, Microsoft a annoncÃ© que les possesseurs de licences Azure DevOps Server (on-premises) auront des tarifs prÃ©fÃ©rentiels pour GitHub (offres Ã  confirmer). Ce cross-licensing rÃ©duit la barriÃ¨re entre les deux mondes et encourage les organisations Ã  utiliser â€œle meilleur des deuxâ€ plutÃ´t que de se limiter Ã  lâ€™un ou lâ€™autre. Exemple dâ€™environnement hybride combinant un dÃ©pÃ´t GitHub (avec ses fonctionnalitÃ©s de code, CI et sÃ©curitÃ©) et les services Azure DevOps pour la gestion de projet (Boards) et les pipelines de dÃ©ploiement. Cette intÃ©gration permet aux Ã©quipes dâ€™exploiter GitHub pour le code et la collaboration tout en conservant les outils Ã©prouvÃ©s dâ€™Azure DevOps pour la planification et la livraison. Les dÃ©veloppeurs bÃ©nÃ©ficient ainsi de GitHub (Codespaces, Actions, Dependabot, etc.), tandis que les chefs de projet et opÃ©rationnels profitent dâ€™Azure Boards et Pipelines au sein dâ€™un Ã©cosystÃ¨me unifiÃ©. Outils de migration : Consciente que de nombreuses organisations envisagent un Ã©ventuel transfert de lâ€™un vers lâ€™autre, Microsoft a commencÃ© Ã  fournir des outils de migration. Le plus abouti est GitHub Enterprise Importer, qui permet de migrer aisÃ©ment les dÃ©pÃ´ts Azure Repos vers GitHub en conservant tout lâ€™historique, les branches et mÃ©tadonnÃ©es importantes. Des centaines de milliers de dÃ©pÃ´ts Azure DevOps ont dÃ©jÃ  Ã©tÃ© migrÃ©s via cet outil. Pour la migration des Ã©lÃ©ments de travail (user stories, tÃ¢ches, bugs) dâ€™Azure Boards vers GitHub Issues, il existe pour le moment des scripts communautaires ou des solutions tierces, mais nul doute que Microsoft simplifiera cela si la demande augmente. En bref, Microsoft prÃ©pare le terrain pour que toute Ã©quipe puisse, le moment venu, passer de lâ€™une Ã  lâ€™autre plateforme sans douleur. Le playbook officiel de migration : Fin 2025, Microsoft a publiÃ© un playbook officiel pour guider les organisations souhaitant migrer progressivement dâ€™Azure DevOps vers GitHub. Les messages principaux sont clairs : La migration doit Ãªtre progressive, en commenÃ§ant par les dÃ©pÃ´ts (repos), puis Ã©ventuellement les pipelines. GitHub Enterprise Importer est lâ€™outil recommandÃ© pour migrer Azure Repos tout en conservant lâ€™historique et les mÃ©tadonnÃ©es Certaines briques (comme Azure Boards ou Test Plans) ne disposent pas encore dâ€™un Ã©quivalent de migration Â« clÃ© en main Â», et Microsoft recommande, lorsquâ€™elles sont critiques, de continuer Ã  les utiliser en parallÃ¨le de GitHub. Le scÃ©nario hybride (GitHub pour le code et la sÃ©curitÃ© + Azure Boards/Pipelines pour la planification et la livraison) est confirmÃ© comme une approche pleinement supportÃ©e par Microsoft. Ce playbook ne force pas la migration : il propose simplement un cadre pour Ã©voluer Ã  son propre rythme. Tous ces efforts dâ€™intÃ©gration montrent bien que Microsoft ne cherche pas Ã  forcer un choix exclusif entre Azure DevOps et GitHub, mais Ã  renforcer leur complÃ©mentaritÃ©. Le message lors de la confÃ©rence Ignite 2024 Ã©tait clair Ã  ce sujet : â€œGitHub et Azure DevOps ne sont pas des concurrents, mais des alliÃ©sâ€ au service des dÃ©veloppeurs et des entreprises. Il est dÃ©sormais possible (et courant) dâ€™avoir un projet dont le code source et les CI runs sont sur GitHub, tandis que la gestion de projet et le suivi des tÃ¢ches restent sur Azure DevOps, le tout fonctionnant de pair de maniÃ¨re fluide. Microsoft parle dâ€™ailleurs dâ€™un â€œÃ©cosystÃ¨me connectÃ©â€ unique. PlutÃ´t que de remplacer brutalement lâ€™un par lâ€™autre, lâ€™Ã©diteur crÃ©e progressivement une expÃ©rience unifiÃ©e pour les deux produits.Une nouvelle Ã¨re : lâ€™Agentic DevOpsDepuis 2025, Microsoft introduit de plus en plus la notion dâ€™Agentic DevOps, une approche oÃ¹ lâ€™IA joue un rÃ´le actif dans les workflows de dÃ©veloppement.Dans ce modÃ¨le, GitHub devient naturellement la plateforme privilÃ©giÃ©e, car : GitHub Actions sâ€™intÃ¨gre profondÃ©ment avec Copilot, les modÃ¨les IA (GitHub Models / Azure) peuvent Ãªtre appelÃ©s directement dans les workflows, la structure repo-centric de GitHub facilite les scÃ©narios â€œcontextualisÃ©sâ€.Azure DevOps nâ€™est pas mis de cÃ´tÃ©, mais cette nouvelle orientation explique pourquoi Microsoft fait porter lâ€™innovation dâ€™abord sur GitHub pour tout ce qui concerne lâ€™automatisation intelligente.Ã€ terme, vers une convergence autour de GitHub ?Que peut-on prÃ©voir Ã  plus long terme ? Ã€ mon avis, la trajectoire actuelle laisse penser que GitHub finira par devenir la plateforme centrale pour la gestion du code et des pipelines, tandis quâ€™il hÃ©ritera des fonctionnalitÃ©s avancÃ©es dâ€™Azure DevOps en matiÃ¨re de planification et de gestion de projet. En clair, GitHub pourrait englober la plupart des cas dâ€™usage, lÃ  oÃ¹ Azure DevOps se fondrait en arriÃ¨re-plan.Plusieurs indices pointent vers ce scÃ©nario : GitHub privilÃ©giÃ© pour le code et la CI/CD : Microsoft investit massivement dans GitHub (nouvelles fonctionnalitÃ©s orientÃ©es dÃ©veloppeurs, intÃ©gration de Copilot dans les workflows GitHub, etc.) et encourage dÃ©jÃ  fortement les clients Azure DevOps Ã  migrer leurs dÃ©pÃ´ts Git vers GitHub. Le fait que GitHub soit la pierre angulaire pour bÃ©nÃ©ficier des derniers outils (exemple : Copilot repo-centric, Advanced Security) confirme que le code source et lâ€™automatisation seront principalement centrÃ©s sur GitHub Ã  lâ€™avenir. Azure Repos et Azure Pipelines, sans disparaÃ®tre du jour au lendemain, pourraient passer en second plan au profit de GitHub (repos) et GitHub Actions. Renforcement des capacitÃ©s de GitHub en gestion de projet : En parallÃ¨le, GitHub Ã©toffe progressivement ses fonctionnalitÃ©s de project management pour sÃ©duire les Ã©quipes structurÃ©es. Lâ€™apparition des GitHub Projects (beta), dâ€™une vue tableau de bord plus flexible, ou lâ€™amÃ©lioration des GitHub Issues (notions dâ€™Ã©pique, champs personnalisÃ©s via Projects) vont dans le sens de combler lâ€™Ã©cart avec Azure Boards. Certes, en 2025 Azure Boards reste plus mature (hiÃ©rarchie de backlogs, capacitÃ©s Scrum natives, requÃªtes WIQL puissantes, etc.), mais on peut sâ€™attendre Ã  ce que GitHub continue de sâ€™enrichir sur ce volet. Il nâ€™est pas farfelu dâ€™imaginer quâ€™un jour Microsoft propose une fusion partielle des deux : par exemple, une intÃ©gration native des work items Azure DevOps dans lâ€™interface GitHub, ou lâ€™ajout Ã  GitHub de modules â€œPlansâ€ et â€œBoardsâ€ repris dâ€™Azure DevOps. Dâ€™ailleurs, les mÃ©canismes de liens automatiques entre commits et work items, ou entre PR et tickets, existent dÃ©jÃ  lorsque lâ€™on connecte Azure Boards Ã  GitHub, preuve que les frontiÃ¨res sâ€™estompent. Ã‰cosystÃ¨me et communautÃ© : GitHub dispose dâ€™une Ã©norme communautÃ© dâ€™utilisateurs et dâ€™un Ã©cosystÃ¨me de Marketplace trÃ¨s riche. En recentrant lâ€™effort sur GitHub, Microsoft capitalise sur cette dynamique communautaire. Azure DevOps, plus confidentiel en termes de communautÃ© (extension Marketplace plus restreinte, moins dâ€™engouement open source), a intÃ©rÃªt Ã  â€œse grefferâ€ Ã  GitHub pour bÃ©nÃ©ficier de cet Ã©lan. On peut imaginer quâ€™Ã  terme, toute lâ€™innovation DevOps de Microsoft se fera sur GitHub, et Azure DevOps recevra ces innovations en tant quâ€™intÃ©grations additionnelles si nÃ©cessaire pour les grands comptes existants.Bien entendu, Azure DevOps ne va pas disparaÃ®tre brutalement. Microsoft lâ€™a bien compris : de nombreux clients entreprises sont trÃ¨s attachÃ©s Ã  Azure DevOps, et une dÃ©prÃ©ciation forcÃ©e les pousserait possiblement vers des solutions concurrentes (Jira/Bitbucket, GitLab, etc.), ce que Microsoft veut Ã©viter. Câ€™est pourquoi la transition est douce et sur la durÃ©e (on parle en annÃ©es). Aucune annonce officielle ne prÃ©voit la fin dâ€™Azure DevOps Ã  court terme, et les deux produits reÃ§oivent encore des mises Ã  jour. Par exemple, Microsoft continue dâ€™amÃ©liorer Azure DevOps (nouvelles fonctionnalitÃ©s dans Boards, interface modernisÃ©e, support de GitHub Advanced Security, etc.), signe quâ€™Azure DevOps reste dans la feuille de route. Simplement, son rÃ´le Ã©volue : il devient le complÃ©ment de GitHub pour les usages avancÃ©s non couverts nativement par GitHub.Ã€ terme, il ne serait pas surprenant de voir Microsoft proposer des outils de migration complets pour basculer dâ€™une plateforme Ã  lâ€™autre, voire une unification partielle. Nous avons dÃ©jÃ  les migrations de dÃ©pÃ´ts automatisÃ©es. Peut-Ãªtre verrons-nous apparaÃ®tre un jour un assistant de migration des work items Azure Boards vers GitHub, ou une convergence des pipelines (par exemple un outil pour convertir un pipeline YAML Azure en workflow GitHub Action). En tout cas, plus le temps passera, plus les deux solutions convergeront en termes de fonctionnalitÃ©s. Il est mÃªme possible quâ€™Ã  long terme, la distinction devienne purement marketing (deux offres avec des noms diffÃ©rents pour adresser diffÃ©rents publics, mais reposant sur un socle technique commun et interopÃ©rable).ConclusionEn 2025, Azure DevOps et GitHub ne sâ€™opposent pas, ils se complÃ¨tent. Azure DevOps apporte la rigueur et la profondeur fonctionnelle recherchÃ©es par les grandes Ã©quipes agiles, tandis que GitHub offre lâ€™innovation, lâ€™ouverture et la convivialitÃ© plÃ©biscitÃ©es par les dÃ©veloppeurs. Microsoft lâ€™a bien saisi et fait converger ces deux univers pour crÃ©er une expÃ©rience cohÃ©rente. Il nâ€™y a donc aucune urgence Ã  abandonner lâ€™un pour lâ€™autre Ã  court terme : vous pouvez tout Ã  fait continuer Ã  exploiter Azure DevOps si câ€™est votre outillage principal, ou GitHub si vous y trouvez votre compte ou mÃªme les deux ensemble. Les passerelles mises en place (intÃ©grations techniques, licences unifiÃ©es) vous assurent que votre choix aujourdâ€™hui ne vous fermera pas de porte pour demain.Mon conseil serait de choisir en fonction de vos besoins actuels, tout en gardant un Å“il sur lâ€™Ã©volution de la convergence. Si votre Ã©quipe apprÃ©cie la gestion de projet dâ€™Azure DevOps, rien ne vous empÃªche de lâ€™utiliser tout en hÃ©bergeant votre code sur GitHub pour profiter de Copilot et de la communautÃ©, le tout fonctionne dÃ©sormais de maniÃ¨re intÃ©grÃ©e. Si au contraire vous dÃ©marrez un projet avec GitHub pour sa simplicitÃ©, vous pourrez plus tard raccrocher un Azure Boards ou migrer certaines donnÃ©es si le besoin sâ€™en fait sentir. Microsoft continue dâ€™investir pour que, le moment venu, le passage de lâ€™un Ã  lâ€™autre se fasse sans encombre. En fin de compte, â€œAzure DevOps vs GitHubâ€ nâ€™est pas un combat avec un perdant et un gagnant, câ€™est une convergence annoncÃ©e oÃ¹ lâ€™utilisateur final sort vainqueur, avec une plateforme DevOps Microsoft unifiÃ©e plus puissante et flexible quâ€™auparavant.Enfin, avec le migration playbook publiÃ© par Microsoft, la coexistence Azure DevOps â†” GitHub et les transitions progressives sont dÃ©sormais des scÃ©narios officiellement encouragÃ©s, ce qui confirme encore davantage lâ€™idÃ©e dâ€™une convergence maÃ®trisÃ©e plutÃ´t que dâ€™un remplacement brutal." }, { "title": "FrÃ©quenter les mises en production - pourquoi livrer souvent change la donne", "url": "/posts/pourquoi-livrer-souvent/", "categories": "", "tags": "", "date": "2025-11-17 18:00:00 -0500", "snippet": "Introduction - le paradoxe de la livraison agileEn thÃ©orie, les mÃ©thodes agiles visent Ã  livrer de la valeur rapidement et Ã  apprendre du feedback utilisateur. Pourtant, de nombreuses Ã©quipes conti...", "content": "Introduction - le paradoxe de la livraison agileEn thÃ©orie, les mÃ©thodes agiles visent Ã  livrer de la valeur rapidement et Ã  apprendre du feedback utilisateur. Pourtant, de nombreuses Ã©quipes continuent dâ€™accumuler des mois de travail sous forme de trains de livraison. Ce mode de fonctionnement crÃ©e de grands lots de changements, multiplie les incertitudes et empÃªche de valider les hypothÃ¨ses en continu. Ã€ lâ€™inverse, livrer frÃ©quemment permet de rÃ©duire la taille des livraisons et de crÃ©er des cycles de rÃ©troaction rapides. Lâ€™objectif de cet article est de convaincre les dÃ©veloppeurs, les Ã©quipes produit et les dÃ©cideurs que la livraison continue est non seulement rÃ©aliste, mais surtout bÃ©nÃ©fique pour la qualitÃ©, lâ€™organisation et les finances.Les bÃ©nÃ©fices de la livraison frÃ©quenteLa livraison frÃ©quente consiste Ã  produire des mises en production rÃ©guliÃ¨res et de petite taille. Cette pratique sâ€™appuie sur des pipelines dâ€™intÃ©gration et de dÃ©ploiement continus (CI/CD) et implique des changements culturels.ğŸ’¡ Les avantages sont nombreux !1. Des bÃ©nÃ©fices techniques RÃ©duction des risques et robustesse accrue : des mises Ã  jour plus petites sont plus faciles Ã  comprendre, Ã  tester et Ã  corriger. Les Ã©quipes peuvent revenir ou corriger rapidement en cas de problÃ¨me, ce qui rÃ©duit lâ€™impact des incidents. Des Ã©tudes montrent que les organisations performantes qui dÃ©ploient plusieurs fois par jour ont un taux dâ€™Ã©chec de changement plus faible et rÃ©parent plus vite que leurs pairs. QualitÃ© du code et stabilitÃ© : lâ€™automatisation des tests et des dÃ©ploiements dans un pipeline CI/CD amÃ©liore la qualitÃ© et la fiabilitÃ© du logiciel. Selon une Ã©tude de CloudBees, lâ€™automatisation standardisÃ©e rÃ©duit les coÃ»ts de prÃ©â€‘dÃ©ploiement de 70 % et les interruptions de service des dÃ©veloppeurs de 99 %. Feedback accÃ©lÃ©rÃ© : livrer rÃ©guliÃ¨rement permet de valider rapidement les hypothÃ¨ses. Lâ€™article de Product Compass rappelle que la dÃ©couverte produit ne suffit pas Ã  Ã©liminer le risque : seule une mise en production permet de mesurer lâ€™usage rÃ©el. Recevoir un retour rapide permet dâ€™amÃ©liorer en continu la solution.2. Des bÃ©nÃ©fices organisationnels Collaboration et responsabilisation : les Ã©quipes qui livrent souvent rÃ©duisent les silos. La culture â€œsupporte ce que tu construisâ€ mise en place chez Netflix a rÃ©duit les temps dâ€™attente entre le dÃ©veloppement et la mise en production, donnant aux Ã©quipes lâ€™autonomie de dÃ©ployer en quelques minutes au lieu de semaines. FluiditÃ© des processus : un pipeline continu supprime le besoin de longues phases dâ€™intÃ©gration et de test en fin de cycle. Thoughtworks souligne que les organisations Ã  haute performance livrent 30 fois plus souvent et rÃ©alisent des dÃ©ploiements 8 000 fois plus rapides, avec 50 % de dÃ©faillances en moins et un temps de restauration 12 fois plus court. Apprentissage et innovation : livrer vite permet dâ€™expÃ©rimenter (A/B testing, feature flags) et dâ€™Ã©prouver rapidement les idÃ©es. Les dÃ©veloppeurs peuvent tester des fonctionnalitÃ©s auprÃ¨s dâ€™un sousâ€‘ensemble dâ€™utilisateurs, recueillir des donnÃ©es, puis itÃ©rer.3. Des bÃ©nÃ©fices Ã©conomiques CoÃ»t de dÃ©veloppement rÃ©duit : lâ€™exemple dâ€™HP LaserJet montre quâ€™en adoptant la livraison continue, lâ€™Ã©quipe a rÃ©duit ses coÃ»ts de dÃ©veloppement de 40 %, augmentÃ© de 140 % le nombre de programmes dÃ©veloppÃ©s et rÃ©duit de 78 % le coÃ»t par programme. Retour sur investissement (ROI) : Forrester a calculÃ© quâ€™en trois ans, lâ€™utilisation de CloudBees CI/CD gÃ©nÃ©rerait un ROI de 426 % avec une valeur nette de 30,9 millions de dollars et une rÃ©duction du temps de dÃ©ploiement de moitiÃ©. Satisfaction et fidÃ©lisation client : selon 3Pillar Global, des mises Ã  jour frÃ©quentes permettent dâ€™intÃ©grer rapidement les retours des utilisateurs, dâ€™amÃ©liorer lâ€™expÃ©rience et dâ€™accroÃ®tre la fidÃ©litÃ© Ã  long terme. Des changements incrÃ©mentaux Ã©vitent aux utilisateurs la frustration de modifications massives et prolongent leur engagement.Les coÃ»ts et inefficacitÃ©s dâ€™une livraison peu frÃ©quenteLâ€™approche â€œbigâ€‘bangâ€ accumule des risques et crÃ©e des inefficacitÃ©s qui nuisent Ã  lâ€™agilitÃ©.Risques techniques Incertitude accrue : regrouper de nombreux changements rend difficile la dÃ©tection de lâ€™origine dâ€™un problÃ¨me. Le Gouvernement Digital Service (GDS) souligne que les grosses mises en production multiplient les risques et rendent les rollbacks plus complexes. Taux dâ€™Ã©chec plus Ã©levÃ© : des versions volumineuses contiennent davantage de code non testÃ© en situation rÃ©elle, augmentant la probabilitÃ© de dÃ©faillances graves. Les corrections peuvent prendre des heures ou des jours, crÃ©ant des interruptions pour les utilisateurs.InefficacitÃ©s organisationnelles Ralentissement du cycle de feedback : attendre plusieurs semaines ou mois avant de livrer retarde les retours des utilisateurs et reporte la dÃ©tection des erreurs. Cela oblige les Ã©quipes Ã  travailler dans lâ€™incertitude et risque dâ€™investir dans des fonctionnalitÃ©s inutiles. Stress et mobilisations nocturnes : lâ€™article de Bill Elafros montre que les dÃ©ploiements rares exigent souvent des mises en production hors des heures de bureau, engendrant un stress et une charge de travail importante pour les Ã©quipes. Manque dâ€™automatisation : les organisations qui dÃ©ploient peu investissent moins dans lâ€™automatisation. Les procÃ©dures manuelles sont alors plus longues, sources dâ€™erreurs et coÃ»teuses.ConsÃ©quences Ã©conomiques CoÃ»t des Ã©checs : un incident sur une version majeure peut coÃ»ter des millions en interruption de service et en temps de correction. Les pertes de productivitÃ© et de crÃ©dibilitÃ© se traduisent par un manque Ã  gagner difficile Ã  rattraper. Retard face Ã  la concurrence : pendant quâ€™une organisation prÃ©pare une version semestrielle, des concurrents qui livrent quotidiennement expÃ©rimentent, ajustent et captent des parts de marchÃ©. Les long cycles rendent le roadmap rigide et rendent lâ€™entreprise moins rÃ©active.Comment adopter la livraison continue ?Passer Ã  des mises en production frÃ©quentes nÃ©cessite des changements techniques et culturels. Automatiser lâ€™intÃ©gration et le dÃ©ploiement : mettre en place un pipeline CI/CD afin de valider, tester et dÃ©ployer chaque modification automatiquement. Des outils comme Azure Pipelines ou GitHub Actions. Lâ€™automatisation supprime les tÃ¢ches manuelles et rÃ©duit les erreurs. RÃ©duire la taille des lots : DORA recommande de rÃ©duire la taille des changements pour amÃ©liorer le dÃ©lai de mise en production et la stabilitÃ©. Cela implique de dÃ©couper les fonctionnalitÃ©s en incrÃ©ments livrables et de favoriser la livraison en continu. Utiliser des feature flags et le dÃ©ploiement progressif : ces techniques permettent dâ€™activer ou de dÃ©sactiver une fonctionnalitÃ© sans dÃ©ployer du nouveau code, de tester en production sur un segment dâ€™utilisateurs et de rÃ©duire lâ€™impact dâ€™Ã©ventuels problÃ¨mes. Mettre en place une culture dâ€™apprentissage : inciter les Ã©quipes Ã  considÃ©rer la production comme un outil dâ€™apprentissage. Les feedbacks des utilisateurs et les mÃ©triques (temps de rÃ©ponse, taux dâ€™erreur, adoption) doivent Ãªtre accessibles pour guider les dÃ©cisions. Les post-mortem sont une formidable occasion dâ€™apprentissage collectif et un levier pour lâ€™amÃ©lioration continue. Investir dans lâ€™observabilitÃ© et la sÃ©curitÃ© : une surveillance continue (logs, mÃ©triques, traces) permet de dÃ©tecter rapidement les anomalies. Les Ã©quipes doivent Ã©galement intÃ©grer des tests de sÃ©curitÃ© automatisÃ©s pour protÃ©ger les donnÃ©es.Conclusion : livrer souvent pour apprendre et prospÃ©rerLâ€™agilitÃ© nâ€™est pas seulement une affaire de postâ€‘its ou de cÃ©rÃ©monies, elle se manifeste par la capacitÃ© Ã  livrer de la valeur rapidement et frÃ©quemment. Les Ã©tudes et les cas dâ€™entreprise montrent que la livraison continue amÃ©liore la qualitÃ©, rÃ©duit les risques, renforce la collaboration et maximise le retour sur investissement. Les organisations qui continuent de planifier des livraisons occasionnelles sâ€™exposent Ã  des risques techniques, organisationnels et Ã©conomiques Ã©levÃ©s. En changeant de culture, en adoptant lâ€™automatisation et en diminuant la taille des lots, il est possible de transformer radicalement la maniÃ¨re de concevoir et de dÃ©livrer des produits.ğŸ‘‰ La question nâ€™est plus de savoir si vous devez livrer plus souvent, mais comment commencer dÃ¨s maintenant." }, { "title": "Essais automatisÃ©s dâ€™architecture dans un projet .NET", "url": "/posts/essais-architecture-automatises-dotnet/", "categories": "outil-developpement, architecture", "tags": "dotnet, essais", "date": "2025-11-03 18:00:00 -0500", "snippet": "IntroductionDans les projets .NET complexes, par exemple appliquant le Domain-Driven Design (DDD) ou une Clean Architecture stricte, il devient crucial de sâ€™assurer que la structure du code respect...", "content": "IntroductionDans les projets .NET complexes, par exemple appliquant le Domain-Driven Design (DDD) ou une Clean Architecture stricte, il devient crucial de sâ€™assurer que la structure du code respecte en permanence les rÃ¨gles dâ€™architecture prÃ©vues. Ã€ mesure quâ€™une base de code grandit et que de nombreux dÃ©veloppeurs contribuent, le risque de dÃ©rive architecturale augmente : sans contrÃ´le, on peut introduire des dÃ©pendances entre couches qui violent le design initial, avec Ã  la clÃ© une perte de maintenabilitÃ© et de lisibilitÃ© du code. Lâ€™idÃ©al serait de valider automatiquement lâ€™architecture pour prÃ©venir ces dÃ©rives dÃ¨s quâ€™elles apparaissent.Exemple de reprÃ©sentation Clean Architecture en cercles concentriques (modÃ¨le Onion). Les dÃ©pendances pointent vers le noyau du systÃ¨me : la partie cÅ“ur mÃ©tier au centre nâ€™a aucune dÃ©pendance sur les couches extÃ©rieures, tandis que les couches externes (UI, Infrastructure) peuvent dÃ©pendre de ce qui est plus central.Dans une architecture de type Clean Architecture, on sâ€™attend par exemple Ã  ce que la couche Domaine ou Application Core ne dÃ©pende dâ€™aucun dÃ©tail dâ€™infrastructure ou dâ€™interface utilisateur. Ce sont au contraire les couches extÃ©rieures (UI, Infrastructure) qui dÃ©pendent des abstractions du domaine. Cette organisation en couches strictes assure une meilleure encapsulation : si lâ€™on change lâ€™implÃ©mentation de la persistance des donnÃ©es ou du framework de prÃ©sentation, le cÅ“ur de lâ€™application nâ€™en souffre pas. Pour garantir cela dans la durÃ©e, les Ã©quipes matures mettent en place des tests automatisÃ©s dâ€™architecture qui vont vÃ©rifier en continu que ces rÃ¨gles de dÃ©pendances et de structure sont bien respectÃ©es.Pourquoi des tests dâ€™architecture automatisÃ©s ?Chaque Ã©quipe dÃ©finit des conventions sur la faÃ§on dâ€™organiser le code. Souvent ces rÃ¨gles sont documentÃ©es sur un wiki ou transmises oralement, mais on sait que les dÃ©veloppeurs lisent peu la documentation et que ces rÃ¨gles â€œnon Ã©critesâ€ finissent par varier dâ€™une personne Ã  lâ€™autre. Les revues de code peuvent attraper certains Ã©carts, mais ce processus manuel est lent, fastidieux, et laisse place Ã  des incohÃ©rences.Automatiser la vÃ©rification de lâ€™architecture apporte une solution Ã©lÃ©gante : on transforme ces conventions en tests unitaires exÃ©cutables. Ainsi, les rÃ¨gles dâ€™architecture de lâ€™Ã©quipe sont â€œgravÃ©es dans le marbreâ€ et ne peuvent plus Ãªtre ignorÃ©es ou mal interprÃ©tÃ©es. Quand un nouveau dÃ©veloppeur rejoint le projet ou quâ€™une refactorisation majeure a lieu, les tests dâ€™architecture servent de garde-fous immÃ©diats.Les avantages sont multiples : cela crÃ©e une documentation vivante des dÃ©cisions dâ€™architecture (les tests dÃ©crivent les contraintes Ã  respecter dans le code). On bÃ©nÃ©ficie des mÃªmes gains que pour les autres tests automatisÃ©s : gain de temps et retour rapide en cas de rÃ©gression, sans devoir jouer en permanence le rÃ´le du gendarme en revue de code. Lâ€™application des rÃ¨gles devient uniforme pour tous les membres de lâ€™Ã©quipe, amÃ©liorant la cohÃ©rence du code. En attrapant trÃ¨s tÃ´t les violations (dÃ¨s le build ou la PR), on Ã©vite que des erreurs de conception ne sâ€™accumulent et ne transforment le code en un â€œbig ball of mudâ€ ingÃ©rable sur le long terme. En rÃ©sumÃ©, ces tests automatisÃ©s sÃ©curisent vos refactorings et garantissent lâ€™intÃ©gritÃ© architecturale du systÃ¨me dans la durÃ©e.Tour dâ€™horizon des outils disponiblesEn environnement .NET, deux principales bibliothÃ¨ques se partagent le domaine des tests dâ€™architecture automatisÃ©s : NetArchTest et ArchUnitNET.NetArchTest est un outil qui fournit une API fluide pour dÃ©finir des rÃ¨gles architecturales sous forme de tests unitaires. Il permet par exemple dâ€™exprimer quâ€™une classe de service ne doit pas dÃ©pendre dâ€™un contrÃ´leur, ou quâ€™une classe de la couche Domaine ne doit pas se trouver dans le namespace de lâ€™Infrastructure. Cependant, NetArchTest semble en perte de vitesse, aucune mise Ã  jour nâ€™a Ã©tÃ© publiÃ©e depuis 2023, le dÃ©pÃ´t GitHub nâ€™ayant plus de commits depuis plus de deux ans. Cela laisse penser que lâ€™outil nâ€™est plus activement maintenu.ArchUnitNET, de son cÃ´tÃ©, est une librairie plus rÃ©cente et soutenue par la communautÃ© (crÃ©Ã©e par TNG Tech) pour tester lâ€™architecture des applications C#. Câ€™est un portage de lâ€™outil Java ArchUnit, bien connu dans lâ€™Ã©cosystÃ¨me JVM. ArchUnitNET permet de dÃ©finir via une API fluent des contraintes solides entre vos classes, modules et namespaces, et de valider automatiquement que votre code reste conforme Ã  lâ€™architecture voulue. Lâ€™outil est open-source (Apache 2.0) et bÃ©nÃ©ficie dâ€™un support actif : au moment oÃ¹ jâ€™Ã©cris, le dÃ©pÃ´t reÃ§oit des contributions rÃ©guliÃ¨res (dernier commit il y a seulement quelques heures !). Face Ã  un NetArchTest vieillissant, le choix dâ€™ArchUnitNET sâ€™est imposÃ© pour notre projet.Pourquoi jâ€™ai choisi ArchUnitNETVoici les principaux atouts qui nous ont fait adopter ArchUnitNET : Support actif et pÃ©rennitÃ© â€“ ArchUnitNET est activement maintenu par ses contributeurs et alignÃ© sur ArchUnit (Java) qui fait rÃ©fÃ©rence. On Ã©vite ainsi le risque dâ€™un outil abandonnÃ© en rase campagne Ã  lâ€™inverse de NetArchTest, restÃ© figÃ© depuis 2023. La frÃ©quence des commits et des releases dâ€™ArchUnitNET tÃ©moigne dâ€™une communautÃ© dynamique (derniÃ¨re release trÃ¨s rÃ©cente, projet mis Ã  jour en continu). API fluide et expressive â€“ La syntaxe proposÃ©e par ArchUnitNET est Ã  la fois puissante et lisible. Elle permet de dÃ©finir finement des rÃ¨gles dâ€™architecture en code C# clair, grÃ¢ce Ã  un DSL (Domain-Specific Language) interne inspirÃ© de ArchUnit Java. Par exemple, on peut Ã©crire que â€ les classes du namespace X devraient rÃ©sider dans telle couche et ne pas dÃ©pendre de tel autre namespace â€œ en une seule ligne expressive. Cette API fluent rend la crÃ©ation de rÃ¨gles assez naturelle, tout en produisant des messages dâ€™erreur explicites en cas de non-respect (on peut mÃªme ajouter un commentaire avec Because(â€œraisonâ€) pour documenter la rÃ¨gle violÃ©e). IntÃ©gration facile dans le projet â€“ ArchUnitNET sâ€™installe comme nâ€™importe quel package NuGet et sâ€™utilise au sein de vos projets de tests existants. Il propose des extensions pour les frameworks de tests populaires (xUnit, NUnit, MSTest) afin de sâ€™intÃ©grer au mieux dans votre pipeline de tests. En pratique, il suffit dâ€™ajouter le package NuGet et de charger les assemblys de votre application dans le moteur ArchUnitNET, puis dâ€™Ã©crire des tests classiques qui exÃ©cutent les vÃ©rifications dâ€™architecture. Aucune Ã©tape lourde ou outil externe nâ€™est requis, ce qui rend son adoption trÃ¨s simple. PortÃ©e fonctionnelle â€“ ArchUnitNET couvre un large Ã©ventail de possibilitÃ©s : contrÃ´le des dÃ©pendances entre assemblies/namespaces, conventions de nommage des classes, structure des couches, rÃ¨gles sur lâ€™hÃ©ritage, prÃ©sence dâ€™attributs spÃ©cifiques, etc. Cette richesse permet de traduire pratiquement toutes les rÃ¨gles dâ€™architecture imaginables en tests automatisÃ©s. De plus, Ã©tant un port direct dâ€™ArchUnit, on bÃ©nÃ©ficie dâ€™un certain recul et de bonnes pratiques dÃ©jÃ  Ã©prouvÃ©es.En rÃ©sumÃ©, ArchUnitNET sâ€™est distinguÃ© pour nous par son support actif, son expressivitÃ© et sa facilitÃ© dâ€™adoption dans notre Ã©cosystÃ¨me .NET, ce qui en fait un choix de confiance pour automatiser nos tests dâ€™architecture.Exemple complet avec ArchUnitNETPour illustrer lâ€™utilisation dâ€™ArchUnitNET, prenons un exemple fictif de projet .NET organisÃ© selon une architecture en couches faÃ§on Clean Architecture. Imaginons une application API Web qui comporte trois couches principales : Application, Domaine, Infrastructure. La couche Application contient la logique applicative (cas dâ€™usage, services applicatifs) et dÃ©pend du Domaine. La couche Domaine encapsule les rÃ¨gles mÃ©tiers (entitÃ©s, agrÃ©gats, interfaces de rÃ©fÃ©rentiels, etc.) et est indÃ©pendante. La couche Infrastructure fournit les implÃ©mentations concrÃ¨tes (accÃ¨s aux donnÃ©es, services externes) et dÃ©pend de Application et Domaine. Lâ€™API (controllers web) fait office de couche de prÃ©sentation et interagit avec Application.Notre objectif est dâ€™Ã©crire des tests pour vÃ©rifier automatiquement quelques rÃ¨gles dâ€™architecture de ce projet : IndÃ©pendance des couches - Par exemple, la couche Application ne doit jamais dÃ©pendre directement de la couche Infrastructure. Ainsi, aucune classe du namespace MonProjet.Application ne doit rÃ©fÃ©rencer une classe de MonProjet.Infrastructure (respect de la dÃ©pendance inverse). Conventions de localisation â€“ Les classes de contrÃ´leur ASP.NET (suffixÃ©es par Controller) doivent rÃ©sider dans le namespace MonProjet.Api.Controllers prÃ©vu Ã  cet effet, et pas ailleurs.CommenÃ§ons par installer ArchUnitNET dans notre projet de tests. Si vous utilisez xUnit, je vous recommande lâ€™extension dÃ©diÃ©e ArchUnitNET.xUnit qui fournit tout le nÃ©cessaire pour Ã©crire des [Fact] vÃ©rifiant lâ€™architecture :dotnet add package TngTech.ArchUnitNET.xUnitUne fois le package ajoutÃ©, nous allons pouvoir dÃ©finir lâ€™architecture Ã  analyser dans nos tests. ArchUnitNET va charger nos assemblies (.dll) et construire un modÃ¨le interne des types et de leurs dÃ©pendances. On peut ensuite formuler des rÃ¨gles via lâ€™API fluent. Typiquement, on crÃ©e une classe de test ArchitectureTests oÃ¹ lâ€™on initialise en static lâ€™objet Architecture et Ã©ventuellement quelques providers de types rÃ©utilisables reprÃ©sentant nos couches :using ArchUnitNET.Domain;using ArchUnitNET.Loader;using ArchUnitNET.Fluent;using static ArchUnitNET.Fluent.ArchRuleDefinition;using Xunit;namespace MonProjet.ArchTests{ // Chargement initial de l'architecture (assemblies Application, Domain, Infrastructure) public class ArchitectureTests { private static readonly Architecture Architecture = new ArchLoader() .LoadAssemblies( System.Reflection.Assembly.Load(\"MonProjet.Application\"), System.Reflection.Assembly.Load(\"MonProjet.Domain\"), System.Reflection.Assembly.Load(\"MonProjet.Infrastructure\") ) .Build(); // DÃ©finition de \"filtres\" rÃ©utilisables pour nos couches private readonly IObjectProvider&lt;IType&gt; ApplicationLayer = Types().That().ResideInNamespace(\"MonProjet.Application\").As(\"Couche Application\"); private readonly IObjectProvider&lt;IType&gt; InfrastructureLayer = Types().That().ResideInNamespace(\"MonProjet.Infrastructure\").As(\"Couche Infrastructure\"); // ... on pourrait dÃ©finir de mÃªme DomainLayer, etc. // 1. Test de rÃ¨gle d'architecture : Application ne dÃ©pend pas d'Infrastructure [Fact] public void Application_NeDoitPasDependre_DeInfrastructure() { IArchRule regleCoucheApp = Types().That().Are(ApplicationLayer) .Should().NotDependOnAny(InfrastructureLayer) .Because(\"la couche Application doit rester indÃ©pendante de l'Infrastructure\"); regleCoucheApp.Check(Architecture); } // 2. Test de convention : les Controllers sont dans le namespace MonProjet.Api.Controllers [Fact] public void Controllers_DansNamespaceControllers() { IArchRule regleControllers = Classes().That().HaveNameEndingWith(\"Controller\") .Should().ResideInNamespace(\"MonProjet.Api.Controllers\") .Because(\"les contrÃ´leurs web doivent Ãªtre dÃ©clarÃ©s dans MonProjet.Api.Controllers\"); regleControllers.Check(Architecture); } }}Dans le premier test (Application_NeDoitPasDependre_DeInfrastructure), on utilise la mÃ©thode fluide Types().That().Are(ApplicationLayer) pour sÃ©lectionner tous les types de la couche Application, puis on exprime la contrainte Should().NotDependOnAny(InfrastructureLayer). Autrement dit, aucun de ces types ne doit avoir de dÃ©pendance vers un type de la couche Infrastructure, ce qui empÃªche par exemple une classe applicative dâ€™utiliser directement un repository concret dÃ©fini dans lâ€™infrastructure. Le .Because(\"...\") permet dâ€™ajouter un message explicatif qui sâ€™affichera si la rÃ¨gle est violÃ©e, ce qui documente le pourquoi de la contrainte (ici â€œla couche Application doit rester indÃ©pendanteâ€). Enfin, on termine par Check(Architecture) pour exÃ©cuter la rÃ¨gle sur lâ€™architecture chargÃ©e. Si une dÃ©pendance interdite est dÃ©tectÃ©e, le test Ã©chouera en listant prÃ©cisÃ©ment quelles classes posent problÃ¨me, facilitant la correction.Le second test (Controllers_DansNamespaceControllers) illustre une vÃ©rification de convention de nommage et de localisation. On sÃ©lectionne toutes les classes dont le nom se termine par Controller puis on impose quâ€™elles rÃ©sident dans le namespace MonProjet.Api.Controllers. LÃ  encore, si par mÃ©garde un contrÃ´leur Ã©tait placÃ© dans un autre namespace (ou mal nommÃ©), le test le signalerait immÃ©diatement. Ce genre de rÃ¨gle agit comme une documentation vivante sur lâ€™organisation attendue des composants : si quelquâ€™un crÃ©e un nouveau contrÃ´leur au mauvais endroit, le test le rappellera Ã  lâ€™ordre sans attendre la revue de code.Ces tests dâ€™architecture sâ€™exÃ©cutent comme nâ€™importe quels tests unitaires via votre runner habituel (ou en CI). Lorsquâ€™une rÃ¨gle Ã©choue, ArchUnitNET fournit un message dâ€™erreur explicite listant les Ã©lÃ©ments fautifs et la raison fournie. Lâ€™Ã©quipe dispose donc dâ€™un feedback clair et actionnable pour corriger lâ€™architecture avant mÃªme que le code nâ€™atterrisse en production.Checklist : Ce que je valide via les tests dâ€™architecturePour rÃ©capituler, voici une checklist de quelques rÃ¨gles que jâ€™automatise dans les projets .NET Ã  lâ€™aide de tests dâ€™architecture : SÃ©paration stricte des couches â€“ Aucune dÃ©pendance illÃ©gitime entre couches nâ€™est tolÃ©rÃ©e. Par exemple, la couche Domaine nâ€™importe pas de classe de lâ€™Infrastructure, lâ€™UI nâ€™accÃ¨de pas directement Ã  la DAL sans passer par la couche Application, etc. Chaque niveau ne connaÃ®t que les contrats (interfaces) dÃ©finis aux niveaux infÃ©rieurs, jamais les implÃ©mentations concrÃ¨tes des niveaux supÃ©rieurs. Conventions de nommage et de localisation â€“ Les classes de contrÃ´leur ASP.NET MVC/Web API doivent Ãªtre suffixÃ©es par Controller et rÃ©sider dans le namespace dÃ©diÃ© (exemple : MonProjet.Api.Controllers). De mÃªme, on peut imposer que les classes de service se terminent par Service, que les interfaces commencent par un I, etc., afin de garder une cohÃ©rence dans tout le code. Pas de dÃ©pendances transverses non dÃ©sirÃ©es â€“ On vÃ©rifie quâ€™aucun composant ne â€œcourt-circuiteâ€ lâ€™architecture en appelant du code quâ€™il ne devrait pas. Par exemple, un service de lâ€™UI ne va pas directement appeler une classe de Domaine sans passer par Application, un contrÃ´leur ne doit pas accÃ©der directement Ã  la base de donnÃ©es, etc. Ces rÃ¨gles encapsulent les bonnes pratiques de lâ€™architecture pour Ã©viter les couplages cachÃ©s. Isolation du cÅ“ur mÃ©tier â€“ Les couches externes (Interface Adapters, comme les contrÃ´leurs/web, ou la couche Infrastructure) ne doivent pas rÃ©fÃ©rencer directement les entitÃ©s du domaine. On prÃ©fÃ¨re quâ€™elles passent par des DTOs ou des interfaces. Cela Ã©vite de fuiter la logique interne du domaine vers lâ€™extÃ©rieur et renforce le principe dâ€™inversion des dÃ©pendances (le domaine expose des abstractions que les autres implÃ©mentent, sans quoi câ€™est le domaine qui se mettrait Ã  dÃ©pendre de dÃ©tails externes). Autres conventions spÃ©cifiques â€“ Tout ce qui relÃ¨ve de rÃ¨gles dâ€™architecture propres Ã  votre projet peut faire lâ€™objet dâ€™un test. Par exemple, vÃ©rifier que les repositories implÃ©mentent bien les interfaces du domaine, que les classes dans tel namespace sont sealed ou encore quâ€™aucun service applicatif nâ€™est statique. Lâ€™important est de capturer ces invariants architecturaux sous forme de tests pour sâ€™assurer quâ€™ils seront vrais en permanence.Bien sÃ»r, la checklist ci-dessus dÃ©pend du contexte de chaque projet. Ã€ vous dâ€™identifier les points nÃ©vralgiques de votre architecture et de les couvrir par des tests adÃ©quats. Lâ€™avantage est quâ€™une fois en place, ce filet de sÃ©curitÃ© tourne en continu et vous alerte au moindre Ã©cart.ConclusionAprÃ¨s plusieurs mois dâ€™utilisation, je peux confirmer que les tests dâ€™architecture automatisÃ©s apportent une rÃ©elle valeur Ã  long terme. Au dÃ©but, cela demande un petit investissement (formaliser les rÃ¨gles, Ã©crire les tests), mais cet effort est largement rentabilisÃ© par la suite. Notre code reste plus propre et cohÃ©rente : fini les dÃ©pendances hasardeuses introduites par inadvertance, car le pipeline de build les attrape immÃ©diatement. Cela facilite aussi les refactorings de grande ampleur, on peut restructurer le code en confiance, en sachant que si une rÃ¨gle dâ€™architecture est brisÃ©e, un test rouge nous le signalera aussitÃ´t.Mon retour dâ€™expÃ©rience est que ces tests jouent un rÃ´le de garde-fou invisible mais nÃ©cessaires. Ils soulÃ¨vent les problÃ¨mes dâ€™architecture trÃ¨s en amont (dÃ¨s la PR ou mÃªme avant, en local) et Ã©vitent des revues de code interminables sur la structure du projet. Lâ€™Ã©quipe gagne en sÃ©rÃ©nitÃ© et peut se concentrer sur le mÃ©tier, sachant que le respect des principes dâ€™architecture est surveillÃ© automatiquement.Pour les Ã©quipes .NET dÃ©jÃ  bien rodÃ©es qui souhaitent franchir un cap supplÃ©mentaire en qualitÃ©, je conseille dâ€™expÃ©rimenter les tests dâ€™architecture. Commencez petit Ã  petit, avec quelques rÃ¨gles simples sur les dÃ©pendances entre couches par exemple, puis Ã©largissez la portÃ©e progressivement. Impliquez lâ€™Ã©quipe dans la dÃ©finition de ces rÃ¨gles, cela peut mÃªme servir de base Ã  des discussions saines sur lâ€™architecture souhaitÃ©e. IntÃ©grez enfin ces tests au pipeline CI/CD afin quâ€™ils tournent Ã  chaque commit et empÃªchent toute rÃ©gression architecturale.En conclusion, les tests dâ€™architecture automatisÃ©s sont un outil de plus dans lâ€™arsenal des Ã©quipes .NET matures pour assurer la pÃ©rennitÃ© du design logiciel. Ils apportent une forme de qualitÃ© logicielle souvent nÃ©gligÃ©e, en faisant vivre les principes dâ€™architecture au mÃªme titre que le code mÃ©tier. Si votre projet commence Ã  prendre de lâ€™ampleur, nâ€™hÃ©sitez pas Ã  les adopter : votre architecture (et vos coÃ©quipiers) vous remercieront sur le long terme !" }, { "title": "Concevoir et maintenir des applications microservices performantes en .NET", "url": "/posts/concevoir-maintenir-application-performante-dotnet/", "categories": "architecture", "tags": "dotnet", "date": "2025-10-20 19:00:00 -0400", "snippet": "La performance logicielle ne doit pas Ãªtre une rÃ©flexion aprÃ¨s coup : elle se conÃ§oit dÃ¨s le dÃ©part et se cultive tout au long de la vie du systÃ¨me. Dans cet article, nous explorons comment sâ€™outil...", "content": "La performance logicielle ne doit pas Ãªtre une rÃ©flexion aprÃ¨s coup : elle se conÃ§oit dÃ¨s le dÃ©part et se cultive tout au long de la vie du systÃ¨me. Dans cet article, nous explorons comment sâ€™outiller pour identifier les problÃ¨mes de performance et les bonnes pratiques pour bÃ¢tir une application scalable, optimisÃ©e et rÃ©siliente. Nous aborderons la conception axÃ©e sur la scalabilitÃ©, lâ€™amÃ©lioration continue des performances et lâ€™automatisation pour la rÃ©silience, puis dÃ©taillerons les Ã©tapes clÃ©s Ã  chaque phase (conception, dÃ©veloppement, dÃ©ploiement) afin dâ€™assurer des applications microservices .NET hautement performantes.Concevoir pour la scalabilitÃ© dÃ¨s le dÃ©partDÃ¨s les premiÃ¨res phases de conception, il est nÃ©cessaire dâ€™intÃ©grer la performance et la scalabilitÃ© dans lâ€™architecture du logiciel : Utiliser des mÃ©triques et de lâ€™observabilitÃ© dÃ¨s le dÃ©but : PrÃ©voir dâ€™emblÃ©e lâ€™instrumentation de lâ€™application (logs, mÃ©triques, tracing distribuÃ©) facilite le suivi des performances. Mettre en place des mÃ©triques clÃ©s (temps de rÃ©ponse, taux dâ€™erreur, utilisation CPU/mÃ©moire, etc.) et centraliser les logs sont des pratiques indispensables pour diagnostiquer les problÃ¨mes plus tard. Par exemple, des outils comme Prometheus/Grafana ou Azure Monitor peuvent capturer et visualiser ces mÃ©triques en temps rÃ©el, et des sondages montrent que 73 % des experts IT estiment que le monitoring temps-rÃ©el a amÃ©liorÃ© leur capacitÃ© Ã  rÃ©soudre les problÃ¨mes de performance. Journaliser les requÃªtes SQL et les performances des bases de donnÃ©es : En environnement .NET, pensez Ã  activer le logging des requÃªtes SQL (par exemple via Entity Framework Core en mode Information). Cela permet de voir chaque commande SQL exÃ©cutÃ©e et sa durÃ©e dâ€™exÃ©cution. Si une requÃªte prend plus de temps que prÃ©vu, vous avez identifiÃ© un coupable potentiel et pouvez enquÃªter sur sa cause (index manquant, requÃªte N+1, etc.). âš ï¸ Attention Ã  ne pas laisser ce logging activÃ© en production de faÃ§on permanente, car il peut ralentir lâ€™application et gÃ©nÃ©rer dâ€™Ã©normes fichiers de log. Utilisez-le ponctuellement pour collecter des donnÃ©es de performance, ou exploitez des outils APM (Application Performance Management) qui capturent ces informations plus finement (par exemple, Azure Application Insights intÃ¨gre les temps dâ€™exÃ©cution des requÃªtes SQL dans son analyse). Choisir la bonne architecture (et la bonne granularitÃ© de services) : Une architecture bien pensÃ©e est la base de la performance. Par exemple, une approche microservices permet une scalabilitÃ© indÃ©pendante de chaque composant : chaque service peut monter en charge sÃ©parÃ©ment selon les besoins, sans devoir dimensionner toute lâ€™infrastructure globalement. Cela offre aussi plus de rÃ©silience (si un service tombe, le reste du systÃ¨me continue de fonctionner). Veillez toutefois Ã  dÃ©finir des frontiÃ¨res de service claires (idÃ©alement alignÃ©es sur des contextes mÃ©tiers via DDD) et Ã  Ã©viter un morcellement excessif qui introduirait une complexitÃ© inutile. Chaque microservice doit Ãªtre faiblement couplÃ© et autonome, communiquant avec les autres via des API lÃ©gÃ¨res ou des messages. En effet, des services dÃ©couplÃ©s (Ã©changes HTTP REST, messages asynchrones via une file, etc.) peuvent Ã©voluer ou Ãªtre modifiÃ©s sans impacter les autres, ce qui amÃ©liore la flexibilitÃ© et la facilitÃ© de mise Ã  lâ€™Ã©chelle. Ã‰viter les dÃ©pendances synchrones entre composants : Les appels synchrones bloquants entre microservices crÃ©ent un couplage fort et peuvent provoquer des cascades de latence. Il est souvent recommandÃ© dâ€™adopter une communication asynchrone via des Ã©vÃ©nements ou des files de messages. Par exemple, plutÃ´t que dâ€™attendre la rÃ©ponse dâ€™un autre service en temps rÃ©el, Ã©mettez un Ã©vÃ©nement que lâ€™autre service traitera Ã  son rythme. Lâ€™Event-Driven Architecture rÃ©duit les dÃ©pendances directes et amÃ©liore la scalabilitÃ© globale du systÃ¨me. En pratique, cela signifie utiliser des solutions comme RabbitMQ, Azure Service Bus ou Kafka pour propager des Ã©vÃ©nements, avec des mÃ©canismes de rÃ©essais et de tolÃ©rance (circuit breakers, timeouts) pour gÃ©rer les dÃ©faillances. Concevoir dÃ¨s le dÃ©part les services pour quâ€™ils puissent Ã©chouer sans tout faire tomber (principe de design for failure) est clÃ© pour la robustesse. Minimiser le couplage et favoriser la cohÃ©sion : Dans la mÃªme veine, structurez vos composants pour quâ€™ils soient le plus autonomes possible (chaque microservice gÃ¨re sa propre base de donnÃ©es, Ã©vitant les dÃ©pendances entre bases de donnÃ©es). Un faible couplage se traduit aussi par lâ€™utilisation de contrats dâ€™API bien dÃ©finis et stables, idÃ©alement avec des modÃ¨les de communication idempotents et stateless qui facilitent la montÃ©e en charge horizontale. ConcrÃ¨tement, assurez-vous quâ€™aucun module nâ€™ait de connaissance interne sur un autre module en dehors des APIs publiÃ©es. Par exemple, un service de commandes ne devrait pas appeler directement la base de donnÃ©es du service Clients ; il utilisera lâ€™API du service Clients si besoin. Ce dÃ©coupage modulaire permet de modifier ou dÃ©ployer un service sans impacter le reste, et de faire Ã©voluer lâ€™architecture plus sereinement. Minimiser le couplage et favoriser la cohÃ©sion : Dans la mÃªme veine, structurez vos composants pour quâ€™ils soient le plus autonomes possible (chaque microservice gÃ¨re sa propre base de donnÃ©es, Ã©vitant les dÃ©pendances entre bases de donnÃ©es). Un faible couplage se traduit aussi par lâ€™utilisation de contrats dâ€™API bien dÃ©finis et stables, idÃ©alement avec des modÃ¨les de communication idempotents et stateless qui facilitent la montÃ©e en charge horizontale. ConcrÃ¨tement, assurez-vous quâ€™aucun module nâ€™ait de connaissance interne sur un autre module en dehors des APIs publiÃ©es. Par exemple, un service de commandes ne devrait pas appeler directement la base de donnÃ©es du service Clients ; il utilisera lâ€™API du service Clients si besoin. Ce dÃ©coupage modulaire permet de modifier ou dÃ©ployer un service sans impacter le reste, et de faire Ã©voluer lâ€™architecture plus sereinement.En rÃ©sumÃ©, intÃ©grer la tÃ©lÃ©mÃ©trie, penser architecture scalable (Ã©ventuellement microservices ou modules bien dÃ©couplÃ©s) et Ã©liminer les interactions bloquantes superflues dÃ¨s la conception pose les fondations dâ€™une application performante.Sâ€™amÃ©liorer en continu pendant la vie du systÃ¨meGarantir la performance nâ€™est pas un effort ponctuel, mais un processus continu tout au long du cycle de vie du logiciel.Voici quelques principes pour instaurer une culture dâ€™amÃ©lioration continue des performances : Mesurer systÃ©matiquement et surveiller en production : On ne peut amÃ©liorer que ce quâ€™on mesure. Mettez en place un monitoring continu de lâ€™application en production pour dÃ©tecter les problÃ¨mes avant les utilisateurs. Des solutions dâ€™APM comme Azure Application Insights (ou New Relic, Dynatrace, etc.) dÃ©tectent automatiquement les anomalies de performance sur vos applications web et peuvent alerter lâ€™Ã©quipe en cas de dÃ©gradation (par exemple, augmentation anormale du taux dâ€™erreurs ou des temps de rÃ©ponse). Configurez des alertes proactives sur vos mÃ©triques clÃ©s (latence, taux dâ€™Ã©chec, utilisation mÃ©moireâ€¦) afin dâ€™Ãªtre notifiÃ© dÃ¨s quâ€™un seuil critique est franchi. Cette surveillance proactive vous aide Ã  corriger les dÃ©rives avant quâ€™elles ne se transforment en panne ou en incident majeur. Nâ€™hÃ©sitez pas Ã  utiliser des tableaux de bord visibles de tous pour suivre lâ€™Ã©volution des performances et des ressources en temps rÃ©el. Identifier et lever rÃ©guliÃ¨rement les goulots dâ€™Ã©tranglement : Les bottlenecks peuvent survenir Ã  diffÃ©rents niveaux (base de donnÃ©es saturÃ©e, appels externes lents, thread CPU bloquÃ©, etc.). GrÃ¢ce aux mÃ©triques et journaux rÃ©coltÃ©s, analysez rÃ©guliÃ¨rement oÃ¹ se situent les points chauds. Par exemple, des temps de rÃ©ponse trÃ¨s Ã©levÃ©s sur une API donnÃ©e peuvent rÃ©vÃ©ler une requÃªte SQL non optimisÃ©e ou un appel Ã  un service tiers trop lent. Lâ€™objectif est dâ€™agir en amont : idÃ©alement, effectuez des tests de charge ou des profils de performance en continu (ou Ã  chaque release) pour identifier les problÃ¨mes de performance potentiels avant quâ€™ils nâ€™affectent les utilisateurs. Adapter vos tests automatisÃ©s pour inclure des tests de performance (mÃªme basiques) peut aider Ã  dÃ©tecter des rÃ©gressions prÃ©coces. En phase de dÃ©veloppement, nâ€™hÃ©sitez pas Ã  utiliser un profiler ou analyser les traces de vos API pour trouver les sections de code les plus lentes. Une fois les goulots repÃ©rÃ©s, traitez-les : par exemple, ajouter un index manquant sur la base de donnÃ©es, mettre en cache une donnÃ©e souvent lue, optimiser un algorithme inefficace, etc. Cette dÃ©marche proactive assure que lâ€™application garde un niveau de performance acceptable au fil des ajouts de fonctionnalitÃ©s. Ã‰viter lâ€™accumulation de dette technique : La dette technique non rÃ©sorbÃ©e finit par ralentir lâ€™application et compliquer son Ã©volution. Un code mal conÃ§u ou obsolÃ¨te peut entraÃ®ner des exÃ©cutions inefficaces et des bugs, impactant directement la performance (par exemple, des algorithmes inadaptÃ©s qui dÃ©gradent le temps de rÃ©ponse). Il est donc vital dâ€™allouer du temps rÃ©guliÃ¨rement pour refactorer les portions de code critiques, amÃ©liorer la lisibilitÃ© et rÃ©duire la complexitÃ©. Par exemple, si une partie du code est responsable de nombreuses requÃªtes redondantes ou de calculs rÃ©pÃ©tÃ©s, la refonte de ce module peut Ã©liminer ces inefficacitÃ©s. IntÃ©grez la rÃ©solution de dette technique dans votre processus Agile (incluez des tÃ¢ches de refactoring dans le carnet de produit, fixez-vous un budget de temps par sprint pour la dette). Veillez aussi Ã  prÃ©venir la dette : suivez les bonnes pratiques de codage, revoyez le code (code reviews) pour dÃ©tecter les antipatterns de performance, et Ã©crivez des tests de performance pour valider que les nouvelles modifications ne rÃ©gressent pas. Une dette technique maÃ®trisÃ©e se traduit par une application plus maintenable et performante sur le long terme. Mettre en place une culture de performance : Finalement, la performance doit devenir lâ€™affaire de toute lâ€™Ã©quipe. Inscrivez des objectifs de performance (SLO/SLI) clairs, par exemple, â€œ95 % des requÃªtes sous 200 msâ€, â€œgÃ©rer 1000 requÃªtes/sec sans dÃ©grader lâ€™expÃ©rienceâ€ et suivez ces indicateurs Ã  chaque version. Si possible, automatisez des tests de non-rÃ©gression de performance dans votre pipeline CI/CD (par exemple via un outil comme k6 ou JMeter en mode headless). Encouragez le partage des connaissances autour des optimisations rÃ©alisÃ©es et des incidents Ã©vitÃ©s. Une telle culture implique aussi de ne pas attendre la veille de la mise en production pour se soucier des performances : idÃ©alement, on teste et on optimise en continu. Comme le rÃ©sume bien un guide, â€œPerformance shouldnâ€™t be an afterthoughtâ€, ne considÃ©rez pas la performance comme un sujet â€œnon-fonctionnel annexeâ€, mais comme un critÃ¨re de qualitÃ© aussi important que les fonctionnalitÃ©s. En sensibilisant dÃ©veloppeurs, QA et Ops, on crÃ©e un cercle vertueux oÃ¹ chacun est attentif aux impacts performance de ses choix et oÃ¹ lâ€™on rÃ©agit vite en cas de problÃ¨me.En amÃ©liorant en continu, en mesurant et en payant rÃ©guliÃ¨rement la dette, vous maintiendrez votre systÃ¨me en forme et Ã©viterez les Â« effets de pourrissement Â» qui mÃ¨nent aux applications lentes et instables avec le temps.Automatiser pour assurer la rÃ©silience et la performanceAu-delÃ  des efforts humains, lâ€™automatisation est une alliÃ©e prÃ©cieuse pour garantir la performance et la stabilitÃ© du systÃ¨me face Ã  la montÃ©e en charge ou aux imprÃ©vus : Autoscaling (mise Ã  lâ€™Ã©chelle automatique) : Tirez parti des capacitÃ©s du cloud pour ajuster dynamiquement les ressources en fonction de la charge. Lâ€™autoscaling horizontal (ajout/retrait dâ€™instances) permet de maintenir les performances lorsque le trafic augmente, puis de rÃ©duire les ressources pour Ã©conomiser les coÃ»ts quand la charge diminue. Par exemple, sur Azure App Service ou Kubernetes, vous pouvez dÃ©finir des rÃ¨gles du type â€œsi lâ€™utilisation CPU dÃ©passe 70 % sur 5 minutes, ajouter une instanceâ€. De mÃªme, fixez une rÃ¨gle de scale-in pour rÃ©duire le nombre dâ€™instances quand la charge retombe, afin dâ€™Ã©viter de surprovisionner. ğŸ’¡ Important : ajustez et affinez ces rÃ¨gles selon les mÃ©triques pertinentes (CPU, mÃ©moire, longueur de file de messages, etc.) et surveillez le comportement (pour Ã©viter des effets de bascule trop frÃ©quents, dÃ©finissez des seuils avec hystÃ©rÃ©sis et un dÃ©lai minimal entre deux scale actions). Un autoscaling bien configurÃ© rÃ©duit le besoin dâ€™intervention manuelle et assure que votre application reste rÃ©active en tout temps, y compris lors de pics soudains de trafic. Nâ€™oubliez pas de prÃ©voir une capacitÃ© maximale suffisante et un nombre minimum dâ€™instances par dÃ©faut pour absorber le trafic de base mÃªme si les mÃ©triques ne sont pas disponibles (sÃ©curitÃ© en cas de panne du monitoring). Tests de performance automatisÃ©s : IntÃ©grez des tests de charge rÃ©guliers dans votre cycle de dÃ©veloppement ou vos pipelines de dÃ©ploiement. Des outils open-source comme Apache JMeter (trÃ¨s populaire et riche en fonctionnalitÃ©s) ou k6 (plus rÃ©cent, orientÃ© dÃ©veloppeur, avec des scripts en JavaScript) sont parfaits pour Ã§a. JMeter, par exemple, est conÃ§u spÃ©cifiquement pour gÃ©nÃ©rer du trafic et mesurer les temps de rÃ©ponse de vos applications web, API, bases de donnÃ©es, etc.. Il permet de simuler un grand nombre dâ€™utilisateurs et divers protocoles (HTTP, JDBC, etc.) pour voir comment votre systÃ¨me se comporte sous stress. De son cÃ´tÃ©, k6 sâ€™est imposÃ© comme un outil moderne et puissant : â€œk6 est un outil de test de charge open-source qui permet de crÃ©er des tests en JavaScript, un langage familier pour beaucoupâ€. Son moteur en Go lui confÃ¨re de hautes performances pour simuler des milliers dâ€™utilisateurs avec une empreinte lÃ©gÃ¨re. Vous pouvez lâ€™exÃ©cuter en local, en distribuÃ© ou via son service cloud, et lâ€™intÃ©grer Ã  vos CI/CD pour des tests en continu. Quel que soit lâ€™outil, lâ€™idÃ©e est de soumettre rÃ©guliÃ¨rement votre application Ã  des scÃ©narios de charge (pic dâ€™utilisateurs, tests dâ€™endurance sur plusieurs heures, tests de spike, etc.) afin de vÃ©rifier sa tenue en conditions extrÃªmes. Ces tests rÃ©vÃ©leront peut-Ãªtre des points faibles (saturation CPU, fuite mÃ©moire, seuil Ã  partir duquel les temps explosent) que vous pourrez corriger avant quâ€™un trafic rÃ©el ne provoque un incident. Surveillance proactive et auto-remÃ©diation : En plus du monitoring passif, pensez Ã  mettre en place des mÃ©canismes de supervision proactive. Par exemple, des sondes de synthetic monitoring peuvent effectuer rÃ©guliÃ¨rement des appels simulÃ©s Ã  vos API ou pages principales et vÃ©rifier quâ€™elles restent performantes, ce qui permet de dÃ©tecter une dÃ©gradation avant mÃªme un utilisateur rÃ©el. Azure Application Insights propose des â€œAvailability Testsâ€ de ce genre. Par ailleurs, configurez votre systÃ¨me pour quâ€™il puisse rÃ©agir automatiquement Ã  certains Ã©vÃ©nements : par exemple, un redÃ©marrage automatique dâ€™une instance en cas de fuite mÃ©moire dÃ©tectÃ©e, ou le dÃ©clenchement dâ€™une scale-up temporaire si une latence anormale est mesurÃ©e sur un composant critique. Lâ€™utilisation combinÃ©e de mÃ©triques, dâ€™alertes et de scripts dâ€™auto-remÃ©diation augmente la rÃ©silience globale. Certaines plateformes cloud offrent des actions automatiques basÃ©es sur des alertes (webhook dÃ©clenchÃ© sur alerte, fonctions Azure Functions ou AWS Lambda lancÃ©es pour gÃ©rer lâ€™incident, etc.). Enfin, envisagez des approches plus avancÃ©es comme le chaos engineering en environnement de staging, pour sâ€™assurer que votre systÃ¨me rÃ©agit bien aux pannes (par exemple, couper un service au hasard et vÃ©rifier que le systÃ¨me reste stable via des mÃ©canismes de circuit breaker).En automatisant la montÃ©e en charge et la surveillance, vous obtenez un systÃ¨me auto-adaptatif : capable de croÃ®tre pour servir la demande, de prÃ©venir les problÃ¨mes avant quâ€™ils nâ€™affectent les clients, et de maintenir une performance constante sans intervention humaine continue. Cela complÃ¨te les efforts manuels dâ€™optimisation en apportant une filet de sÃ©curitÃ© opÃ©rationnel.Ã‰tapes clÃ©s pour une application performanteSynthÃ©tisons ces bonnes pratiques sous forme dâ€™un guide Ã©tape-par-Ã©tape couvrant le cycle de vie du projet, en prenant lâ€™exemple dâ€™une application distribuÃ©e en microservices .NET :1. Planification et conception Profilage des besoins et objectifs : DÃ¨s le lancement du projet, dÃ©finissez les exigences de performance et de scalabilitÃ©. Quel volume dâ€™utilisateurs ou de requÃªtes visez-vous (charge prÃ©vue Ã  court et moyen terme) ? Quels sont les SLA/SLO attendus (temps de rÃ©ponse max, throughput minimal) ? Cette analyse initiale aide Ã  dimensionner lâ€™architecture. Profitez-en pour estimer les coÃ»ts associÃ©s Ã  certaines charges (exemple : coÃ»t de lâ€™infrastructure pour 1000 utilisateurs simultanÃ©s) afin dâ€™orienter les choix techniques en fonction du budget. Architecture adaptÃ©e aux performances : Concevez lâ€™architecture en fonction de ces exigences. Par exemple, pour un trÃ¨s fort trafic en lecture, peut-Ãªtre opter pour une base de donnÃ©es NoSQL distribuÃ©e ou mettre en place une cache distribuÃ©e (Redis) devant la base de donnÃ©es SQL. Pour un besoin de haute disponibilitÃ©, prÃ©voyez le dÃ©ploiement sur plusieurs instances et zones gÃ©ographiques. Si votre domaine sâ€™y prÃªte, choisissez une architecture microservices pour isoler les contextes et permettre une scalabilitÃ© horizontale service par service. Veillez aussi Ã  la conception de la base de donnÃ©es (normalisation vs dÃ©normalisation, sharding possible, choix entre SQL/NoSQL selon les besoins). Identifiez dÃ¨s la conception les bottlenecks potentiels : par exemple, un service central par lequel passent toutes les requÃªtes, assurez-vous quâ€™il puisse monter en charge (le cas Ã©chÃ©ant, introduisez de la mise en cache ou un mÃ©canisme de rÃ©partition de charge). Si une fonctionnalitÃ© risque dâ€™Ãªtre trÃ¨s consommatrice (par exemple, la gÃ©nÃ©ration de rapports lourds), pensez Ã  lâ€™isoler dans un service ou un processus asynchrone. En rÃ©sumÃ©, anticipez les points de contention possibles et cherchez Ã  les mitiger dans le design (via du parallÃ©lisme, une distribution de la charge, etc.). Bonnes pratiques de conception : Appliquez les principes de base dâ€™une architecture performante : faible couplage, haute cohÃ©sion, stateless autant que possible, idempotence des traitements, etc. Par exemple, un service stateless (sans Ã©tat en mÃ©moire entre les requÃªtes) peut Ãªtre clonÃ© Ã  lâ€™infini derriÃ¨re un load balancer, ce qui est idÃ©al pour lâ€™autoscaling. Adoptez aussi dÃ¨s le dÃ©part les patterns qui amÃ©liorent la performance et la rÃ©silience : circuit breaker et retry pour les appels externes (afin dâ€™Ã©viter dâ€™attendre indÃ©finiment un service en panne), bulkheads (pour compartimenter les ressources et Ã©viter lâ€™effet domino), mise en file des tÃ¢ches non-urgentes, utilisation dâ€™un CDN pour les contenus statiques, etc. Ne nÃ©gligez pas la phase de revue dâ€™architecture, faites Ã©ventuellement des â€œthreat modelingâ€ de performance, câ€™est-Ã -dire demander â€œque se passe-t-il si X utilisateur font telle action en mÃªme temps ?â€ et voir si lâ€™architecture tient la route ou si un composant deviendrait le goulot.2. DÃ©veloppement et optimisation Coder avec lâ€™efficacitÃ© en tÃªte : Au niveau du code, suivez les bonnes pratiques de performance .NET. Ã‰vitez les allocations mÃ©moire inutiles, en particulier dans les boucles ou les mÃ©thodes appelÃ©es frÃ©quemment. Par exemple, privilÃ©giez lâ€™utilisation de types comme Span&lt;T&gt; ou Memory&lt;T&gt; pour manipuler des segments de donnÃ©es sans copie. Ces types permettent de rÃ©duire drastiquement les allocations et le garbage collection, ce qui amÃ©liore les temps dâ€™exÃ©cution. Pour illustrer : au lieu de faire un substring qui alloue une nouvelle string, on peut utiliser un ReadOnlySpan&lt;char&gt; pointant vers la portion de la chaÃ®ne dâ€™origine, puis parser directement ce span. Le gain est notable : plus aucune allocation, et un temps dâ€™exÃ©cution rÃ©duit (~30% plus rapide dans cet exemple simple). Voici un petit comparatif en C# : string s = \"Le rÃ©sultat est 1532.\";// Approche classique â€“ alloue une nouvelle string pour \"1532\"string nombreStr = s.Substring(15, 4);int value1 = int.Parse(nombreStr); // 'value1' vaut 1532// Approche optimisÃ©e avec Span&lt;T&gt; â€“ aucune nouvelle allocationReadOnlySpan&lt;char&gt; span = s.AsSpan(15, 4);int value2 = int.Parse(span); // 'value2' vaut aussi 1532, sans copie Dans ce cas, Substring crÃ©ait un nouvel objet string alors que lâ€™utilisation de AsSpan Ã©vite cette allocation. Ã€ grande Ã©chelle (par exemple, traitement de nombreuses lignes de texte), ces optimisations rÃ©duisent la pression mÃ©moire et accÃ©lÃ¨rent le programme. De mÃªme, soyez attentifs Ã  vos allocations dâ€™objets en boucle : utiliser des structures (struct) quand câ€™est pertinent, rÃ©utiliser des objets via des pools (exemple : ArrayPool&lt;T&gt;), ou encore utiliser des algorithmes in-place peuvent aider. Prioriser lâ€™asynchronisme et Ã©viter le code bloquant : .NET offre un modÃ¨le asynchrone puissant avec async/await et le Task-based programming. Exploitez cela pour toute opÃ©ration dâ€™entrÃ©e-sortie (accÃ¨s BD, appels HTTP, lecture de fichierâ€¦) de sorte Ã  ne pas bloquer les threads inutillement. Un thread bloquÃ© en attente dâ€™I/O est un thread qui ne sert Ã  rien pendant ce temps, limitant la scalabilitÃ© (surtout sur un serveur web oÃ¹ le nombre de threads est limitÃ©). Donc, â€œavoid blocking on async code with .Result or .Wait(), instead use fully async callsâ€. En pratique, Ã©vitez des choses comme : // Mauvaise pratique â€“ bloque le thread en attendant le rÃ©sultat var data = SomeLongOperationAsync().Result; // STOP, potentiellement bloquant PrÃ©fÃ©rez systÃ©matiquement la propagation de lâ€™asynchronisme : // Bonne pratique â€“ lâ€™appel est asynchrone de bout en bout var data = await SomeLongOperationAsync(); // Non-bloquant, libÃ¨re le thread en attente Ne mÃ©langez pas code synchrone et asynchrone sans raison, cela peut mener Ã  des deadlocks subtils (en particulier dans les applications ASP.NET ou GUI qui ont un contexte de synchronisation). Ã‰vitez Ã©galement les verrous globaux ou les sections critiques longue durÃ©e qui empÃªcheront lâ€™exploitation du parallÃ©lisme. Si vous devez limiter un accÃ¨s concurrent (par exemple, pour une ressource partagÃ©e), utilisez des mÃ©canismes non bloquants quand possible (exemple : SemaphoreSlim async au lieu dâ€™un lock classique, collections thread-safe, etc.). Lâ€™asynchronisme bien utilisÃ© permet au runtime dâ€™optimiser lâ€™utilisation des threads, et donc de traiter plus de requÃªtes simultanÃ©es avec la mÃªme infrastructure. Optimiser les accÃ¨s aux donnÃ©es : Dans une application de gestion, lâ€™accÃ¨s Ã  la base de donnÃ©es est souvent le facteur limitant. Il faut donc porter une attention particuliÃ¨re aux requÃªtes SQL gÃ©nÃ©rÃ©es ou Ã©crites. Ã‰vitez le N+1 query problem (quand une boucle engendre une requÃªte par itÃ©ration) en utilisant les jointures ou Include nÃ©cessaires pour tout rÃ©cupÃ©rer en une fois. Indexez correctement vos tables selon les requÃªtes rÃ©elles en production (analyses de plans dâ€™exÃ©cution Ã  lâ€™appui). Si vous utilisez un ORM comme Entity Framework, traquez les requÃªtes non souhaitÃ©es et Ã©valuez le coÃ»t du suivi de changements (le mode tracking par dÃ©faut a un coÃ»t mÃ©moire, envisagez le mode AsNoTracking pour les requÃªtes purement lecture). Pour les lectures intensives, envisagez une cache applicative afin de ne pas solliciter la BD inutilement. Enfin, surveillez les appels rÃ©seau ou externes : regroupez-les si possible (appel dâ€™API en lot plutÃ´t quâ€™un par Ã©lÃ©ment) et utilisez le caching des rÃ©ponses externes quand câ€™est pertinent. Mesurer et profiler le code critique : Introduisez dÃ¨s le dÃ©veloppement des tests de performance sur les mÃ©thodes sensibles. Par exemple, si vous avez un algorithme de calcul intensif, crÃ©ez un micro-benchmark pour comparer diffÃ©rentes implÃ©mentations. La bibliothÃ¨que BenchmarkDotNet est idÃ©ale pour cela : elle permet de transformer facilement des mÃ©thodes en benchmarks et de mesurer prÃ©cisÃ©ment leur temps dâ€™exÃ©cution, allocations mÃ©moire, etc. Cet outil gÃ¨re le warming, les itÃ©rations multiples et fournit un rapport complet. Selon CODE Magazine, â€œbenchmarking code is critical for knowing the performance metrics of your methodsâ€¦ Ã§a aide Ã  identifier les bottlenecks et Ã  savoir quelles parties du code optimiserâ€. Nâ€™hÃ©sitez pas Ã  Ã©crire un petit projet console de benchmarks pour vos fonctions critiques (par exemple, comparer deux mÃ©thodes de parsing, ou deux approches de tri, etc.). De plus, utilisez les profilers lors du dÃ©bogage (Visual Studio Diagnostic Tools, dotTrace, PerfViewâ€¦) pour voir oÃ¹ le temps est passÃ© et oÃ¹ la mÃ©moire est allouÃ©e lors dâ€™un scÃ©nario complet. Ces informations guideront vos optimisations de maniÃ¨re objective. Rappelez-vous : il est facile de se tromper sur lâ€™origine dâ€™un ralentissement, seules les mesures peuvent vous le confirmer. Tests unitaires et de charge en local : Durant le dÃ©veloppement, outre les tests unitaires fonctionnels, pensez Ã  effectuer de petits tests de charge localement sur vos endpoints (avec un outil comme K6) pour avoir un aperÃ§u de comment se comporte votre API avec, par exemple, 100 requÃªtes concurrentes. Cela peut rÃ©vÃ©ler tÃ´t des soucis (contenention, exceptions, etc.). Assurez-vous Ã©galement dâ€™avoir des environnements de staging sur lesquels vous pouvez simuler des charges plus rÃ©alistes avant la mise en production.3. DÃ©ploiement et suivi en production Activer le monitoring en production : Une fois lâ€™application dÃ©ployÃ©e, branchez-la sur des outils de monitoring. Sur Azure, activez Application Insights pour votre application .NET, câ€™est un APM qui va collecter les logs, mÃ©triques et traces automatiquement (requÃªtes HTTP, dÃ©pendances externes, requÃªtes SQL, exceptionsâ€¦). Application Insights peut mÃªme â€œanalyser automatiquement les performances de votre application et vous alerter en cas de problÃ¨mes potentielsâ€. Il dÃ©tecte par exemple une hausse anormale du taux dâ€™erreurs ou une dÃ©gradation de la durÃ©e de certaines requÃªtes, et gÃ©nÃ¨re des alertes (Smart Detection). Configurez Ã©galement Azure Monitor pour vos ressources (par exemple, surveiller la mÃ©trique de DTU ou dâ€™utilisation CPU de votre base de donnÃ©es Azure SQL, la saturation de vos instances App Service, etc.). Pensez aux logs distribuÃ©s, dans une architecture microservices, centralisez les logs de chaque service dans un outil (Elastic Stack/ELK, Azure Log Analytics, Seqâ€¦) et corrÃ©lez-les avec du tracing distribuÃ© (propagation dâ€™un ID de corrÃ©lation pour suivre une requÃªte de bout en bout Ã  travers les services, via des outils comme Jaeger ou Zipkin). Ce niveau dâ€™observabilitÃ© vous permettra de diagnostiquer rapidement en production les Ã©ventuels problÃ¨mes de performance (exemple : identifier quâ€™un ralentissement global vient en fait du service X spÃ©cifique, ou mÃªme dâ€™une Ã©tape prÃ©cise dans un workflow). Tests de charge rÃ©guliers en environnement de prÃ©-production : Ne faites pas lâ€™impasse sur des tests de charge avant chaque version majeure. IdÃ©alement, reproduisez un environnement aussi proche que possible de la production (en termes de configuration, de volume de donnÃ©es, etc.) et exÃ©cutez-y des scÃ©narios de charge avec vos outils (k6, JMeterâ€¦). Ceci pour valider que la nouvelle version supporte toujours la charge prÃ©vue et quâ€™aucune rÃ©gression de performance nâ€™a Ã©tÃ© introduite. Vous pouvez mÃªme automatiser un test de performance rapide aprÃ¨s le dÃ©ploiement (par exemple, un test qui envoie pendant 5 minutes du trafic Ã  X req/s et vÃ©rifie que les temps de rÃ©ponse restent conformes). ğŸ’¡ Astuce : conservez des baseline (mÃ©triques de rÃ©fÃ©rence) des tests de charge des versions prÃ©cÃ©dentes, de sorte Ã  pouvoir comparer lâ€™Ã©volution. Si vous constatez une dÃ©gradation, mieux vaut la comprendre avant de mettre en prod que subir un incident. Les tests de charge rÃ©guliers garantissent aussi que votre infrastructure dâ€™autoscaling est bien calibrÃ©e ! Par exemple, vÃ©rifier quâ€™Ã  80% de CPU vos instances se dupliquent correctement et absorbent le pic. Adapter les rÃ¨gles dâ€™autoscaling aux habitudes rÃ©elles : AprÃ¨s quelques temps en production, utilisez les donnÃ©es collectÃ©es pour affiner vos paramÃ¨tres. Peut-Ãªtre que vous aviez prÃ©vu un autoscaling sur CPU Ã  70%, mais vous rÃ©alisez que la mÃ©moire est le facteur limitant sur vos services : il faudrait alors ajouter une rÃ¨gle sur la mÃ©moire (exemple : scale-out si &gt;85% mÃ©moire utilisÃ©e) pour ne pas saturer les instances. Inversement, si vous voyez que lâ€™application scale trop frÃ©quemment (phÃ©nomÃ¨ne de flapping), envisagez dâ€™augmenter un peu les seuils ou dâ€™ajouter du dÃ©lai pour Ã©viter les oscillations inutiles. Revoyez aussi la capacitÃ© maximale : si rÃ©guliÃ¨rement vous touchez le plafond dâ€™instances en heure de pointe, rÃ©flÃ©chissez Ã  lâ€™augmenter ou Ã  opter pour des instances plus puissantes. Lâ€™objectif est dâ€™ajuster en continu vos ressources en fonction des tendances dâ€™utilisation, afin dâ€™assurer la performance tout en optimisant les coÃ»ts. Supervision et rÃ©ponses en temps rÃ©el : En exploitation, mettez en place des routines de revue des indicateurs (par exemple, un petit stand-up hebdomadaire dÃ©diÃ© performance/rÃ©silience oÃ¹ lâ€™on passe en revue les alertes de la semaine, les mÃ©triques hors normes, etc.). Investiguez toute alerte ou anomalie de performance dÃ¨s que possible, mÃªme si aucun utilisateur ne sâ€™en est plaint (exemple : si un pic de latence a eu lieu la nuit, chercher la cause : opÃ©ration batch, sauvegarde, garbage collection majeur, etc.). Avoir une approche SRE (Site Reliability Engineering) peut aider : dÃ©finir un budget dâ€™erreurs (erreur budget) et se fixer des objectifs de disponibilitÃ©/performance. En cas dâ€™incident (panne ou forte dÃ©gradation), procÃ©dez Ã  une analyse post-mortem pour en tirer des leÃ§ons et Ã©viter la rÃ©pÃ©tition. Par exemple, si un service a crashÃ© faute de mÃ©moire, vous pourriez implÃ©menter un recycle automatique de ce service avant quâ€™il nâ€™atteigne la limite, ou amÃ©liorer son code pour consommer moins. Enfin, continuez de tester en production de maniÃ¨re contrÃ´lÃ©e : par exemple les chaos tests (dÃ©brancher un service pour vÃ©rifier que le failover fonctionne) ou des canary releases pour mesurer lâ€™impact perf dâ€™une nouvelle version sur un sous-ensemble du trafic avant dÃ©ploiement global.En suivant ces Ã©tapes de maniÃ¨re disciplinÃ©e, vous crÃ©ez un cycle vertueux : planification soignÃ©e, dÃ©veloppement optimisÃ©, surveillance active, et boucle de rÃ©troaction pour continuellement amÃ©liorer la performance. Chaque phase alimente la suivante â€“ les enseignements de la production guident la prochaine planification, etc. Ainsi, votre application pourra Ã©voluer en fonctionnalitÃ©s tout en restant rapide, scalable et fiable.ConclusionLa performance applicative est un effort transversal qui commence Ã  lâ€™architecture initiale et se poursuit tout au long du cycle de vie du logiciel. En concevant dÃ¨s le dÃ©part pour la scalabilitÃ©, vous Ã©vitez de sÃ©rieux Ã©cueils plus tard. En instaurant une amÃ©lioration continue (mesure, optimisation, rÃ©duction de la dette technique), vous prÃ©venez la dÃ©gradation progressive quâ€™on observe souvent dans les systÃ¨mes qui vieillissent. Et en automatisant la rÃ©silience via lâ€™autoscaling, le monitoring proactif et les tests rÃ©guliers, vous vous assurez que lâ€™application peut encaisser la charge et rester stable face aux imprÃ©vus.Une application .NET bien pensÃ©e, utilisant par exemple une architecture microservices dÃ©couplÃ©e, des patterns asynchrones, des optimisations comme Span&lt;T&gt;, et outillÃ©e de mÃ©triques et dâ€™APM, peut atteindre des niveaux de performance Ã©levÃ©s de maniÃ¨re pÃ©renne. La clÃ© est de considÃ©rer la performance comme un critÃ¨re de qualitÃ© Ã  part entiÃ¨re, Ã  chaque dÃ©cision technique. Ainsi, vous livrerez non seulement des fonctionnalitÃ©s, mais aussi une expÃ©rience fluide et rÃ©active aux utilisateurs, et vous pourrez dormir sur vos deux oreilles lors des pics de charge ğŸ˜„.En appliquant ces conseils et en restant Ã  lâ€™Ã©coute de votre application (les donnÃ©es de production sont vos meilleures amies), vous dÃ©velopperez un vÃ©ritable sens de la performance. Rappelez-vous : â€œBuild it, but also make sure it runs fast and scales!â€. Bonne optimisation Ã  tous !" }, { "title": ".NET 10 - NouveautÃ©s, Performances et Support ProlongÃ©", "url": "/posts/dotnet10/", "categories": "", "tags": "dotnet", "date": "2025-10-06 19:00:00 -0400", "snippet": "En novembre 2025, Microsoft lancera .NET 10, probablement lors de .NET Conf (du 11 au 13 novembre). Cette version sâ€™annonce riche en amÃ©liorations de performance et nouvelles fonctionnalitÃ©s, tout ...", "content": "En novembre 2025, Microsoft lancera .NET 10, probablement lors de .NET Conf (du 11 au 13 novembre). Cette version sâ€™annonce riche en amÃ©liorations de performance et nouvelles fonctionnalitÃ©s, tout en apportant des changements de support importants. Ã€ lâ€™heure oÃ¹ jâ€™Ã©cris cet article en fin septembre 2025, .NET 10 est en RC1 (depuis le 9 septembre), et la version finale sera une LTS (Long Term Support) prise en charge jusquâ€™en novembre 2028. Notons dâ€™ailleurs un ajustement de stratÃ©gie : les versions STS (Standard Term Support, impaires comme .NET 9) bÃ©nÃ©ficient dÃ©sormais de 24 mois de support au lieu de 18 auparavant. En clair, .NET 9 (sorti en 2024) aura sa fin de support repoussÃ©e Ã  novembre 2026, soit le mÃªme jour que .NET 8 LTS..NET 10 Ã©tant une version majeure, il est illusoire de vouloir lister tous les changements dans un seul article. Nous allons donc nous concentrer sur les faits marquants. PrÃ©parez-vous Ã  dÃ©couvrir des gains de performance notables, des nouveautÃ©s du cÃ´tÃ© du framework et du langage C#14, ainsi que quelques conseils sur la migration.Des performances encore amÃ©liorÃ©esChaque nouvelle version de .NET apporte son lot dâ€™optimisations, et .NET 10 ne fait pas exception. Lâ€™ingÃ©nieur Stephen Toub a publiÃ© son traditionnel pavÃ© de plus de 200 pages dÃ©taillant â€œdes centaines de petites amÃ©liorations qui, mises bout Ã  bout, rendent .NET 10 plus rapideâ€. Inutile de chercher une rÃ©volution unique : les gains viennent dâ€™une multitude de micro-optimisations dans le JIT, le GC, les collections, etc., qui grattent des nanosecondes par-ci, quelques octets par-lÃ , sur des opÃ©rations exÃ©cutÃ©es des milliards de fois. Par exemple, le runtime est dÃ©sormais plus intelligent pour allouer certains objets sur le stack plutÃ´t que dans le heap (grÃ¢ce Ã  lâ€™escape analysis), Ã©vitant des allocations mÃ©moire. Les boucles foreach sur des collections sont un poil plus efficaces, tout comme de nombreuses mÃ©thodes LINQ courantes qui ont Ã©tÃ© retravaillÃ©es.En somme, .NET 10 est encore plus performant. Vos applications web, services et outils gagneront en rapiditÃ© sans changer une ligne de code, grÃ¢ce aux amÃ©liorations du JIT, des algorithmes de tri, du threading et bien dâ€™autres. De quoi faire plaisir aux dÃ©veloppeurs en quÃªte de performanceâ€¦ et aux utilisateurs finaux qui apprÃ©cieront la rÃ©activitÃ©.Nouvelles fonctionnalitÃ©s cÃ´tÃ© framework et runtimeJSON Patch intÃ©grÃ© Ã  System.Text.JsonEnfin une bonne nouvelle pour les API REST : .NET 10 introduit une implÃ©mentation native de JSON Patch (RFC 6902) basÃ©e sur System.Text.Json. Jusquâ€™Ã  prÃ©sent, appliquer un patch JSON dans ASP.NET Core nÃ©cessitait la bibliothÃ¨que externe Newtonsoft.Json. DÃ©sormais, plus besoin de cet extra, on peut manipuler des JsonPatchDocument directement via System.Text.Json, avec de bien meilleures performances et une empreinte mÃ©moire rÃ©duite. ConcrÃ¨tement, un package Microsoft.AspNetCore.JsonPatch.SystemTextJson fait son apparition pour activer cette fonctionnalitÃ© dans vos contrÃ´leurs Web API. ğŸ’¡ Note : cette nouvelle implÃ©mentation ne gÃ¨re pas les types dynamic et nâ€™est pas totalement compatible Ã  100% avec lâ€™ancienne (quelques cas limites diffÃ¨rent). Mais pour lâ€™Ã©crasante majoritÃ© des usages, on peut enfin se passer de Newtonsoft et appliquer proprement des opÃ©rations de patch (add, remove, replace, etc.) sur nos modÃ¨les JSON.ObservabilitÃ© renforcÃ©e : mÃ©triques ASP.NET CoreLes Ã©quipes .NET ont beaucoup travaillÃ© sur lâ€™observabilitÃ©. .NET 10 enrichit ASP.NET Core de nouvelles mÃ©triques intÃ©grÃ©es pour mieux monitorer vos applications. Par exemple, le framework expose dÃ©sormais des compteurs (counters, histogramsâ€¦) pour suivre les Ã©vÃ©nements dâ€™authentification et dâ€™autorisation : nombre de nouvelles crÃ©ations dâ€™utilisateurs, changements de mot de passe, tentatives de connexion, etc. Des mÃ©triques spÃ©cifiques Ã  ASP.NET Core Identity ont Ã©tÃ© ajoutÃ©es pour mesurer la durÃ©e de certaines opÃ©rations (exemple : aspnetcore.identity.user.create.duration pour la crÃ©ation dâ€™un utilisateur).De mÃªme, le gestionnaire de mÃ©moire interne dâ€™ASP.NET (pools de mÃ©moire) expose maintenant des compteurs de Â«memory evictionÂ», utiles pour voir si votre application Ã©vacue souvent des donnÃ©es du cache en mÃ©moire. MÃªme Blazor a droit Ã  des mÃ©triques de cycle de vie de composant et du traÃ§age plus poussÃ©, ce qui facilitera le diagnostic de vos applications WebAssembly cÃ´tÃ© client.Toutes ces mÃ©triques sont accessibles via le systÃ¨me de mÃ©triques .NET (basÃ© sur EventCounters/OpenTelemetry Metrics). En clair, vous pouvez brancher vos tableaux de bord de monitoring (Prometheus, Grafana, Application Insightsâ€¦) et obtenir une observabilitÃ© fine sans Ã©crire de code maison. Un vrai plus pour dÃ©tecter les goulots dâ€™Ã©tranglement et surveiller la santÃ© de vos applications en production.SÃ©curitÃ© et identitÃ© : support des PasskeysLa rÃ©volution passwordless arrive dans .NET 10 ! ASP.NET Core Identity prend dÃ©sormais en charge les Passkeys (clÃ©s dâ€™authentification FIDO2/WebAuthn). ConcrÃ¨tement, le template dâ€™application Blazor (avec identitÃ© individuelle) a Ã©voluÃ© : lâ€™espace Â« Passkeys Â» permet aux utilisateurs dâ€™enregistrer une clÃ© biomÃ©trique ou un appareil de sÃ©curitÃ© pour se connecter.Les passkeys fournissent une mÃ©thode dâ€™authentification sans mot de passe, utilisant des dispositifs biomÃ©triques ou des clÃ©s sÃ©curisÃ©es liÃ©es Ã  lâ€™appareil de lâ€™utilisateur. Avec .NET 10, on peut donc offrir aux utilisateurs la possibilitÃ© de se connecter avec Windows Hello, Touch ID, clÃ©s USB de sÃ©curitÃ©, etc., plutÃ´t quâ€™avec le traditionnel couple login/mot de passe. Câ€™est Ã  la fois plus sÃ©curisÃ© (immunisÃ© contre le phishing) et plus pratique une fois configurÃ©.Petite dose de rÃ©alisme toutefois : dans lâ€™Ã©tat actuel (Preview 6), le gabarit impose encore de crÃ©er un mot de passe lors de lâ€™inscription initiale, le passkey venant en option ensuite. Cela a fait tiquer certains (aprÃ¨s tout, le but des passkeys est dâ€™Ã©liminer totalement les mots de passeâ€¦). Gageons que de futures itÃ©rations permettront une inscription 100% sans mot de passe. Quoi quâ€™il en soit, intÃ©grer WebAuthn nativement dans le framework est un grand pas en avant. Si la sÃ©curitÃ© de vos applications web vous tient Ã  cÅ“ur, vous pourrez tirer parti de cette fonctionnalitÃ© pour proposer le login le plus sÃ»r Ã  vos utilisateurs.Mode script en C#Avec .NET 10, Microsoft continue de faciliter la vie des dÃ©veloppeurs en matiÃ¨re de dÃ©ploiement et de prototypage. Le Mode script C# (dotnet run &lt;fichier&gt;.cs) est lâ€™une des nouveautÃ©s les plus cool pour les dÃ©veloppeurs : .NET 10 permet dâ€™exÃ©cuter directement un fichier .cs sans projet ni solution. En tapant simplement dotnet run monScript.cs, le SDK compile et lance le code C# immÃ©diatement. IdÃ©al pour tester un bout de code, Ã©crire un petit utilitaire jetable, ou apprendre C# sans passer par la case .csproj. Plus besoin de crÃ©er un projet complet pour un programme de 5 lignes ! Cette expÃ©rience Â« script Â» rapproche C# de langages comme Python ou JavaScript pour les scenarios rapides. (Ne jetez pas encore Visual Studio, pour des applications sÃ©rieuses, le projet reste pertinent, mais pour un script dâ€™administrateur, câ€™est la vie.).NET 10 amÃ©liore la productivitÃ© sur tout le cycle de vie : du prototype au dÃ©ploiement final.Ã‰volutions du langage C# 14Qui dit nouvelle version .NET dit souvent nouvelle version du langage C#. .NET 10 sâ€™accompagne de C# 14, qui apporte quelques raffinements trÃ¨s apprÃ©ciables pour les dÃ©veloppeurs, sans bouleverser la syntaxe existante.Extension Members : la fonctionnalitÃ© â€œextension everythingâ€C# 14 rÃ©alise un fantasme de longue date : permettre des members dâ€™extension, pas juste des mÃ©thodes. Depuis C# 3.0 (en 2007), on peut Ã©crire des mÃ©thodes dâ€™extension (extension methods) pour ajouter des mÃ©thodes â€œvirtuellesâ€ Ã  des types existants. Mais on ne pouvait pas crÃ©er de propriÃ©tÃ©s dâ€™extension, ni Ã©tendre les mÃ©thodes statiques. Plusieurs tentatives avaient Ã©chouÃ© dans le passÃ©, surnommÃ©es â€œextension everythingâ€. Cette fois câ€™est la bonne : les extension members dÃ©barquent en C# 14.ConcrÃ¨tement, une nouvelle syntaxe permet de dÃ©clarer dans une classe statique un bloc extension ciblant un type (par exemple IEnumerable&lt;T&gt;) et dâ€™y dÃ©finir : des propriÃ©tÃ©s dâ€™extension (comme un IsEmpty accessible sur nâ€™importe quel IEnumerable&lt;T&gt;), des mÃ©thodes dâ€™extension statiques (sâ€™apparentant Ã  des mÃ©thodes de classe du type Ã©tendu), mÃªme des opÃ©rateurs dâ€™extension (surcharge de operator+ par exemple) qui agissent comme si le type les proposait nativement.Cette syntaxe est un peu verbeuse au premier abord, mais elle organise mieux le code.Par exemple, on peut maintenant Ã©crire :public static class Enumerable { extension&lt;T&gt;(IEnumerable&lt;T&gt; source) { public bool IsEmpty =&gt; !source.Any(); // propriÃ©tÃ© dâ€™extension public IEnumerable&lt;T&gt; Where(Func&lt;T,bool&gt; p) { â€¦ } // mÃ©thode dâ€™extension (instance) } extension&lt;T&gt;(IEnumerable&lt;T&gt;) { public static IEnumerable&lt;T&gt; Combine(IEnumerable&lt;T&gt; first, IEnumerable&lt;T&gt; second) { ... } // mÃ©thode statique dâ€™extension public static IEnumerable&lt;T&gt; operator+ (IEnumerable&lt;T&gt; left, IEnumerable&lt;T&gt; right) =&gt; left.Concat(right); // opÃ©rateur dâ€™extension }}DÃ¨s lors, je peux appeler maListe.IsEmpty comme sâ€™il sâ€™agissait dâ€™une propriÃ©tÃ© native du type List, ou encore IEnumerable&lt;int&gt;.Combine(seq1, seq2) comme une mÃ©thode statique de IEnumerable Ã‡a rend le code plus lisible et discoverable (surtout dans IntelliSense) quâ€™une mÃ©thode dâ€™extension classique perdue dans une classe utilitaire. Bref, une fonctionnalitÃ© favorite de beaucoup (dont moi-mÃªme ğŸ˜Š), qui va permettre de mieux organiser nos extensions.(Et pour les puristes : pas de panique, vos mÃ©thodes dâ€™extension actuelles continuent de fonctionner comme avant, cette nouvelle syntaxe est optionnelle et rÃ©trocompatible.)Mot-clÃ© field : propriÃ©tÃ©s auto-gÃ©rÃ©esAutre petit sucre syntaxique bienvenu : le mot-clÃ© field, qui simplifie la gestion des champs privÃ©s dans les propriÃ©tÃ©s. Si vous avez dÃ©jÃ  implÃ©mentÃ© manuellement le backing field dâ€™une propriÃ©tÃ© pour, par exemple, contrÃ´ler la valeur lors du set (value ?? throwâ€¦), vous savez que câ€™est un peu verbeux. C# 14 permet maintenant dâ€™Ã©crire directement la logique dans lâ€™accesseur set, en utilisant field pour reprÃ©senter le champ sous-jacent.Exemple : au lieu de :private string _msg;public string Message { get =&gt; _msg; set =&gt; _msg = value ?? throw new ArgumentNullException(nameof(value));}On peut Ã©crire simplement :public string Message { get; set =&gt; field = value ?? throw new ArgumentNullException(nameof(value));}Le compilateur se charge de gÃ©nÃ©rer le champ privÃ© cachÃ©. Le code est plus concis et clair. Bien sÃ»r, field nâ€™est valable que dans le contexte dâ€™un accesseur de propriÃ©tÃ©, il reprÃ©sente le stockage interne. Attention si vous avez dÃ©jÃ  une variable nommÃ©e field dans votre classe, cela peut ambiguÃ¯ser, il est conseillÃ© de la renommer ou dâ€™utiliser @field pour lever lâ€™ambiguÃ¯tÃ©.Numeric String Comparer : le tri â€œhumainâ€ dÃ©barqueUn petit irritant qui nous donnait la vie dure : le tri de chaÃ®nes â€œavec des nombresâ€ rend souvent des rÃ©sultats surprenants. Par exemple, quand on trie des versions ou des Ã©tiquettes comme [\"v1\", \"v2\", \"v10\"], un tri lexicographique donnera quelque chose comme [\"v1\", \"v10\", \"v2\"], ce qui est contre-intuitif. ğŸ˜…Avec .NET 10, le numeric string comparer corrige cela. Il permet de comparer les chaÃ®nes en traitant les morceaux numÃ©riques Ã  lâ€™intÃ©rieur comme des nombres, ce qui donne un tri â€œlogiqueâ€ :var list = [\"Windows 10\", \"Windows 7\"];// tri par dÃ©fautlist.Sort(StringComparer.Orginal);Console.WriteLine(string.Join(\", \"), list)); // \"Windows 10\", \"Windows 7\"list.Sort(StringComparer.NumericOrdering);Console.WriteLine(string.Join(\", \"), list)); // \"Windows 7\", \"Windows 10\"Pourquoi câ€™est utile : Pour trier des versions ou des tags dans les interfaces utilisateurs, logs, ou listes de fichiers, oÃ¹ le rÃ©sultat â€œhumainâ€ est prÃ©fÃ©rable. Pour le frontend ou les API qui renvoient des listes de fichiers ou de rÃ©visions, afin dâ€™Ã©viter de devoir post-traiter ou Ã©crire une logique â€œmaisonâ€. Pour les dÃ©veloppeurs de NuGet ou outils CLI qui manipulent des noms de versions, identifiants, etc.nameof plus puissant et autres bricolesLe mot-clÃ© nameof(...) Ã©volue aussi lÃ©gÃ¨rement : il supporte Ã  prÃ©sent les types gÃ©nÃ©riques non construits (unbound generic types). En clair, vous pouvez obtenir le nom dâ€™un type gÃ©nÃ©rique sans spÃ©cifier ses paramÃ¨tres. Par exemple, nameof(Dictionary&lt;T&gt;) renverra â€œDictionaryâ€ (ou une forme qui indique les paramÃ¨tres gÃ©nÃ©riques). Auparavant, ce nâ€™Ã©tait pas possible directement, ce qui obligeait Ã  des contournements. Ce changement, bien que mineur, facilite la rÃ©flexion ou la gÃ©nÃ©ration de code source en Ã©vitant des exceptions ou manipulations de chaÃ®nes peu Ã©lÃ©gantes.Parmi les autres nouveautÃ©s de C# 14, plus ponctuelles, citons : les opÃ©rateurs dâ€™affectation composÃ©s personnalisÃ©s (on peut surcharger operator += par exemple, si le type supporte dÃ©jÃ  +), les constructeurs partiels (utile pour les gÃ©nÃ©rateurs de code source), la possibilitÃ© dâ€™ajouter des modificateurs ref, out ou params aux paramÃ¨tres des lambdas, ou encore lâ€™affectation null-conditionnelle (x?.Prop = value qui Ã©vite de devoir faire un if null avant dâ€™assigner). Ces raffinements visent Ã  rendre le langage plus cohÃ©rent et expressif, sans bouleverser vos habitudes.AmÃ©liorations ASP.NET Core et Ã©cosystÃ¨meMinimal APIs : support de ProblemDetails et autresLe modÃ¨le des Minimal API (introduit en .NET 6) continue de combler ses manques. En .NET 10, la gestion des erreurs de validation de modÃ¨le sâ€™amÃ©liore : les minimal APIs intÃ¨grent dÃ©sormais nativement le support de IProblemDetailsService pour formater les rÃ©ponses dâ€™erreurs. En dâ€™autres termes, si vous avez des Endpoints en minimal API qui valident lâ€™entrÃ©e et retournent une 400 en cas dâ€™erreur, vous pourrez profiter dâ€™une rÃ©ponse JSON standardisÃ©e ProblemDetails sans effort supplÃ©mentaire. Fini les tours de magie pour aligner le format dâ€™erreur sur celui du reste de votre application : le framework fournit une sortie cohÃ©rente, et vous pouvez mÃªme la personnaliser en enregistrant votre implÃ©mentation de ProblemDetailsService (par exemple, pour ajouter un code erreur personnalisÃ©, un lien de documentation, etc.).Au niveau des APIs, ASP.NET Core 14 supporte aussi pleinement les Server-Sent Events (SSE) en sortie des contrÃ´leurs ou minimal APIs, via une mÃ©thode utilitaire TypedResults.ServerSentEvents(...). Cela facilite lâ€™envoi de flux temps-rÃ©el du serveur vers le client sans WebSockets (utile pour du monitoring, des notifications live, etc.). Câ€™Ã©tait faisable Ã  la main avant, mais lÃ  encore le framework le prend en charge nativement.Ouverture et standards : OpenAPI 3.1 et gRPCLe support dâ€™OpenAPI 3.1 est dÃ©sormais intÃ©grÃ© dans .NET 10. Si vous gÃ©nÃ©rez la documentation Swagger de vos APIs, elle sera produite par dÃ©faut au format OpenAPI 3.1 (au lieu de 3.0 prÃ©cÃ©demment). Cette version 3.1 apporte notamment une meilleure prise en charge de JSON Schema 2020-12 (ceux qui ont sacrÃ© avec les schÃ©mas de nullabilitÃ© en OpenAPI 3.0 apprÃ©cieront : plus besoin du champ nullable: true, on utilise directement un type null dans le schÃ©ma). ConcrÃ¨tement, vos int et long nullable apparaÃ®tront correctement comme type: [\"integer\",\"null\"] dans le JSON de la spÃ©cification. Quelques changements breaking sont Ã  noter dans la librairie OpenAPI.NET interne (passage en v2.0 de Microsoft.OpenApi), surtout si vous avez Ã©crit des document filters ou operation filters custom : les types ont un peu changÃ© (les schÃ©mas utilisent une interface IOpenApiSchema au lieu dâ€™une classe concrÃ¨te, etc.). En rÃ©sumÃ©, .NET suit lâ€™Ã©volution du standard OpenApi, pour que vos APIs restent au goÃ»t du jour.Pendant quâ€™on parle de services web : du cÃ´tÃ© de gRPC, notons une amÃ©lioration sympathique : la gestion du streaming cÃ´tÃ© client (Client Streaming) devient plus ergonomique. .NET 10 apporte la prise en charge des message handlers HTTP dans les appels gRPC client, permettant des scÃ©narios de retry ou de logging plus intÃ©grÃ©s. (Dâ€™accord, câ€™est un peu pointu, mais si vous faites du gRPC, jetez un Å“il aux release notes pour dÃ©couvrir ces ajustements.)EF Core 10 : filtres globaux nommÃ©s et Left JoinEntity Framework Core 10 accompagne la sortie de .NET 10, et apporte lui aussi son lot de nouveautÃ©s. Le plus notable est sans doute lâ€™arrivÃ©e des filtres de requÃªte nommÃ©s (named query filters). EF Core propose depuis longtemps les filtres globaux (HasQueryFilter) pour, par exemple, implÃ©menter le soft delete (exclure les entitÃ©s dont IsDeleted = true) ou la multi-tenance (filtrer par TenantId). Cependant, jusquâ€™Ã  prÃ©sent on ne pouvait dÃ©finir quâ€™un seul filtre global par entitÃ©, pas trÃ¨s pratique si lâ€™on voulait combiner, puis en dÃ©sactiver un sÃ©lectivement. EF 14 lÃ¨ve cette limitation : on peut maintenant attacher plusieurs filtres globaux sur un mÃªme modÃ¨le en leur donnant un nom unique.Exemple :modelBuilder.Entity&lt;Blog&gt;() .HasQueryFilter(\"SoftDeletionFilter\", b =&gt; !b.IsDeleted) .HasQueryFilter(\"TenantFilter\", b =&gt; b.TenantId == tenantId);Puis, dans une requÃªte LINQ, dÃ©cider dâ€™ignorer lâ€™un des deux filtres :// RÃ©cupÃ©rer tous les blogs en incluant ceux supprimÃ©s, // mais en gardant le filtre de Tenantvar allBlogs = context.Blogs .IgnoreQueryFilters(\"SoftDeletionFilter\") .ToList();Cette granularitÃ© Ã©tait trÃ¨s attendue pour les applications complexes : on peut enfin combiner proprement des filtres globaux multiples (et Ã©viter les contorsions du style Enable/Disable Filter sur tout le contexte).Parmi les autres amÃ©liorations dâ€™EF Core 14, on notera le support direct des jointures Left Join et Right Join en LINQ. Auparavant, Ã©crire une requÃªte LINQ Ã©quivalente Ã  un LEFT JOIN SQL demandait une syntaxe peu intuitive avec GroupJoin + DefaultIfEmpty.DÃ©sormais, on dispose dâ€™une mÃ©thode dâ€™extension .LeftJoin(...) (et .RightJoin(...)), rendant le code plus lisible.Par exemple :var query = context.Students .LeftJoin( context.Departments, student =&gt; student.DepartmentID, dept =&gt; dept.ID, (student, dept) =&gt; new { student.Name, Department = dept.Name ?? \"[NONE]\" } );EF Core sait traduire Ã§a en SQL (LEFT JOIN ou RIGHT JOIN). Cela ne change rien en termes de performance par rapport Ã  avant, mais niveau lisibilitÃ© du code, câ€™est le jour et la nuit.Enfin, EF 14 ajoute plein de petits plus : la recherche Full-Text sur Azure Cosmos DB, des traductions SQL supplÃ©mentaires (exemple, la mÃ©thode DateOnly.ToDateTime() dÃ©sormais convertie nativement en SQL), des amÃ©liorations de performance sur les split queries (pour Ã©viter des incohÃ©rences de tri dans les requÃªtes fractionnÃ©es), etc. De quoi rendre vos accÃ¨s aux donnÃ©es plus flexibles et efficaces.En route vers .NET 10 : faut-il migrer ?La grande question pour les Ã©quipes : devez-vous passer Ã  .NET 10 rapidement ? La rÃ©ponse dÃ©pend de votre situation, mais voici quelques Ã©lÃ©ments de rÃ©flexion : .NET 10 est une version LTS (support long), ce qui en fait un candidat solide pour vos applications en production. Elle sera supportÃ©e 3 ans (jusquâ€™Ã  la fin de 2028), ce qui donne une vision claire Ã  long terme. Si vous Ãªtes encore sur .NET 8 LTS, rien ne presse (le support de .NET 8 court jusquâ€™en nov. 2026, mais .NET 10 reprÃ©sente la prochaine cible LTS â€œnaturelleâ€). Si vous Ãªtes sur .NET 9 (STS), notez que le nouvel alignement du support fait que .NET 9 expirera en mÃªme temps que .NET 8 en 2026. Autrement dit, .NET 9 ne prolonge pas votre horizon par rapport Ã  .NET 8. Dans ce cas, migrer vers .NET 10 (LTS) dÃ¨s que possible vous remet sur un cycle plus confortable. Les gains de performance et les nouvelles fonctionnalitÃ©s peuvent justifier la migration, surtout si vous avez besoin de lâ€™une des nouveautÃ©s (par exemple, le JSON Patch natif ou les passkeys pour vos utilisateurs). .NET 10 apporte aussi de nombreuses corrections de bugs et amÃ©liorations de stabilitÃ© accumulÃ©es depuis .NET 8 et .NET 9. Breaking changes : Ã©videmment, qui dit nouvelle version dit potentiels changements incompatibles. Microsoft a publiÃ© la liste des breaking changes de .NET. Au moment de la RC1, cette liste nâ€™Ã©tait pas totalement finalisÃ©e, mais on sait dÃ©jÃ  quâ€™il y aura quelques ajustements de comportements (par exxemple, le nameof sur types gÃ©nÃ©riques mentionnÃ© plus haut, ou des API retirÃ©es aprÃ¨s obsolescence). Avant de migrer, examinez attentivement ces notes de compatibilitÃ© pour Ã©valuer lâ€™impact sur votre code. La plupart du temps, lâ€™upgrade se passera sans encombre pour du code standard, mais mieux vaut prÃ©venir que guÃ©rir !En pratique, .NET 10 sâ€™annonce comme une version mature et aboutie, avec un bel Ã©quilibre entre performance, productivitÃ© et modernisation du stack. Si vos tests de validation passent et que vos dÃ©pendances (NuGet, frameworks) supportent .NET 10, il nâ€™y a pas de raison dâ€™attendre trop longtemps. Microsoft elle-mÃªme encourage la mise Ã  niveau : â€œSi vous prÃ©voyez de passer de .NET 9 Ã  10 bientÃ´t, continuez dans cette voie, .NET 10 apporte plein de nouvelles capacitÃ©s et de meilleures performancesâ€.Pour finir : outillage et Ã©cosystÃ¨meUn dernier mot pour les dÃ©veloppeurs curieux : en parallÃ¨le de .NET 10, Microsoft commence Ã  dÃ©voiler la prochaine version de Visual Studio. Eh oui, Visual Studio 2026 (Insiders) est dÃ©jÃ  disponible en aperÃ§u. Au menu : amÃ©liorations de lâ€™Ã©diteur, support de C# 14 bien sÃ»r, et dâ€™une intÃ©gration toujours plus poussÃ©e de GitHub Copilot et de lâ€™IA. Vous pouvez installer Visual Studio 2026 Insiders en parallÃ¨le de votre installation existante sans risque, la grande majoritÃ© des extensions de Visual Studio 2022 sont compatibles, et dÃ¨s cette version la couverture de code est incluse par dÃ©faut dans toutes les Ã©ditions, inutile dÃ©sormais dâ€™avoir la version Entreprise pour en profiter.Enfin, lâ€™Ã©cosystÃ¨me .NET suit le mouvement : attendez-vous Ã  voir arriver Entity Framework 14, ASP.NET Core 14, Blazor, MAUI 14, etc., alignÃ©s sur cette nouvelle mouture. Les outils de build, CI/CD, containers Docker et autres seront mis Ã  jour Ã©galement. La documentation Microsoft Learn est enrichie progressivement (il existe dÃ©jÃ  des pages â€œWhatâ€™s newâ€ pour .NET 10, EF 14, C# 14â€¦ trÃ¨s utiles pour approfondir chaque nouveautÃ©).En rÃ©sumÃ©, .NET 10 marque une Ã©tape importante en 2025 pour les dÃ©veloppeurs .NET. Support Ã©tendu, optimisation de tous les recoins du runtime, nouvelles fonctionnalitÃ©s qui suppriment des irritants (adieu certaines bibliothÃ¨ques tierces obligatoires), langage C# toujours plus expressifâ€¦ Le tout sans rÃ©volution brutale : la montÃ©e en version devrait Ãªtre relativement fluide. Que vous soyez un dÃ©veloppeur back-end curieux des moindres gains de performances, ou un lead dev prudent qui ne jure que par les LTS stables, .NET 10 a de quoi vous sÃ©duire.Alors, allez-vous migrer ? Si vous cherchez la sÃ©curitÃ© et la durabilitÃ©, .NET 10 LTS est lÃ  pour vous jusquâ€™en 2028. Si vous aimez les nouveautÃ©s, foncez, vous aurez de quoi vous amuser. Dans tous les cas, commencez Ã  Ã©valuer cette version, Ã  faire tourner vos tests unitaires dessus, et prÃ©parez sereinement lâ€™avenir de vos applications.Bonne exploration de .NET 10, et happy coding !" }, { "title": "Les design patternsÂ - les bons Legos pour vos applications .NET", "url": "/posts/utilisation-design-patterns/", "categories": "architecture", "tags": "", "date": "2025-09-22 10:00:00 -0400", "snippet": "IntroductionÂ : Ne rÃ©inventez pas la roue, assemblez les LegosEn conception logicielle, un design pattern (ou patron de conception) est une solution Ã©prouvÃ©e Ã  un problÃ¨me de conception rÃ©current. C...", "content": "IntroductionÂ : Ne rÃ©inventez pas la roue, assemblez les LegosEn conception logicielle, un design pattern (ou patron de conception) est une solution Ã©prouvÃ©e Ã  un problÃ¨me de conception rÃ©current. Ce sont en quelque sorte nos briques de Lego Ã  nous, dÃ©veloppeursÂ : plutÃ´t que de tailler chaque piÃ¨ce dans le bois, on pioche dans la boÃ®te le composant adaptÃ© au bon moment. Bien comprendre ces patterns permet dâ€™assembler des systÃ¨mes plus robustes et Ã©volutifs, un peu comme construire un chÃ¢teau de Lego solide sans recoller les briques au chewing-gum. ğŸ˜‰En dâ€™autres termes, les design patterns nous Ã©vitent de rÃ©inventer la roue sur chaque projet. Ils dÃ©finissent des structures de classes ou dâ€™objets qui ont fait leurs preuves. Par exemple, le Singleton sâ€™assure de lâ€™existence dâ€™un seul objet de son genre et fournit un point dâ€™accÃ¨s global Ã  celui-ci. Utiliser un pattern appropriÃ© au bon contexte, câ€™est comme choisir la bonne piÃ¨ce Lego standard plutÃ´t que de mouler la vÃ´tre. Cela accÃ©lÃ¨re le dÃ©veloppement et amÃ©liore la maintenabilitÃ©, tout en respectant les principes SOLID.Avant de plonger dans sept patterns importants pour les applications de gestion en .NET Core, mentionnons une excellente ressourceÂ : la sÃ©rie de vidÃ©os de Christopher Okhravi sur les design patterns (disponible sur YouTube). Elle reste encore aujourdâ€™hui une rÃ©fÃ©rence pÃ©dagogique (et humoristique) sur le sujet. Vous verrez quâ€™avec un peu de pratique, les design patterns deviendront vos alliÃ©s du quotidien pour structurer proprement un code mÃ©tier (facturation, inventaire, etc.) sans transformer votre code en plat de spaghetti. ğŸNous allons explorer les patterns suivants, avec pour chacun une explication, un exemple concret, un extrait de code C#, les cas dâ€™usage Ã  favoriser (et Ã  Ã©viter), ainsi que des astuces dâ€™intÃ©gration en .NET (notamment via lâ€™IoC container et autres outils comme Scrutor pour le pattern Decorator).En pisteÂ !SingletonÂ : Un seul pour les gouverner tous, un seul pour les instancier,Le pattern Singleton garantit quâ€™une classe nâ€™a quâ€™une seule instance accessible globalement.Le Singleton est le Highlander des patternsÂ : Â«Â There can be only one!Â Â» ğŸ˜… Son objectif est de restreindre lâ€™instanciation dâ€™une classe Ã  un seul objet, partagÃ© et accessible partout. Dans une application de gestion, on utilise souvent ce pattern pour des objets qui doivent Ãªtre uniquesÂ : par exemple un gestionnaire de configuration global, une cache en mÃ©moire ou encore un logger central. Cela Ã©vite dâ€™avoir plusieurs copies incohÃ©rentes de ces ressources critiques dissÃ©minÃ©es un peu partout.Exemple concretImaginons un systÃ¨me de facturation qui doit accÃ©der aux paramÃ¨tres de configuration (taux de TVQ, devise, etc.) depuis nâ€™importe quel module. On pourrait crÃ©er une classe ConfigurationManager en Singleton. Ainsi, que ce soit le module de gÃ©nÃ©ration de facture ou le module dâ€™inventaire, tous rÃ©cupÃ¨rent la mÃªme instance unique du ConfigurationManager avec les paramÃ¨tres chargÃ©s au lancement de lâ€™application.public sealed class ConfigurationManager{ private static readonly ConfigurationManager _instance = new(); public static ConfigurationManager Instance =&gt; _instance; // DonnÃ©es de configuration (exemple) public decimal TauxTVQ { get; private set; } public string DeviseParDefaut { get; private set; } // Constructeur privÃ© pour empÃªcher les instanciations externes private ConfigurationManager() { // Simulation de chargement depuis une source de vÃ©ritÃ© (fichier, base, ou grimoire fiscal) TauxTVQ = 0.09975m; // TVQ actuelle au QuÃ©bec DeviseParDefaut = \"CAD\"; // Vive le dollar canadien! } // MÃ©thode d'accÃ¨s optionnelle public static ConfigurationManager GetInstance() =&gt; Instance;}Dans le code ci-dessus, ConfigurationManager.Instance renverra toujours la mÃªme instance, initialisÃ©e une seule fois. On dÃ©clare le constructeur en private pour empÃªcher de faire new ConfigurationManager(). Seule la propriÃ©tÃ© statique Instance permet dâ€™obtenir lâ€™objet unique.Quand lâ€™utiliser ğŸŸ¢ : Lorsquâ€™une seule instance dâ€™un service doit exister et Ãªtre partagÃ©e partout. Exemples classiquesÂ : configuration globale, journalisation (logging), connexion unique Ã  une base de donnÃ©es, cache applicatif. Cela permet de centraliser un Ã©tat ou un accÃ¨s et dâ€™Ã©viter les duplications.Ã€ Ã©viter ğŸ”´ : Nâ€™abusez pas du Singletonâ€¯! Un Singleton mal placÃ© peut devenir un god object omniprÃ©sent qui rend le code difficile Ã  tester (dÃ©pendances globales cachÃ©es) et moins modulable. Si plusieurs instances seraient acceptables ou que lâ€™objet a un cycle de vie limitÃ©, prÃ©fÃ©rez des instances normales gÃ©rÃ©es par injection de dÃ©pendances. En .NET, beaucoup considÃ¨rent dâ€™ailleurs le Singleton comme un antipattern si utilisÃ© Ã  tort et Ã  travers.IntÃ©gration .NET ğŸ‘©â€ğŸ’» : PlutÃ´t que dâ€™implÃ©menter le Singleton â€œÃ  la mainâ€ comme ci-dessus, on profite souvent du conteneur dâ€™inversion de contrÃ´le (IoC) de .NET. En enregistrant une classe en singleton dans le container, le framework garantit lui-mÃªme lâ€™unicitÃ© de lâ€™instance. Par exemple, dans le Program.csÂ :services.AddSingleton&lt;IConfigurationManager, ConfigurationManager&gt;();Ici le container va crÃ©er une seule instance de ConfigurationManager pour toute lâ€™application. Câ€™est thread-safe et plus simple Ã  tester (on peut Ã©ventuellement substituer lâ€™implÃ©mentation via lâ€™interface). Astuce: Si votre Singleton est lourd Ã  instancier et pas toujours utilisÃ©, envisagez le pattern du Lazy Singleton (instanciation diffÃ©rÃ©e).FactoryÂ : Fabrique dâ€™objets, Ã©vitez le newLe pattern Factory (ici la Factory Method) crÃ©e des objets selon le type demandÃ© sans exposer la logique de crÃ©ation.Le pattern Factory est un patron de crÃ©ation qui repose sur le principe suivantÂ : Â«â€¯Ne dites pas new, dites Factory!â€¯Â». PlutÃ´t que dâ€™instancier directement des classes dans votre code (ce qui le couplerait Ã  des implÃ©mentations concrÃ¨tes), vous dÃ©lÃ©guez cette tÃ¢che Ã  une fabrique. La Factory centralise la logique de crÃ©ation et peut dÃ©cider quel sous-type concret retourner en fonction du contexte. Câ€™est un peu comme une usine qui, sur base dâ€™une commande, sort le bon produit de la chaÃ®ne sans que lâ€™acheteur ne sache exactement comment il a Ã©tÃ© fabriquÃ©.Exemple concretDans un systÃ¨me de gestion documentaire (factures, bons de commande, devis, etc.), on peut avoir une interface commune IDocument et plusieurs implÃ©mentations (Facture, Devis, BonCommandeâ€¦). Au lieu dâ€™Ã©parpiller du code new Facture() partout, on dÃ©finit une factory qui va produire le bon type de document selon un paramÃ¨tre. Par exemple, une mÃ©thode DocumentFactory.CreateDocument(TypeDocument type) qui retourne un IDocument :// Type Ã©numÃ©rÃ© pour les diffÃ©rents documents possiblespublic enum TypeDocument{ Facture, Devis, BonCommande}public interface IDocument { string Numero { get; } DateTime DateEmission { get; } // ... autres membres communs}public class Facture : IDocument { /* ... */ }public class Devis : IDocument { /* ... */ }public class BonCommande : IDocument { /* ... */ }public static class DocumentFactory { public static IDocument CreateDocument(TypeDocument type) { return type switch { TypeDocument.Facture =&gt; new Facture(), TypeDocument.Devis =&gt; new Devis(), TypeDocument.BonCommande =&gt; new BonCommande(), _ =&gt; throw new ArgumentException(\"Type de document inconnu\") }; }}Ici, le DocumentFactory expose une mÃ©thode statique de crÃ©ation. Le client appelle par exemple IDocument doc = DocumentFactory.CreateDocument(TypeDocument.Facture); pour obtenir une facture prÃªte Ã  lâ€™emploi, sans savoir quelle classe concrÃ¨te est instanciÃ©e. Si demain on ajoute un nouveau type de document (par exemple un avoir), il suffit dâ€™Ã©tendre la factory sans impacter le code client.Quand lâ€™utiliser ğŸŸ¢ : Quand la logique dâ€™instanciation est complexe ou conditionnelle. Par exemple, si la crÃ©ation dâ€™un objet dÃ©pend de paramÃ¨tres (contexte utilisateur, configuration) ou nÃ©cessite de dÃ©cider parmi plusieurs types dÃ©rivÃ©s. Les Factories amÃ©liorent la lisibilitÃ© (on donne un nom explicite au processus de crÃ©ation) et centralisent en un seul endroit le code qui fait des new. TrÃ¨s utile pour : Factory MethodÂ : laisser des sous-classes dÃ©cider de lâ€™instanciation (patron utilisÃ© dans les frameworks, par exemple pour crÃ©er des contrÃ´les UI spÃ©cifiques). Abstract FactoryÂ : groupe de factories pour crÃ©er des familles dâ€™objets liÃ©s (exemple une AbstractFactory pour GUI qui peut produire soit des boutons et fenÃªtres version â€œWindowsâ€, soit version â€œmacOSâ€).Ã€ Ã©viter ğŸ”´ : Si la crÃ©ation de lâ€™objet est triviale et ne nÃ©cessite pas de logique conditionnelle, une factory ajoute une complexitÃ© inutile. Inutile de sur-ingÃ©nierie : on ne va pas faire une factory pour instancier un simple DTO par exemple. De plus, trop de factories peuvent rendre le code abstrait Ã  lâ€™excÃ¨s (on finit par avoir des usines qui fabriquent dâ€™autres usinesâ€¦). Comme toujours, dosez le pattern oÃ¹ il apporte une rÃ©elle valeur.IntÃ©gration .NET ğŸ‘©â€ğŸ’» : Les IoC containers rÃ©duisent parfois le besoin explicite de factory, car ils peuvent eux-mÃªmes choisir quelle implÃ©mentation injecter selon la configuration. Par exemple, on peut enregistrer plusieurs implÃ©mentations dâ€™une interface et en sÃ©lectionner une par nom ou via une factory lambda dans le container. NÃ©anmoins, le pattern Factory reste utile pour crÃ©er des objets mÃ©tiers complexes.En .NET Core, vous pouvez aussi injecter une factory sous forme de fonction. Ex: Func&lt;TypeDocument, IDocument&gt; que lâ€™IoC pourrait construire. Une autre approche est dâ€™utiliser la factory en tant que serviceÂ : vous crÃ©ez une classe DocumentFactory (non statique) enregistrÃ©e en singleton, et qui a peut-Ãªtre besoin dâ€™autres services (elle peut les recevoir via injection dans son constructeur) pour fabriquer les objets. Cette technique vous permet par exemple de choisir lâ€™implÃ©mentation en fonction de donnÃ©es Ã  lâ€™exÃ©cution (on le verra avec le patron Strategy juste aprÃ¨s).StrategyÂ : des algorithmes interchangeables, choisis Ã  la volÃ©eLe pattern Strategy dÃ©finit des algorithmes interchangeables. Le contexte dÃ©lÃ¨gue Ã  une stratÃ©gie concrÃ¨te (ConcreteStrategyA ou ConcreteStrategyB) choisie dynamiquement.Le pattern Strategy permet de dÃ©finir une famille dâ€™algorithmes, de les encapsuler chacun dans une classe distincte, et de les rendre interchangeables Ã  la volÃ©e dans le contexte oÃ¹ ils sont utilisÃ©s. En clair, on sÃ©pare le quoi (lâ€™algorithme Ã  exÃ©cuter) du quand/comment il est utilisÃ©.Lâ€™un des grands bÃ©nÃ©fices de ce pattern est quâ€™il respecte le principe dâ€™ouverture/fermeture (Open/Closed Principle) : votre code est ouvert Ã  lâ€™extension, mais fermÃ© Ã  la modification. Autrement dit, si un jour un nouveau mode de calcul doit Ãªtre ajoutÃ© (par exemple une livraison par drone ğŸ›¸), pas besoin de toucher au calculateur existant : on ajoute une nouvelle stratÃ©gie, et câ€™est tout.Câ€™est une alternative Ã©lÃ©gante et maintenable aux chaÃ®nes de if/else ou de switch sur des enums, qui violent souvent ce principe (car chaque ajout nÃ©cessite de modifier le bloc conditionnel existant). Avec le patron Strategy, on Ã©tend le comportement en ajoutant une nouvelle classe, plutÃ´t quâ€™en modifiant du code existant â€” ce qui rÃ©duit les risques de rÃ©gression.Exemple concretDans une application de gestion de commerce en ligne, prenons le calcul des frais de livraison. Selon le mode dâ€™expÃ©dition choisi par le client, le calcul du coÃ»t diffÃ¨re (transporteur standard, express, retrait en magasin gratuit, etc.). Sans pattern, on aurait peut-Ãªtre dans la classe Commande un code semblable Ã Â :decimal frais = mode switch{ \"Standard\" =&gt; CalculerStandard(), \"Express\" =&gt; CalculerExpress(), \"Magasin\" =&gt; 0, _ =&gt; throw new NotImplementedException()};...Avec le patron Strategy, on va crÃ©er une interface IFraisLivraisonStrategy avec une mÃ©thode CalculerFrais(Commande commande). Puis on implÃ©mente une classe concrÃ¨te par modeÂ : StandardStrategy, ExpressStrategy, RetraitMagasinStrategy, etc., chacune encapsulant son calcul. Le contexte (par exemple la classe CalculateurLivraison) utilise une rÃ©fÃ©rence Ã  IFraisLivraisonStrategy.On peut pousser plus loin en combinant avec une Factory Method pour choisir la stratÃ©gie en fonction de la commandeÂ :// StratÃ©gie de calcul des frais de livraisonpublic interface IFraisLivraisonStrategy { decimal CalculerFrais(Commande cmd);}// ImplÃ©mentations concrÃ¨tespublic class LivraisonStandard : IFraisLivraisonStrategy { public decimal CalculerFrais(Commande cmd) =&gt; 5m; // Forfait simple}public class LivraisonExpress : IFraisLivraisonStrategy { public decimal CalculerFrais(Commande cmd) =&gt; cmd.Poids * 1.5m;}public class RetraitMagasin : IFraisLivraisonStrategy { public decimal CalculerFrais(Commande cmd) =&gt; 0m;}// Contexte qui utilise une stratÃ©giepublic class CalculateurLivraison { private IFraisLivraisonStrategy _strategie; // On devrait injecter la stratÃ©gie via le constructeur ! public void ChoisirStrategie(IFraisLivraisonStrategy strat) =&gt; _strategie = strat; public decimal CalculerFrais(Commande cmd) =&gt; return _strategie?.CalculerFrais(cmd) ?? throw new InvalidOperationException(\"StratÃ©gie non dÃ©finie.\");}Et quelque part dans le code appelant (par exemple lors de la finalisation de la commande)Â :var calculateur = new CalculateurLivraison();IFraisLivraisonStrategy strategie = modeChoisi switch { \"Standard\" =&gt; new LivraisonStandard(), \"Express\" =&gt; new LivraisonExpress(), \"Magasin\" =&gt; new RetraitMagasin(), _ =&gt; throw new InvalidOperationException(\"Mode inconnu\")};calculateur.ChoisirStrategie(strategie);decimal frais = calculateur.CalculerFrais(commande);Ici, on choisit la stratÃ©gie dynamiquement en fonction dâ€™un paramÃ¨tre modeChoisi. Le calculateur de livraison nâ€™a pas besoin de connaÃ®tre les dÃ©tails de chaque mode, il dÃ©lÃ¨gue Ã  la stratÃ©gie. Si un jour on ajoute un mode Drone ou Ã  dos de vache ğŸ®, on crÃ©e une nouvelle classe implÃ©mentant IFraisLivraisonStrategy et on ajuste la sÃ©lection (idÃ©alement via une factory au lieu dâ€™un switch inline).Quand lâ€™utiliser ğŸŸ¢ : Quand vous avez plusieurs variantes algorithmiques interchangeables selon un critÃ¨re (par exemple, diffÃ©rents rÃ¨gles de calcul, politiques, stratÃ©gies de tarification, etc.). Le Strategy apporte de la flexibilitÃ©Â : on peut mÃªme changer la stratÃ©gie en cours dâ€™exÃ©cution si besoin. Câ€™est aussi un bon moyen dâ€™adhÃ©rer au principe Open/Closed â€“ ajouter une nouvelle stratÃ©gie nâ€™impacte pas les existantes ni le contexte.Ã€ Ã©viter ğŸ”´ : Si vos variantes dâ€™algorithmes sont trÃ¨s simples ou quâ€™il nâ€™y en a quâ€™une ou deux peu susceptibles dâ€™Ã©voluer, le pattern peut Ãªtre overkill. Aussi, nâ€™introduisez pas une stratÃ©gie juste pour Ã©viter un if unique â€” le remÃ¨de serait pire que le mal. Enfin, veillez Ã  ce que les stratÃ©gies partagent bien la mÃªme interface commune et puissent rÃ©ellement varier indÃ©pendamment du resteÂ : si elles finissent par dÃ©pendre fortement du contexte externe, le gain de dÃ©couplage diminue.IntÃ©gration .NET ğŸ‘©â€ğŸ’» : On peut tirer parti de lâ€™IoC container pour gÃ©rer les stratÃ©gies. Par exemple, vous pourriez injecter toutes les implÃ©mentations dâ€™une interface et sÃ©lectionner la bonne Ã  lâ€™exÃ©cution (via un dictionnaire de stratÃ©gies, ou en taguant chaque implÃ©mentation dâ€™un attribut \\ [Strategy(\"Nom\")]). Une autre approche consiste Ã  enregistrer une Factory (comme vue plus haut) dans lâ€™IoC qui retourne la stratÃ©gie voulue. .NET Core facilite mÃªme cela avec les Named Options ou en combinant avec le pattern Policy. Bref, le patron Strategy se marie bien avec lâ€™injection de dÃ©pendances pour Ã©viter de faire de new manuellement : on peut demander au container de rÃ©soudre la stratÃ©gie dont on a besoin, ce qui simplifie le remplacement (ex: injection dâ€™une fausse stratÃ©gie en tests unitaires).DecoratorÂ : Ajouter des fonctionnalitÃ©s Ã  la volÃ©e (comme une cache sur vos services)Le pattern Decorator ajoute dynamiquement des comportements Ã  un objet en lâ€™enveloppant dans un â€œdÃ©corateurâ€ qui implÃ©mente la mÃªme interface.Le pattern Decorator (ou Wrapper) est un patron structurel qui permet dâ€™ajouter dynamiquement des responsabilitÃ©s Ã  un objet, sans modifier son code source. Imaginez un cadeau quâ€™on emballe et re-emballeÂ : le contenu reste le mÃªme, mais chaque couche de papier ajoute une fonctionnalitÃ© (un message, un ruban, etc.). En informatique, un dÃ©corateur est un objet qui implÃ©mente la mÃªme interface que lâ€™objet quâ€™il dÃ©core, et qui contient une rÃ©fÃ©rence vers lui-mÃªme. Il peut ainsi intercepter les appels, faire quelque chose en plus, puis dÃ©lÃ©guer Ã  lâ€™objet rÃ©el.Exemple concretConsidÃ©rons un microservice dâ€™inventaire qui expose un service IProduitService avec une mÃ©thode ObtenirProduit(int id) retournant les dÃ©tails dâ€™un produit. Les appels Ã  ce service peuvent Ãªtre coÃ»teux (imaginons quâ€™il interroge une base de donnÃ©es distante). On souhaite mettre en cache les rÃ©sultats pour amÃ©liorer les performances. Sans changer le code du service existant, on peut crÃ©er un dÃ©corateur de cache.public interface IProduitService{ ValueTask&lt;Produit&gt; ObtenirProduit(int id);}// ImplÃ©mentation principale (accÃ¨s base de donnÃ©es par exemple)public class ProduitService : IProduitService{ public async ValueTask&lt;Produit&gt; ObtenirProduit(int id) { Console.WriteLine($\"AccÃ¨s BDD pour le produit {id}\"); await Task.Delay(50); // Simule un accÃ¨s I/O return new Produit { Id = id, Nom = $\"Produit {id}\" }; }}// Decorator de cachepublic class ProduitServiceCacheDecorator : IProduitService{ private readonly IProduitService _produitService; // Devrait provenir de l'IoC private readonly MemoryCache _cache = new(new MemoryCacheOptions()); public ProduitServiceCacheDecorator(IProduitService produitService) { _produitService = produitService; } public ValueTask&lt;Produit&gt; ObtenirProduit(int id) { if (_cache.TryGetValue(id, out Produit produit)) { Console.WriteLine($\"Cache hit pour le produit {id}\"); return new ValueTask&lt;Produit&gt;(produit); // disponible de maniÃ¨re synchrone } return ObtenirEtCacher(id); } private async ValueTask&lt;Produit&gt; ObtenirEtCacher(int id) { var resultat = await _produitService.GetProduit(id); _cache.Set(id, resultat, TimeSpan.FromMinutes(5)); return resultat; }} ğŸ’¡ Pourquoi ValueTask ici ? Parce que dans le cas oÃ¹ la cache rÃ©pond, il est inutile de crÃ©er une Task, ValueTask permet de rÃ©duire la pression sur le GC pour les cas de rÃ©ponse rapide. Mais attention : si le service sous-jacent est trÃ¨s souvent asynchrone, Task&lt;T&gt; reste le bon choix !Ici, ProduitServiceCacheDecorator dÃ©core un IProduitService concret (_produitService). Il ajoute la fonctionnalitÃ© de caching autour de lâ€™appel rÃ©el. Pour le consommateur, câ€™est transparentÂ : il utilise un IProduitService sans savoir si câ€™est le cache ou le service de base. On peut composer plusieurs dÃ©corateurs les uns avec les autres si besoin (log, sÃ©curitÃ©, etc.), chaque dÃ©corateur enveloppant le prÃ©cÃ©dent.Quand lâ€™utiliser ğŸŸ¢ : Quand vous voulez enrichir ou modifier dynamiquement le comportement dâ€™un objet sans altÃ©rer son code. Câ€™est idÃ©al pour les fonctionnalitÃ©s transversales (caching, logging, contrÃ´le dâ€™accÃ¨s, mesure de performanceâ€¦). Le Decorator est plus souple que lâ€™hÃ©ritage car on peut empiler plusieurs dÃ©corateurs et en activer/dÃ©sactiver certains Ã  la configuration.Ã€ Ã©viter ğŸ”´ : Si la hiÃ©rarchie de dÃ©corateurs devient trop complexe ou que vous en avez un trÃ¨s grand nombre, Ã§a peut devenir difficile Ã  suivre en debug (effet oignon, on se perd dans les couches). Si lâ€™objet de base a dÃ©jÃ  une bonne extension via des mÃ©canismes plus simples, inutile de rajouter en plus des dÃ©corateurs. Enfin, nâ€™utilisez pas un dÃ©corateur juste pour factoriser du code commun entre deux classesÂ : dans ce cas, une abstraction ou un mixin serait plus appropriÃ©.IntÃ©gration .NET ğŸ‘©â€ğŸ’» : .NET Core facilite la vie avec lâ€™IoC container pour chaÃ®ner les dÃ©corateurs sans douleur, grÃ¢ce Ã  des librairies comme Scrutor. Scrutor fournit une mÃ©thode dâ€™extension Decorate&lt;,&gt;() qui enregistre un dÃ©corateur pour un service existant. Par exemple, pour notre cas ci-dessusÂ :services.AddScoped&lt;IProduitService, ProduitService&gt;();services.Decorate&lt;IProduitService, ProduitServiceCacheDecorator&gt;();Comme le dÃ©crit Andrew Lock, â€œScrutor cherche tout service enregistrÃ© en IProduitService (ici ProduitService) et le remplace par ProduitServiceCacheDecorator qui prendra en paramÃ¨tre de constructeur lâ€™ancien serviceâ€. En un appel, on a intercalÃ© le cache entre lâ€™application et le service rÃ©el. Scrutor gÃ¨re lâ€™ordre (il faut ajouter le dÃ©corateur aprÃ¨s le service de base) et la rÃ©solution des dÃ©pendances supplÃ©mentaires du dÃ©corateur.Sans Scrutor, on peut toujours enregistrer manuellement un dÃ©corateurÂ : par exemple, on enregistre ProduitService puis on enregistre IProduitService pointant vers un provider =&gt; new ProduitServiceCacheDecorator(new ProduitService(...)). Mais avouons que Scrutor fait Ã§a proprement, de faÃ§on dÃ©clarative.MiddlewareÂ : le pipeline HTTP dans ASP.NET CorePipeline de middlewares ASP.NET CoreÂ : chaque middleware (1, 2, 3) traite la requÃªte puis appelle le suivant, formant une chaÃ®ne autour de la requÃªte/rÃ©ponse.Le terme Middleware dÃ©signe un composant logiciel intermÃ©diaire qui sâ€™insÃ¨re dans une chaÃ®ne de traitement. En ASP.NET Core, le pipeline de requÃªte HTTP est une illustration concrÃ¨te du pattern Chain of Responsibility appliquÃ© aux requÃªtes et rÃ©ponses HTTP. Chaque middleware a la possibilitÃ© dâ€™agir sur la requÃªte entrante, de dÃ©cider de la court-circuiter (renvoyer directement une rÃ©ponse) ou de faire quelque chose avant/aprÃ¨s de passer la main au middleware suivant.En fait, un middleware ASP.NET Core est une implÃ©mentation spÃ©cialisÃ©e du pattern Decorator/Chain, dÃ©diÃ©e aux requÃªtes HTTP. Vous en utilisez Ã  chaque fois que vous faites app.UseXyz(...) dans le Program.cs : authentification, logging, routing, etc., sont des middlewares standards.Exemple concretSupposons que lâ€™on veuille journaliser le temps de traitement de certaines requÃªtes pour une API de gestion dâ€™inventaire. On peut Ã©crire un middleware custom LoggingMiddleware qui, pour chaque requÃªte, enregistre lâ€™heure de dÃ©but, appelle le composant suivant, puis enregistre lâ€™heure de fin et calcule la durÃ©e.public class LoggingMiddleware{ private readonly RequestDelegate _next; private readonly TimeProvider _timeProvider; public LoggingMiddleware(RequestDelegate next, TimeProvider timeProvider) { _next = next; _timeProvider = timeProvider; } public async Task Invoke(HttpContext context) { var debut = _timeProvider.GetUtcNow(); await _next(context); var duration = _timeProvider.GetUtcNow() - debut; Console.WriteLine($\"RequÃªte {context.Request.Path} traitÃ©e en {duration.TotalMilliseconds} ms\"); }}Dans le Program.cs, on lâ€™enregistre dans le pipelineÂ :var app = builder.Build();app.UseMiddleware&lt;LoggingMiddleware&gt;();Chaque requÃªte HTTP va passer par ce middleware, puis continuer. On pourrait chaÃ®ner dâ€™autres middlewares avant/aprÃ¨s. Par exemple, on pourrait ajouter un middleware de cache en amont qui vÃ©rifie si la rÃ©ponse nâ€™est pas dÃ©jÃ  en cache (et ne pas appeler _next du tout sâ€™il trouve quelque chose), ou un middleware dâ€™authentification qui vÃ©rifie le token JWT puis appelle _next si OK, ou renvoie 401 immÃ©diatement si non autorisÃ©.Quand lâ€™utiliser ğŸŸ¢ : Les middlewares sont incontournables en dÃ©veloppement ASP.NET Core pour tout ce qui est cross-cutting au niveau des requÃªtes HTTP. Câ€™est littÃ©ralement la faÃ§on de faire officielle pour filtrer/travailler sur les requÃªtes et rÃ©ponses. Dans un contexte plus large, on peut assimiler nâ€™importe quelle suite de traitements sÃ©quentiels modulaires Ã  ce concept de middleware. Donc utilisez ce pattern/pipeline dÃ¨s que vous avez un traitement en plusieurs Ã©tapes oÃ¹ chaque Ã©tape peut dÃ©cider de stopper ou de modifier le flux.Ã€ Ã©viter ğŸ”´ : En dehors du contexte web, Ã©vitez dâ€™abuser des pipelines si un simple appel direct suffit. Inutile de sur-architecturer une simple sÃ©quence dâ€™appels en pipeline ultra-gÃ©nÃ©rique si elle ne sera jamais modifiÃ©e. Pour ASP.NET, faites attention Ã  lâ€™ordre des middlewaresÂ : une mauvaise ordre (par exemple placer lâ€™authentification aprÃ¨s le routing alors quâ€™on en a besoin avant) peut causer des bugs subtils. Ce nâ€™est pas tant le pattern en lui-mÃªme quâ€™il faut Ã©viter, mais plutÃ´t sa mauvaise configuration.IntÃ©gration .NET ğŸ‘©â€ğŸ’» : Pour ASP.NET Core, lâ€™intÃ©gration est native (mÃ©thodes UseMiddleware&lt;T&gt;(), etc.). Vous pouvez profiter de lâ€™injection de dÃ©pendances dans vos middlewares en dÃ©finissant un constructeur qui prend les services voulus (le framework les injectera). Ainsi votre middleware peut trÃ¨s bien Ãªtre un dÃ©corateur qui utilise un service de cache ou un repo injectÃ© du container. Enfin, notez que dâ€™autres librairies adoptent ce pattern de pipeline configurableÂ : citons MediatR (avec ses Pipeline Behaviors qui agissent autour des requÃªtes MediatR), ou Message delegating handlers pour HttpClient. Comprendre le fonctionnement des middlewares vous aidera donc dans de nombreux recoins du framework .NET.MediatorÂ : un hub pour la communication, grÃ¢ce Ã  MediatRLe pattern Mediator dÃ©finit un objet intermÃ©diaire qui centralise les communications entre plusieurs composants au lieu quâ€™ils interagissent directement. Imaginez une tour de contrÃ´le dâ€™aÃ©roport : les avions (composants) ne se parlent pas entre eux en direct, ils parlent tous Ã  la tour (mÃ©diateur) qui coordonne tout. Cela rÃ©duit le couplage : chaque objet a juste Ã  connaÃ®tre le mÃ©diateur, pas les dÃ©tails des autres.En .NET, on utilise frÃ©quemment une librairie nommÃ©e MediatR (de Jimmy Bogard) pour implÃ©menter ce pattern. MediatR permet dâ€™envoyer des requÃªtes/commandes et de les faire traiter par un ou plusieurs handlers enregistrÃ©s, sans que lâ€™Ã©metteur et le rÃ©cepteur se connaissent.Exemple concretDans une application de gestion dâ€™inventaire, imaginons quâ€™on veuille dÃ©coller la logique mÃ©tier des contrÃ´leurs API. On peut dÃ©finir une commande du domaine, par exemple AjouterProduitCommand (avec les infos du produit), et un handler associÃ© AjouterProduitHandler qui contient la logique pour ajouter le produit (vÃ©rifier stock, enregistrer en base de donnÃ©es, etc.). Le contrÃ´leur va juste envoyer la commande via le mÃ©diateur, qui lui se charge de trouver et exÃ©cuter le handler adÃ©quat.Avec MediatR, cela se traduit parÂ :// DÃ©finition d'une commande (implÃ©mente IRequest&lt;T&gt; de MediatR)public record AjouterProduitCommand(string Reference, int Quantite) : IRequest&lt;ResultatProduit&gt;;// ImplÃ©mentation du handler pour cette commandepublic class AjouterProduitHandler : IRequestHandler&lt;AjouterProduitCommand, ResultatProduit&gt;{ private readonly IProduitRepository _produitRepository; public AjouterProduitHandler(IProduitRepository produitRepository) { _produitRepository = produitRepository; } public async Task&lt;ResultatProduit&gt; Handle(AjouterProduitCommand command, CancellationToken cancellationToken) { Produit prod = new Produit { Reference = command.Reference, Quantite = command.Quantite }; await _produitRepository.Ajouter(prod, cancellationToken); return new ResultatProduit(prod.Id, \"Produit ajoutÃ© avec succÃ¨s\"); }}Dans le contrÃ´leur (ou nâ€™importe quel endroit du code) qui a besoin dâ€™ajouter un produit, on feraitÂ :// _mediator est injectÃ© via IMediator de MediatRvar resultat = await _mediator.Send(new AjouterProduitCommand(\"REF123\", 50));Console.WriteLine(resultat.Message);Ici, _mediator.Send(...) va en coulisses trouver le AjouterProduitHandler (grÃ¢ce au container IoC et MediatR), exÃ©cuter sa mÃ©thode Handle, et renvoyer le rÃ©sultat. Le contrÃ´leur nâ€™a aucune rÃ©fÃ©rence directe sur le handler ou le repository, il passe par le mÃ©diateur.Quand lâ€™utiliser ğŸŸ¢ : Lorsque vous voulez dÃ©coupler fortement les composants qui interagissent. Le Mediator est roi dans les architectures CQRS / MÃ©diation : il permet dâ€™envoyer des Commandes et Query sans lier le code dâ€™envoi et le code de traitement. Utile aussi pour implÃ©menter un systÃ¨me de notifications/Ã©vÃ©nements internes : par exemple, plusieurs parties de lâ€™application Ã©coutent un Ã©vÃ©nement via Mediator (avec MediatR ce sont les INotification et leurs handlers multiples). En somme, dÃ¨s que votre application commence Ã  ressembler Ã  un enchevÃªtrement de signaux entre modules, introduire un mÃ©diateur peut apporter de la lisibilitÃ© et une architecture en Ã©toile plutÃ´t quâ€™en plat de nouilles.Ã€ Ã©viter ğŸ”´ : Si le trafic via le mÃ©diateur devient trop centralisÃ©, le Mediator peut lui-mÃªme devenir un goulot dâ€™Ã©tranglement ou un god object dÃ©guisÃ© (ex: un seul mediator gigantesque qui sait trop de choses). Il faut lâ€™utiliser pour ce quâ€™il fait bien : la dÃ©couplage de lâ€™envoi et du traitement. Si deux composants sont naturellement faits pour interagir directement (faible couplage, usage local), inutile de forcer le passage par un mÃ©diateur. Par ailleurs, un excÃ¨s de mÃ©diation peut compliquer le suivi du flux dâ€™appel (on perd un peu la trace de â€œqui appelle quiâ€ car tout passe par le hub central). Comme toujours, câ€™est un Ã©quilibre.IntÃ©gration .NET ğŸ‘©â€ğŸ’» : Lâ€™intÃ©gration de MediatR en .NET Core est trÃ¨s simpleÂ : on installe le package, puis dans Program.cs on fait services.AddMediatR(cfg =&gt; cfg.RegisterServicesFromAssemblyContaining&lt;Program&gt;()); (ou Ã©quivalent) pour enregistrer tous les handlers du projet. MediatR utilise le container IoC pour rÃ©soudre les handlers. On peut configurer des comportements additionnels (les fameux Pipeline Behaviors mentionnÃ©s plus haut, qui permettent dâ€™implÃ©menter des cross-cutting concerns autour des requÃªtes Mediator, comme la validation, le logging, etc. â€“ câ€™est en fait un Chain of Responsibility autour du Mediator!).Avec MediatR, on peut envoyer des commandes synchrones ou asynchrones, des requÃªtes qui attendent une rÃ©ponse, ou des notifications sans rÃ©ponse (pub/sub interne). Câ€™est un outil formidable pour structurer du code mÃ©tier dans les applications de gestion, en appliquant les principes CQRS (sÃ©parer Ã©criture/lecture) et Clean Architecture. Ã€ noter : plusieurs implÃ©mentations du pattern Mediator existent dans lâ€™Ã©cosystÃ¨me .NET. Il est possible dâ€™en coder une version minimaliste maison, ou dâ€™utiliser des bibliothÃ¨ques comme MediatR, qui a longtemps Ã©tÃ© une rÃ©fÃ©rence. Toutefois, attention : MediatR qui sera prochainement sous licence commerciale, ce qui limite son usage dans les projets professionnels.BuilderÂ : construction pas-Ã -pas et syntaxe fluideLe Builder pattern sÃ©pare la construction dâ€™un objet complexe de sa reprÃ©sentation. Ici, le Director utilise un Builder (fluent) pour assembler un Product Ã©tape par Ã©tape.Dernier pattern mais non des moindresÂ : Builder. Si je vous dis Â«Â constructeur avec 12 paramÃ¨tresÂ Â», vous me dites ğŸ¤®. En effet, quand un objet possÃ¨de beaucoup de propriÃ©tÃ©s optionnelles, le passage de paramÃ¨tres devient illisible et sujet Ã  erreur. Le Builder vient Ã  la rescousse en fournissant une interface de construction progressive (souvent fluide). On crÃ©e lâ€™objet en plusieurs Ã©tapes, via des mÃ©thodes dÃ©diÃ©es, plutÃ´t quâ€™un seul gros constructeur. Câ€™est un peu le mode dâ€™emploi IkeaÂ : on assemble piÃ¨ce par piÃ¨ce, et Ã  la fin on obtient le meuble.Exemple concretDans une application de gestion, imaginons un module de gÃ©nÃ©ration de rapport PDF complexe (par exemple un rapport dâ€™activitÃ© mensuel avec plusieurs sections, en-tÃªte, pied de page, etc.). PlutÃ´t que dâ€™avoir une mÃ©thode gÃ©ante qui prend 20 arguments pour tout configurer, on va utiliser un ReportBuilder qui va fournir des mÃ©thodes pour ajouter les diffÃ©rentes parties du rapport de maniÃ¨re lisible.public class Report { public string Titre { get; set; } public List&lt;string&gt; Sections { get; set; } = []; public string PiedDePage { get; set; } // ... Ã©ventuellement d'autres propriÃ©tÃ©s complexes (graphique, tableau, etc.)}public class ReportBuilder{ private readonly Report _report = new Report(); public ReportBuilder Titre(string titre) { _report.Titre = titre; return this; // on retourne le builder pour chaÃ®ner } public ReportBuilder AjouterSection(string contenuSection) { _report.Sections.Add(contenuSection); return this; } public ReportBuilder PiedDePage(string textePied) { _report.PiedDePage = textePied; return this; } public Report Build() { return _report; }}Utilisation en syntaxe fluide (fluent interface)Â :Report rapport = new ReportBuilder() .Titre(\"Rapport dâ€™activitÃ© - Mars 2025\") .AjouterSection(\"Chiffre d'affaires : 1M â‚¬\") .AjouterSection(\"Nouveaux clients : 50\") .PiedDePage(\"Confidentiel - interne\") .Build();On lit quasiment du franÃ§ais dans le code ! On a progressivement construit lâ€™objet rapport sans se soucier de lâ€™ordre interne des initialisations ni dâ€™oublier un paramÃ¨tre requis. Le Builder encapsule la logique dâ€™assemblage (ici triviale, mais elle pourrait Ãªtre plus complexe avec par exemple calcul de totaux en interne avant Build).Quand lâ€™utiliser ğŸŸ¢ : Quand la crÃ©ation dâ€™un objet est complexe, câ€™est-Ã -dire : comporte de nombreux paramÃ¨tres (optionnels ou obligatoires) rendant un constructeur classique impraticable ou ambigu, nÃ©cessite des Ã©tapes multiples (par exemple certaines parties doivent Ãªtre construites avant dâ€™autres), ou quand on veut offrir Ã  lâ€™API utilisateur une syntaxe fluide trÃ¨s lisible pour configurer un objet.Les cas dâ€™utilisation concrets abondent en applications de gestionÂ : construction dâ€™un rapport comme vu, configuration dynamique dâ€™un objet de paramÃ©trage, montage dâ€™une requÃªte SQL/ElasticSearch complexe via un QueryBuilder, etc. Le Builder pattern permet aussi dâ€™isoler la logique de crÃ©ation en un point unique (respect du single responsibility).Ã€ Ã©viter ğŸ”´ : Si lâ€™objet Ã  crÃ©er est simple (quelques paramÃ¨tres), un constructeur ou les object initializers de C# (init { Prop1 = â€¦, Prop2 = â€¦ }) suffisent amplement. Le Builder, sâ€™il est mal conÃ§u, peut Ã©galement laisser lâ€™objet dans un Ã©tat partiel incohÃ©rent tant que Build() nâ€™a pas Ã©tÃ© appelÃ© â€“ attention donc Ã  garantir une utilisation correcte (parfois on marque le constructeur de lâ€™objet cible internal pour forcer le passage par le builder). Ã‰vitez aussi le builder juste pour faire du fluent sur des opÃ©rations statiques ou non liÃ©es Ã  un objet complexe â€” lÃ  câ€™est le nom qui peut prÃªter Ã  confusion : Builder sert vraiment Ã  construire un objet.IntÃ©gration .NET ğŸ‘©â€ğŸ’» : La syntaxe fluide (fluent syntax) est trÃ¨s rÃ©pandue dans lâ€™Ã©cosystÃ¨me .NET moderne, souvent inspirÃ©e du Builder pattern. Par exemple, les Options de configuration se construisent via des optionsBuilder.AddX() en chaÃ®ne, les requÃªtes LINQ en chaÃ®ne, lâ€™API Fluent Validation (RuleFor(x =&gt; x.Name).NotEmpty().WithMessage(\"...\")), etc. Sans Ãªtre toujours de purs â€œBuildersâ€, ces syntaxes fluides amÃ©liorent la lisibilitÃ© et lâ€™enchaÃ®nement dâ€™appels.En interne, .NET utilise aussi le Builder pattern pour la construction de gros objets comme lâ€™HostBuilder / WebApplicationBuilder (qui configureront Ã©tape par Ã©tape votre application).CÃ´tÃ© IoC, un Builder est souvent crÃ©Ã© via une factory ou fourni par un framework. On ne lâ€™enregistre gÃ©nÃ©ralement pas dans le container (on crÃ©e le builder quand on en a besoin, puis on jette). Toutefois, rien nâ€™empÃªche dâ€™injecter un builder prÃ©-configurÃ© si cela a du sens dans votre design.En rÃ©sumÃ©, le Builder est un peu lâ€™inverse du FactoryÂ : on lâ€™utilise pour composer petit Ã  petit un objet complexe, lÃ  oÃ¹ la Factory crÃ©e dâ€™emblÃ©e un objet souvent simple ou retourne une implÃ©mentation. Avec le Builder pattern, on prend le temps dâ€™assembler et grÃ¢ce au fluent interface, le code appelant est clair et expressif.ConclusionÂ : Choisir le bon pattern au bon momentNous avons fait un tour dâ€™horizon de plusieurs design patterns clÃ©s en .NET Core. Chaque pattern est un outil dans votre boÃ®te Ã  outilsÂ : il a un usage privilÃ©giÃ©, des avantages, des inconvÃ©nients. Lâ€™art de lâ€™architecture logicielle consiste Ã  choisir le bon Lego au bon moment. Il nâ€™y a pas de solution universelleÂ : parfois un simple if vaut mieux quâ€™un Strategy surdimensionnÃ©, parfois un Mediator apporte une structure bienvenue lÃ  oÃ¹ le couplage devenait ingÃ©rable.Un conseil : entraÃ®nez-vous Ã  reconnaÃ®tre dans votre code ou dans les frameworks que vous utilisez quels patterns sont Ã  lâ€™Å“uvre. Vous verrez que ASP.NET Core, Entity Framework, etc., sont truffÃ©s de ces concepts (Singletons pour les services, Factory methods pour les DbContext, Decorators dans les pipeline, etc.). Comprendre les design patterns vous permettra non seulement de mieux utiliser les API .NET, mais aussi de concevoir vos propres composants de maniÃ¨re Ã©lÃ©gante et maintenable.Enfin, pour aller plus loin, je le redis, la sÃ©rie de Christopher Okhravi sur YouTube est un excellent complÃ©ment visuel et pÃ©dagogique, avec une touche dâ€™humour qui, je lâ€™espÃ¨re, aura fait Ã©cho Ã  la lecture de cet article. ğŸ˜‰En maÃ®trisant ces patterns, vous Ã©viterez de rÃ©inventer la roue carrÃ©e et vous construirez des applications Ã©volutives brique par brique. Alors Ã  vos Legos, prÃªts, codezÂ ! ğŸš€" }, { "title": "Faut-il vraiment adopter une architecture microservices", "url": "/posts/quand-utiliser-architecture-microservices/", "categories": "architecture", "tags": "", "date": "2025-09-08 20:00:00 -0400", "snippet": "PrÃ©ambule â€œOn part en microservices ou on reste monolithique ?â€Câ€™est une des questions les plus posÃ©es et les moins bien tranchÃ©es du monde logiciel. Une question qui revient Ã  chaque projet un pe...", "content": "PrÃ©ambule â€œOn part en microservices ou on reste monolithique ?â€Câ€™est une des questions les plus posÃ©es et les moins bien tranchÃ©es du monde logiciel. Une question qui revient Ã  chaque projet un peu dâ€™envergure, et qui dÃ©clenche systÃ©matiquement les mÃªmes dÃ©bats passionnÃ©s entre collÃ¨gues, souvent avec des effets secondaires comme la hausse du dÃ©bit de voix et des soupirs lourds de sens.Certains brandissent lâ€™argument de la scalabilitÃ©, dâ€™autres invoquent le Graal du dÃ©ploiement indÃ©pendant. Puis quelquâ€™un Ã©voque Netflix ou Amazon, et Ã§a y est, tout le monde commence Ã  vouloir des centaines de services, un orchestrateur et une armÃ©e de pipelines CI/CD.Mais en mÃªme tempsâ€¦ on a tous vu (ou vÃ©cu) des projets oÃ¹ lâ€™adoption des microservices a amenÃ© plus de douleurs que de solutions. ComplexitÃ© opÃ©rationnelle, effet spaghetti distribuÃ©, Ã©quipe dÃ©passÃ©e par les appels rÃ©seau qui se perdent dans la brume. Bref, la promesse des microservices peut vite se transformer en cauchemarâ€¦ si on sâ€™est trompÃ© de combat.Alors voilÃ  : aujourdâ€™hui, on prend le temps de vraiment rÃ©pondre Ã  cette question. Sans dogme. Sans buzzword. En regardant froidement (mais gentiment ğŸ˜„) ce que Ã§a implique de faire â€” ou pas â€” du microservice dans un projet .NET Core. Lâ€™objectif ? Te donner une mÃ©thodologie claire, des exemples concrets, et surtout de quoi prendre une vraie bonne dÃ©cision pour ton prochain projet.Allez, on dÃ©balle tout Ã§a !Monolithe vs microservicesÂ : le duel en brefDans le coin gauche, lâ€™application monolithique : un seul bloc dÃ©ployable, rassemblant toutes les fonctionnalitÃ©s (base de donnÃ©es, logique mÃ©tier, UI) dans une mÃªme application. Câ€™est lâ€™architecture Â«Â tout-en-unÂ Â» classique. Ã€ droite, lâ€™architecture microservicesÂ : une constellation de petits services autonomes, chacun focalisÃ© sur une fonctionnalitÃ© mÃ©tier spÃ©cifique, qui communiquent entre eux via des APIs rÃ©seau. En somme, monolithe = une application unique, microservices = plein dâ€™applications collaborant ensemble.ğŸ§± Architecture monolithique â€“ toutes les fonctionnalitÃ©s (paiement, panier, inventaire, etc.) cohabitent dans une seule application dÃ©ployÃ©e.ğŸ§© Architecture microservices â€“ chaque fonctionnalitÃ© mÃ©tier est un service indÃ©pendant (paiement, panier, inventaireâ€¦), communiquant via des appels rÃ©seau. Un API Gateway (ou une interface unifiÃ©e) sert dâ€™entrÃ©e pour le client.ConcrÃ¨tement, dans un monolithe, un module peut appeler directement une fonction dâ€™un autre module en mÃ©moire, comme on passe dâ€™une piÃ¨ce Ã  lâ€™autre dans la mÃªme maison. Câ€™est simple et rapide (une bonne vieille fonction appelÃ©e directement). En microservices, ces appels deviennent des requÃªtes rÃ©seau (HTTP, gRPC, etc.), un peu comme passer des coups de fil entre maisons distinctesÂ : câ€™est plus lourd et Ã§a peut Ã©chouer pour tout un tas de raisons indÃ©pendantes du code mÃ©tier.ExempleÂ : imaginons un module de paiement qui doit vÃ©rifier une carte de crÃ©dit. En monolithe, un simple appel de mÃ©thode suffitÂ :// Appel interne en monolithebool estPaiementValide = _servicePaiement.ValiderCarte(numeroCarte);if (!estPaiementValide){ return \"Paiement refusÃ©\";}En microservices, le module paiement serait un service sÃ©parÃ©Â ; il faut alors faire un appel HTTP (ou autre)Â :// Appel distant en microservicesvar reponse = await _httpClient.GetAsync($\"http://service-paiement/api/verifierCarte/{numeroCarte}\");// gestion d'erreur...bool estPaiementValide = await reponse.Content.ReadFromJsonAsync&lt;bool&gt;();ğŸ‘† Vous voyez la diffÃ©renceÂ : on Ã©change la simplicitÃ© dâ€™un appel direct contre la complexitÃ© (gestion des erreurs rÃ©seau, sÃ©rialisation JSON, etc.) dâ€™un appel distant. Cet exemple illustre bien le surcoÃ»t inhÃ©rent aux microservices : ce qui Ã©tait un dÃ©tail trivial en monolithe (appeler une fonction) devient une mini-aventure technique quand on dÃ©coupe tout en services indÃ©pendants.Cela ne veut pas dire que monolithes = bien et microservices = mal (ou vice-versa). Chacun a ses avantages et inconvÃ©nients. Un monolithe, câ€™est simple Ã  dÃ©velopper et Ã  dÃ©bugger (tout est au mÃªme endroit) et dÃ©ployer se rÃ©sume Ã  lancer une â€œseule applicationâ€. En revanche, un monolithe peut devenir lourd Ã  faire Ã©voluer quand il grossit tropÂ : la moindre modification nÃ©cessite de redÃ©ployer lâ€™ensemble, et on ne peut pas passer Ã  lâ€™Ã©chelle un composant spÃ©cifique indÃ©pendamment des autres. Ã€ lâ€™opposÃ©, les microservices apportent de la flexibilitÃ©Â : chaque service est plus petit, modulaire, dÃ©ployable indÃ©pendamment, potentiellement scalable sÃ©parÃ©ment. Mais ils introduisent une tonne de complexitÃ© distribuÃ©e : appels rÃ©seau, gestion de la cohÃ©rence des donnÃ©es entre services, multiplication des projets, monitoring plus arduâ€¦ bref, ce nâ€™est pas la panacÃ©e universelle non plus.Maintenant que le dÃ©cor est plantÃ©, entrons dans le vif du sujetÂ : dans quels cas concrets les microservices sont-ils pertinents, et quand risquent-ils de vous causer plus de soucis quâ€™autre choseÂ ? Pour le savoir, suivez le guide en 6 Ã©tapes.MÃ©thodologieÂ : Faut-il partir sur une architecture microservicesÂ ?Avant de â€œdÃ©couper Ã  la scie mÃ©caniqueâ€ votre application en microservices, passez en revue les Ã©tapes suivantes. Elles forment une checklist pour Ã©valuer si le jeu en vaut la chandelle dans votre contexte spÃ©cifique.Ã‰tapeÂ 1Â : Ã‰valuer la taille et complexitÃ© du projetPremier rÃ©flexe : prenez du recul et regardez lâ€™ampleur du projet. Quelle est la taille de votre application et la richesse de son domaine mÃ©tierÂ ? Sâ€™agit-il dâ€™un petit site web ou dâ€™un outil interne avec trois formulaires, ou bien dâ€™une plateforme tentaculaire Ã  la Netflix/Amazon avec des dizaines de fonctionnalitÃ©s mÃ©tiers distinctesÂ ? Si votre projet est modeste ou en phase de dÃ©marrage, partir dâ€™emblÃ©e sur des microservices serait comme vouloir dÃ©sosser un mulot avec un scalpel de chirurgien. ğŸ­âš¡ En clair, câ€™est overkill. Il est souvent recommandÃ© de dÃ©buter par un monolithe bien structurÃ©, quitte Ã  le faire Ã©voluer plus tard si nÃ©cessaire. Martin Fowler note dâ€™ailleurs que presque toutes les success stories de microservices ont commencÃ© par un monolithe qui a grossi avant dâ€™Ãªtre dÃ©coupÃ©, tandis que les rares projets dÃ©marrÃ©s directement en microservices ont souvent accumulÃ© les ennuis. Le monolithe initial permet de valider rapidement que lâ€™application rÃ©pond Ã  un besoin, sans sâ€™alourdir dâ€™une complexitÃ© prÃ©maturÃ©e (principe YAGNI : You Ainâ€™t Gonna Need It). Il vaut mieux un petit systÃ¨me qui marche quâ€™une usine Ã  gaz microservicielle pour un produit incertain. En revanche, si vous anticipez que votre application va devenir trÃ¨s large, trÃ¨s complexe, avec de multiples sous-domaines mÃ©tier clairement identifiables, lÃ  un dÃ©coupage pourra se justifier Ã  terme. Par exemple, une plateforme de commerce Ã©lectronique internationale a des sous-domaines Ã©vidents (catalogue produits, gestion des commandes, facturation, recherche, recommandations, etc.) qui pourraient devenir chacun un service. Mais attention : mÃªme dans ce cas, rien ne presse de tout micro-dÃ©couper dÃ¨s le jour 1. Il est souvent plus sage de commencer monolithique, puis de refactorer en microservices une fois que les frontiÃ¨res naturelles entre composants se sont clarifiÃ©es dans le temps. Un mauvais dÃ©coupage prÃ©coce peut faire plus de mal que de bien. En rÃ©sumÃ©, taille modeste = monolithe favorisÃ©, grande Ã©chelle potentielle = microservices envisagÃ©s, mais idÃ©alement aprÃ¨s avoir atteint les limites du monolithe. Comme le dit de dicton : â€œnâ€™optimisez pas prÃ©maturÃ©mentâ€. Visez la simplicitÃ© dâ€™abord, la sophistication ensuite, seulement si nÃ©cessaire.Ã‰tapeÂ 2Â : ConsidÃ©rer la taille de lâ€™Ã©quipe et lâ€™organisationDeuxiÃ¨me facteur clÃ© : qui va dÃ©velopper et maintenir tout Ã§a ? La taille et la structure de votre Ã©quipe influencent Ã©normÃ©ment le choix dâ€™architecture : Si vous Ãªtes une toute petite Ã©quipe (ou un dÃ©veloppeur solo), lancer 10 microservices serait un peu comme un agriculteur qui court aprÃ¨s 10 vaches Ã©chappÃ©es dans des champs diffÃ©rents. âš ï¸ Spoiler : yâ€™en a toujours une qui finit chez le voisin ğŸ˜…. Vous risquez de vous Ã©puiser rapidement â€¦ Un monolithe est bien plus adaptÃ© aux petites Ã©quipesÂ : tout le monde travaille sur le mÃªme code, câ€™est plus facile Ã  suivre et Ã  tester. Nâ€™oublions pas que chaque microservice additionnel, câ€™est du fardeau opÃ©rationnel en plus (pipelines CI/CD multiples, dÃ©ploiements multiples, versions multiplesâ€¦). Quand on nâ€™a que 2-3 dÃ©veloppeurs, mieux vaut les concentrer sur une base de code unique que de les disperser. Ã€ lâ€™inverse, si vous disposez de plusieurs Ã©quipes dÃ©diÃ©es, avec chacune son pÃ©rimÃ¨tre fonctionnel, les microservices peuvent aider Ã  dÃ©coupler le travail. Câ€™est lÃ  quâ€™intervient la fameuse rÃ¨gle dâ€™Amazon des â€œtwo-pizza teamsâ€. Jeff Bezos a instaurÃ© que chaque Ã©quipe doit Ãªtre suffisamment petite pour Ãªtre nourrie avec deux pizzas. En pratique, chaque Ã©quipe chez Amazon est propriÃ©taire dâ€™un service et peut le faire Ã©voluer Ã  son rythme. Cette organisation en microservices a permis Ã  Amazon de scaler tant au niveau technique quâ€™humainÂ : des Ã©quipes autonomes, livrant indÃ©pendamment, sans se marcher sur les pieds. On retrouve une idÃ©e similaire chez Uber : quand lâ€™entreprise est passÃ©e de quelques dizaines Ã  des centaines de dÃ©veloppeurs, le monolithe dâ€™origine est devenu un goulot dâ€™Ã©tranglement, car toutes les Ã©quipes Ã©taient couplÃ©es par ce code unique. Le passage Ã  une multitude de services a permis Ã  chaque groupe dâ€™avancer plus librement, sans attendre que â€œle monolithe veuille bien dÃ©ployerâ€. En gros, conformez lâ€™architecture Ã  votre organisation (câ€™est la fameuse Conwayâ€™s Law). Si vous avez dÃ©jÃ  des Ã©quipes ou des domaines bien sÃ©parÃ©s, les microservices peuvent reflÃ©ter ce dÃ©coupage naturel. Si votre Ã©quipe est un bloc unique, imposer des microservices crÃ©e artificiellement des frontiÃ¨resâ€¦ et potentiellement des silos injustifiÃ©s. Un autre aspect humain : les compÃ©tences. Une petite Ã©quipe full-stack â€œclassiqueâ€ sera plus Ã  lâ€™aise Ã  travailler sur un seul projet monolithique. Tandis que dans une grande organisation, on peut avoir des Ã©quipes spÃ©cialisÃ©es (ex: une team pour chaque microservice, avec Ã©ventuellement des technos diffÃ©rentes). Pas dâ€™Ã©quipe dÃ©diÃ©e = pas de microservice dÃ©diÃ©, câ€™est un bon rÃ©flexe Ã  avoir.Petite anecdote : Amazon nâ€™est pas dogmatique non plus. RÃ©cemment, leur division Prime Video a opÃ©rÃ© un mouvement surprise en abandonnant une architecture microservices/serverless au profit dâ€™un bon gros monolitheâ€¦ RÃ©sultat : des coÃ»ts rÃ©duits et de bien meilleures performances pour leur workload! Preuve que mÃªme avec des centaines de dÃ©veloppeurs, la solution â€œplein de microservicesâ€ nâ€™est pas toujours la plus efficiente â€“ cela dÃ©pend du contexte et du problÃ¨me Ã  rÃ©soudre.Ã‰tapeÂ 3Â : Identifier les besoins de scalabilitÃ© et de performancePassons au cÃ´tÃ© technique : quelle charge votre application doit-elle encaisser et comment veut-on la faire monter en charge (scaling)Â ? Les microservices sont souvent vendus comme LE remÃ¨de anti-surcharge, mais la rÃ©alitÃ© est nuancÃ©e. Si votre application doit gÃ©rer des volumes massifs de trafic ou de donnÃ©es, et surtout de faÃ§on inÃ©gale selon les fonctionnalitÃ©s, alors une architecture microservices est pertinente. Elle permet de scaler indÃ©pendamment chaque service selon la demande. Par exemple, imaginons un jeu en ligne oÃ¹ le module â€œclassement des pointsâ€ est ultra-sollicitÃ©, bien plus que le module â€œprofil utilisateurâ€. En microservices, on pourrait dÃ©ployer 10 instances du service Classement pour chaque instance du service Profil, afin dâ€™absorber la charge lÃ  oÃ¹ câ€™est nÃ©cessaire. De grandes entreprises ont adoptÃ© ce principe : Netflix a scindÃ© sa plateforme en plus de 700 microservices pour gÃ©rer chaque partie du systÃ¨me de maniÃ¨re autonome, aprÃ¨s avoir souffert des limites dâ€™un monolithe. En 2008, un incident cÃ©lÃ¨bre a vu la base de donnÃ©es centrale de Netflix corrompue, plongeant tout le service dans le noir pendant trois jours. Cette panne a mis en lumiÃ¨re le point faible du monolithe : un seul composant qui flanche peut tout faire tomber. Netflix a alors migrÃ© vers AWS et une architecture microservices, Ã©liminant les points uniques de dÃ©faillance et permettant de faire Ã©voluer sÃ©parÃ©ment chaque brique (lecture de vidÃ©os, recommandations, facturation, etc.). Depuis, lâ€™infrastructure Netflix repose sur une multitude de petits services robustes, capables de servir des millions dâ€™utilisateurs et de dÃ©ployer des changements en continu â€“ parfois des milliers de dÃ©ploiements par jour ! De mÃªme, Amazon (encore eux) a besoin que certains pans de son site puissent encaisser des pics Ã©normes (pensez au Black Friday sur le panier dâ€™achat ou les paiements). Leur architecture microservices permet de renforcer uniquement les services critiques en pic de charge, sans toucher aux autres. Câ€™est comme pouvoir ajouter des voies sur lâ€™autoroute lÃ  oÃ¹ il y a des embouteillages, sans avoir Ã  refaire toutes les routes du pays. En revanche, si votre application nâ€™a pas le besoin dâ€™une scalabilitÃ© granulaire, un monolithe bien conÃ§u peut suffire largement. Beaucoup dâ€™applications mÃ©tier â€œclassiquesâ€ tournent trÃ¨s bien avec un dÃ©ploiement monolithique dupliquÃ© sur quelques serveurs en load-balancing pour gÃ©rer la charge. MÃªme un gros monolithe peut scaler horizontalement en dÃ©ployant plusieurs instances identiques derriÃ¨re un rÃ©partiteur de charge. On sous-estime parfois jusquâ€™oÃ¹ un monolithe peut aller : vous pouvez dÃ©jÃ  bÃ¢tir un commerce qui tourne rondement avec une seule base de donnÃ©es et une appclication web bien optimisÃ©e sur un serveur aux ressources gÃ©nÃ©reuses. Si une partie de votre application commence Ã  saturer les ressources, il est tout Ã  fait possible de lâ€™optimiser ou de monter en vertical (plus de CPU, de RAM) avant dâ€™envisager un dÃ©coupage. Par ailleurs, microservices nâ€™Ã©gale pas automatiquement meilleures performances. En fait, chaque appel distant ajoute de la latence et de la charge (sÃ©rialisation/dÃ©sÃ©rialisation, routage rÃ©seauâ€¦). Donc, pour de fortes exigences de performance en temps rÃ©el, trop de microservices peuvent nuire. Il faut un certain seuil de complexitÃ©/Ã©chelle pour que les bÃ©nÃ©fices surpassent les coÃ»ts. Lâ€™anecdote dâ€™Amazon Prime Video lâ€™illustre bien : ils avaient dÃ©coupÃ© un workflow en fonctions serverless (microservices), pensant gagner en scaling, mais ont atteint des limites Ã  seulement 5% de charge prÃ©vue. Leur solution a Ã©tÃ© de re-fusionner les composants critiques en un seul service optimisÃ© et bingo, Ã§a a bien mieux tenu la charge. Morale : si vos besoins de scalabilitÃ© peuvent Ãªtre satisfaits par un bon vieux monolithe optimisÃ©, inutile dâ€™ajouter de la complexitÃ© sans justification concrÃ¨t. En rÃ©sumÃ©, posez-vous les questions suivantes : Est-ce que certaines parties du systÃ¨me ont des profils de charge trÃ¨s diffÃ©rents des autresÂ ? Dois-je pouvoir scaler un module sans toucher aux autresÂ ? Ai-je des contraintes de disponibilitÃ© trÃ¨s fines qui nÃ©cessitent dâ€™isoler les pannes potentiellesÂ ? Si oui, orientez-vous vers une segmentation en services. Sinon, un monolithe peut trÃ¨s bien faire le boulot, plus simplement.Ã‰tapeÂ 4Â : Examiner la frÃ©quence de dÃ©ploiement et lâ€™indÃ©pendance des fonctionnalitÃ©sUn avantage souvent mis en avant des microservices, câ€™est de permettre des dÃ©ploiements indÃ©pendants et frÃ©quents de chaque composant. Voyons si câ€™est pertinent pour vous : Votre Ã©quipe dÃ©ploie-t-elle trÃ¨s souvent de nouvelles fonctionnalitÃ©s, de maniÃ¨re dÃ©couplÃ©eÂ ? Par exemple, si le module â€œFacturationâ€ doit Ãªtre mis en production 3 fois par semaine, alors que le reste du systÃ¨me bouge peu, lâ€™architecture microservices vous permettrait de dÃ©ployer le service Facturation seul, sans interrompre ni revalider toute lâ€™application. Idem, sâ€™il y a une Ã©quipe dÃ©diÃ©e qui ne travaille que sur le moteur de recherche du site, elle pourrait livrer ses Ã©volutions indÃ©pendamment des autres. Ce scÃ©nario plaide en faveur de microservices, pour gagner en vitesse de livraison. Les gÃ©ants du web en profitent : Amazon a des dÃ©ploiements en continu de microservices tout au long de la journÃ©e, sans quoi il leur serait impossible de faire Ã©voluer leur plateforme tentaculaire sans tout casser. Chaque Ã©quipe publie son service quand elle est prÃªte, point. Cela a considÃ©rablement accÃ©lÃ©rÃ© lâ€™innovation et le time-to-market. DÃ©pendances modulaires : Si vos fonctionnalitÃ©s sont relativement indÃ©pendantes les unes des autres sur le plan mÃ©tier, les microservices Ã©vitent que dÃ©ployer A implique de retester B, C et D qui nâ€™ont rien Ã  voir. Câ€™est un atout pour la qualitÃ© et lâ€™agilitÃ©. Par exemple, chez Netflix, les Ã©quipes peuvent modifier le service de recommandation de films et le dÃ©ployer, sans devoir geler le service de lecture vidÃ©o ou le service dâ€™abonnement. Dans un monolithe, une petite modification dans le code de recommandations nÃ©cessiterait de reconstruire et redÃ©ployer tout le monolithe, avec les risques que cela comporte. Ã€ lâ€™inverse, si vous dÃ©ployez plutÃ´t rarement et que vos releases englobent de toute faÃ§on toutes les parties de lâ€™application en mÃªme temps (train de livraison coordonnÃ©), le bÃ©nÃ©fice de microservices diminue. Beaucoup de systÃ¨mes internes dâ€™entreprise suivent des cycles de mise en production globales (par ex. une release mensuelle de lâ€™application entiÃ¨re). Le fait dâ€™Ãªtre en microservices ne changerait pas grand-chose, puisque vous attendriez quand mÃªme dâ€™avoir tout packagÃ© pour dÃ©ployer. Voire pire : cela pourrait compliquer la synchronisation (sâ€™assurer que les 10 services sont tous alignÃ©s pour la release, avec les bonnes versions dâ€™API, etc.). Dans ce genre de contexte, un monolithe peut simplifier la livraison. Posez-vous la question de la coordination des versions : si chaque microservice Ã©volue indÃ©pendamment, il faut gÃ©rer la compatibilitÃ© entre eux. Câ€™est jouable (contrats dâ€™API stables, versions rÃ©trocompatiblesâ€¦), mais câ€™est du travail de plus. Si votre Ã©quipe a dÃ©jÃ  du mal Ã  coordonner du versioning au sein dâ€™un monolithe, la multiplication des services ne va pas arranger les choses par magie, au contraire. En somme, les microservices prennent tout leur sens si vous visez un modÃ¨le DevOps/CI-CD trÃ¨s poussÃ©, avec des livraisons continues par composant. Vous pourrez publier plus vite, en isolant les risques. Si ce niveau de frÃ©quence nâ€™est ni nÃ©cessaire ni rÃ©aliste pour vous, ne vous infligez pas la complexitÃ© dâ€™une architecture distribuÃ©e. Parfois, dÃ©ployer calmement un bon gros monolithe une fois toutes les deux semaines, Ã§a suffit amplement pour rendre les utilisateurs heureux.Ã‰tapeÂ 5Â : VÃ©rifier les capacitÃ©s techniques (DevOps, monitoring, etc.)Câ€™est un aspect souvent sous-estimÃ© : avez-vous lâ€™infrastructure et les outils pour supporter une galaxie de microservicesÂ ? Et lâ€™expertise qui va avecÂ ?Mettre en place des microservices, ce nâ€™est pas juste dÃ©couper du code en petits morceaux. Il faut aussi tout un Ã©cosystÃ¨me technique pour les faire tourner correctementÂ : Conteneurs, orchestrateurs : En gÃ©nÃ©ral, qui dit microservices dit Docker, Kubernetes, ou autre plateforme orchestrÃ©e. DÃ©ployer manuellement 20 services sur des VM, câ€™est ingÃ©rable, vous aurez besoin dâ€™automatisation. Votre Ã©quipe est-elle familiÃ¨re avec ces technologiesÂ ? A-t-elle quelquâ€™un qui maÃ®trise Kubernetes (AKS, EKS, peu importe) pour opÃ©rer lâ€™infrastructureÂ ? Si ce nâ€™est pas le cas, prÃ©voyez une solide courbe dâ€™apprentissage et du temps en â€œRecherche et DÃ©veloppementâ€, sinon vous allez vous retrouver Ã  hÃ©berger vos microservices â€œÃ  la mainâ€, ce qui est aventureux ğŸ˜…. Monitoring &amp; logging : Un monolithe a souvent une seule source de logs, facile Ã  parcourir. Avec 10 microservices, bonjour la chasse aux logs dans 10 endroits diffÃ©rents. Il vous faudra mettre en place une solution de logs centralisÃ©s (ELK, Application Insights, etc.) et du monitoring distribuÃ©. Idem pour le tracing des requÃªtes entre services (systÃ¨mes de traÃ§age distribuÃ© de type OpenTelemetry). Sans ces outils, vous serez aveugle lorsque quelque chose plantera Ã  3h du matin dans un enchaÃ®nement de 5 services. Il faut donc investir du temps Ã  instrumenter chaque service, Ã  se doter de dashboards, dâ€™alertesâ€¦ Est-ce que votre Ã©quipe a les compÃ©tences pour Ã§aÂ ? Et le tempsÂ ? Si non, câ€™est un signal fort que le passage aux microservices pourrait Ãªtre prÃ©maturÃ©. Robustesse et tolÃ©rance aux pannes : Dans un systÃ¨me distribuÃ©, tout peut arriver : un service peut tomber, un appel rÃ©seau peut Ã©chouer ou expirer, etc. Vos dÃ©veloppeurs doivent adopter des pratiques de code rÃ©silient (patterns de retry, circuit-breaker â€“ avec Polly en .NET par exemple). Ils doivent penser â€œet si le service en face ne rÃ©pond pasÂ ?â€ en permanence. Câ€™est un Ã©tat dâ€™esprit et une expertise diffÃ©rente de la programmation monolithique oÃ¹ un NullReferenceException est souvent le pire scÃ©nario. Ici on parle de gÃ©rer des time out, des files dâ€™attente, des messages perdusâ€¦ Si lâ€™Ã©quipe nâ€™a jamais fait Ã§a, il y aura des ratÃ©s. Ce nâ€™est pas insurmontable, mais câ€™est un coÃ»t de formation et dâ€™expÃ©rience Ã  anticiper. Netflix, par exemple, a dÃ» inventer lâ€™outil Chaos Monkey qui Ã©teint alÃ©atoirement des services en production pour sâ€™assurer que le systÃ¨me global survit aux pannes individuelles â€“ câ€™est dire le niveau de maturitÃ© Ã  atteindre pour dompter un tel Ã©cosystÃ¨me ! ComplexitÃ© de debug : PrÃ©parez-vous Ã  ce que votre dÃ©bogage â€œF5 dans VisualÂ Studioâ€ se transforme en parties de cache-cache dans une ferme de services. Un dÃ©veloppeur racontait, un brin dÃ©sabusÃ©, comment ses microservices passaient leur temps Ã  â€œÃ©changer entre eux sur le rÃ©seauâ€, et que dÃ¨s quâ€™un service â€œfaisait un capriceâ€, bonne chance pour identifier la source du problÃ¨me. Ce qui Ã©tait un simple appel de fonction devient une succession de requÃªtes asynchrones Ã  travers le rÃ©seau, avec des erreurs possiblement silencieuses en chemin. Debugger Ã§a sans outillage, câ€™est comme chercher une aiguille dans un champ de foinâ€¦ la nuitâ€¦ avec une lampe frontale dÃ©chargÃ©e. ğŸ”¦ğŸ˜… Autrement dit, assurez-vous dâ€™avoir les outils et les compÃ©tences de dÃ©bogage distribuÃ©, sinon vos nuits pourraient devenir trÃ¨s courtes. En rÃ©sumÃ©, une architecture microservices nâ€™est viable que si votre stack technique et votre Ã©quipe sont prÃªtes Ã  en assumer les Ã -cÃ´tÃ©s. Posez-vous sincÃ¨rement la question : Mon Ã©quipe est-elle suffisamment DevOpsÂ ? Avons-nous les ressources pour mettre en place : CI/CD, conteneurs, monitoring, tests end-to-end sur un systÃ¨me distribuÃ©Â ? Si la rÃ©ponse est â€œnahhhhhhâ€, il vaut peut-Ãªtre mieux consolider vos pratiques sur un monolithe dâ€™abord. Rien nâ€™empÃªche dâ€™adopter certaines bonnes pratiques devops (intÃ©gration continue, conteneurisation) sur le monolithe en prÃ©vision, mais ne pas se jeter dans le grand bain â€œmulti-servicesâ€ avant de savoir nager.Ã‰tapeÂ 6Â : Peser les coÃ»ts et la valeur ajoutÃ©eDernier point, et non des moindresÂ : le coÃ»t et le retour sur investissement. Les microservices ont un coÃ»t technique et organisationnel. Il faut sâ€™assurer que les bÃ©nÃ©fices espÃ©rÃ©s valent cette dÃ©pense. CoÃ»t de dÃ©veloppement : Diviser une application en 10 services, câ€™est potentiellement 10 dÃ©pÃ´ts git, 10 pipelines de build, 10 projets Ã  tester et maintenir. La productivitÃ© peut en prendre un coup. Un architecte rapportait que dans son entreprise, dÃ©velopper une fonctionnalitÃ© Ã©quivalente prenait 8 fois plus de temps avec une architecture microservices quâ€™avec un monolithe, et nÃ©cessitait deux fois plus de dÃ©veloppeurs! Pourquoi ? Parce quâ€™il faut dÃ©finir des contrats dâ€™API, gÃ©rer les versions, Ã©crire du code de communication, synchroniser les dÃ©ploiementsâ€¦ tout cela, un monolithe nous lâ€™Ã©pargne en grande partie. Alors oui, 8x câ€™est un cas extrÃªme, mais ne soyez pas surpris si au dÃ©but vos livraisons ralentissent en passant aux microservices. Câ€™est un investissement long terme : on accepte de ralentir maintenant en espÃ©rant aller plus vite plus tard, quand lâ€™application sera trÃ¨s grosse et que les Ã©quipes parallÃ©liseront vraiment le travail. Pour un petit projet, cet investissement ne sera jamais rentabilisÃ©. CoÃ»t opÃ©rationnel : Plus de services = plus de ressources serveurs (mÃ©moire, CPU, instances multiples). Si chaque microservice a sa base de donnÃ©es sÃ©parÃ©e (idÃ©alement oui), Ã§a fait autant de serveurs de BD Ã  gÃ©rer/payer/licencier. Sur le cloud, multiplier les conteneurs ou fonctions serverless peut faire grimper la facture Ã  la fin du mois. LÃ  oÃ¹ un monolithe tournait sur 2 VM Ã  80% CPU, vos 10 microservices tournent peut-Ãªtre sur 20 conteneurs globalement sous-utilisÃ©sâ€¦ mais facturÃ©s quand mÃªme. Avez-vous le budget pour ce overheadÂ ? Parfois, le jeu en vaut la chandelle (si vos microservices attirent 100Ã— plus dâ€™utilisateurs, le coÃ»t supplÃ©mentaire est justifiÃ©). Parfois non : on a vu des startups flamber leur budget infonuagique en orchestrant une flotte de microservices qui auraient pu tenir dans un seul petit serveur. ComplexitÃ© = risques : La complexitÃ© additionnelle peut engendrer plus de pannes ou de bugs subtils. Chaque service ajoutÃ© est un point potentiel de dÃ©faillance en plus (une panne rÃ©seau, un plantage de service, ou un Ã©chec de synchronisation des donnÃ©esâ€¦). Et quand Ã§a plante, le temps passÃ© Ã  rÃ©soudre (MTTR) risque dâ€™Ãªtre plus long quâ€™avec un monolithe. Ce coÃ»t-lÃ  est dur Ã  chiffrer, mais bien rÃ©el (heures supplÃ©mentaires, interruptions de service, image de marque, etc.). Pour le minimiser, il faut investir dans la fiabilitÃ© (tests, monitoring, redondance) ce qui renchÃ©rit encore le coÃ»t technique initial. ROI mÃ©tier : demandez-vous quels problÃ¨mes concrets les microservices vont rÃ©soudre pour votre business. Si la rÃ©ponse est du genre â€œcâ€™est Ã  la modeâ€ ou â€œon le fait pour faire propreâ€, ce nâ€™est pas un vrai ROI ğŸ˜…. En revanche, si vous identifiez clairement que â€œÃ§a nous permettra de dÃ©ployer plus vite des features pour nos clientsâ€ ou â€œÃ§a Ã©liminera les indisponibilitÃ©s totales en cas de bugâ€ ou â€œon pourra tenir la charge pendant les soldes sans planterâ€, lÃ  on parle. Quantifiez autant que possible : par exemple, passer de 1 dÃ©ploiement par mois Ã  10 dÃ©ploiements par jour grÃ¢ce aux microservices a une valeur Ã©norme si votre marchÃ© exige cette rÃ©activitÃ©. Mais si votre application interne nâ€™a pas besoin dâ€™Ã©voluer si frÃ©quemment, ce gain est inutile. En somme, faites le bilan coÃ»t/bÃ©nÃ©fice le plus objectivement possible. Les microservices apportent des bÃ©nÃ©fices indÃ©niables (Ã©chelle, rÃ©silience, agilitÃ©) mais au prix dâ€™une complexitÃ© qui, dans bien des cas, nâ€™est tout simplement pas justifiÃ©e. Il nâ€™y a pas de honte Ã  rester sur un bon monolithe efficace si câ€™est la solution la plus rentable pour vous ! Dâ€™ailleurs, on voit un certain retour de balancier dans lâ€™industrie : aprÃ¨s lâ€™engouement dÃ©bridÃ© pour â€œtout microserviciserâ€, certaines entreprises reviennent Ã  des architectures plus simples pour rÃ©duire les coÃ»ts et la complexitÃ©, quitte Ã  sacrifier un peu de hype. Le tout, câ€™est dâ€™y gagner au final.ConclusionÂ : Trouver le juste milieu (et dormir sur ses deux oreilles)La grande leÃ§on Ã  retenirÂ ? â€œMicroserviceâ€ nâ€™est pas synonyme de â€œmieuxâ€ par dÃ©faut. Câ€™est un outil architectural parmi dâ€™autres, avec ses cas dâ€™usage idÃ©aux et ses piÃ¨ges. De mÃªme, le monolithe nâ€™est pas un dinosaure ringard bon pour le musÃ©e : il demeure tout Ã  fait pertinent dans bon nombre de situations.Pour rÃ©capituler avec un petit guide quand ou ne pas utiliser une architecture microservicesÂ : âœ… Oui aux microservices dans des contextes tels que : Une application trÃ¨s complexe, couvrant plusieurs domaines fonctionnels clairement sÃ©parables, et/ou une grande organisation avec des Ã©quipes dÃ©diÃ©es par domaine. Ex: Un site web de type Amazon avec panier, paiement, recommandations, chacun Ã©voluant sÃ©parÃ©ment par des Ã©quipes diffÃ©rentes. Des besoins de scalabilitÃ© fine : certaines parties du systÃ¨me doivent pouvoir monter en charge indÃ©pendamment (ex: moteur de recherche Ã  scaler sans scaler toute lâ€™application). Une nÃ©cessitÃ© de dÃ©ploiements frÃ©quents et indÃ©pendants de certaines fonctionnalitÃ©s, pour livrer plus vite aux utilisateurs. Une exigence de haute rÃ©silience : on veut quâ€™une panne dâ€™un composant nâ€™impacte pas tout le reste (architecture tolÃ©rante aux pannes). Un environnement technique mature en DevOps, avec outils en place (CI/CD, container orchestration, monitoring distribuÃ©) et une Ã©quipe Ã  lâ€™aise avec ces concepts. En somme, quand le gain attendu (autonomie, vitesse, robustesse, Ã©chelle) dÃ©passe le coÃ»t en complexitÃ©. ğŸš« Non (ou pas encore) aux microservices dans les cas suivants : Projet trop petit ou jeune : si vous pouvez dÃ©velopper lâ€™ensemble de votre application en quelques mois avec 2-3 devs, un microservice ne va que ralentir la livraison. Partez monolithique, vous verrez plus tard si Ã§a devient trop gros. Ã‰quipe rÃ©duite/inexpÃ©rimentÃ©e en devops : pas dâ€™expertise conteneurs/Cloud, pas dâ€™Ã©quipe dÃ©diÃ©e Ã  lâ€™infrastructureâ€¦ Mieux vaut ne pas se compliquer la vie tout de suite. Comme on dit, â€œil ne faut pas plusieurs chefs pour faire bouillir la marmiteâ€ â€“ une petite brigade de dÃ©veloppeurs fonctionne mieux autour dâ€™un seul chaudron (le monolithe). Domaines trÃ¨s interconnectÃ©s : si vos fonctionnalitÃ©s sont toutes fortement liÃ©es, les sÃ©parer en services entraÃ®nera beaucoup dâ€™appels rÃ©seau entre eux (le syndrome du distributed monolith). Vous nâ€™y gagnerez rien, si ce nâ€™est dâ€™Ã©changer un couplage interne contre un couplage rÃ©seau tout aussi contraignant. Contraintes de performance temps rÃ©el strictes : une application low-latency (ex : transactions boursiÃ¨res automatisÃ©es ultra-rapides, calcul scientifique synchronisÃ©) peut difficilement tolÃ©rer la latence ajoutÃ©e des microservices. Un monolithe optimisÃ© en C# (ou en Rust natif ğŸ˜‰) sera plus efficace. Budget serrÃ© : si chaque dollar compte, sachez que la complexitÃ© microservices peut impliquer plus de dÃ©penses (machines, temps de dÃ©veloppeur, consultants spÃ©cialisÃ©sâ€¦). Assurez-vous que votre responsable budgÃ©taire soit dâ€™accord avant de multiplier les dÃ©ploiements comme des lapins. En bref, si les bÃ©nÃ©fices ne sont pas clairs et immÃ©diats, ou si votre contexte nâ€™est pas prÃªt, il est parfaitement raisonnable de rester sur un monolithe ou dâ€™adopter une approche intermÃ©diaire (par ex. un monolithe modulaire, bien dÃ©coupÃ© en couches ou en modules internes â€“ parfois appelÃ© â€œmodulitheâ€ ou â€œmonolithique modulableâ€). On peut trÃ¨s bien concevoir son code comme des microservices (avec de bonnes sÃ©parations logiques), tout en dÃ©ployant une seule application. Câ€™est souvent un excellent compromis pour dÃ©marrer. En guise de clÃ´ture, rappelons-nous que lâ€™architecture sert les objectifs du logiciel, pas lâ€™inverse. Ne choisissez pas microservices ou monolithe pour suivre la mode, mais en fonction de ce qui apporte le plus de valeur Ã  votre produit et vos utilisateurs. La prochaine fois quâ€™on vous dira â€œIl faut absolument des microservices, câ€™est plus scalable et hypeâ€œ, vous pourrez rÃ©torquer : â€œPeut-Ãªtre, mais encore faut-il que ce soit justifiÃ© â€“ parlons concret ! â€œ.Au fond, monolithes et microservices cohabitent dans la grande boÃ®te Ã  outils de lâ€™architecte. Le vrai talent est de sortir le bon outil au bon moment. Ce nâ€™est pas parce quâ€™on a vu des fermes industrielles avec des robots Ã  traire quâ€™il faut transformer sa petite Ã©table en usine Ã  microservices. Et Ã  lâ€™inverse, pour bÃ¢tir une cathÃ©drale logicielle frÃ©quentÃ©e par des millions de personnes, un seul bloc monolithique pourrait devenir un carcan rigide â€“ un Ã©chafaudage microservices offre alors plus de souplesse.En dÃ©finitive, soyez pragmatique : commencez simple, ayez conscience des trade-offs, et Ã©voluez lâ€™architecture quand les signaux le demandent (goulots dâ€™Ã©tranglement, blocage organisationnel, etc.). Si vous suivez les Ã©tapes et conseils ci-dessus, vous devriez Ã©viter aussi bien le â€œmicroservice washingâ€ inconsidÃ©rÃ© que le â€œmonolith forever par inertieâ€.Et surtout, nâ€™oubliez pas de savourer le voyage : que vous construisiez un majestueux monolithe ou une myriade de microservices, lâ€™essentiel est dâ€™apprendre en sâ€™amusant. AprÃ¨s tout, on Ã©crit du code pour rÃ©soudre des problÃ¨mes et pour le plaisir intellectuel. Alors faites les bons choix au bon moment, et laissez lâ€™architecture servir votre projet, pas lâ€™inverse." }, { "title": "Architecture Vertical Slice dans lâ€™Ã©cosystÃ¨me .NET", "url": "/posts/vertical-slice-architecture/", "categories": "architecture", "tags": "", "date": "2025-06-29 20:00:00 -0400", "snippet": "Lâ€™architecture Vertical Slice (ou architecture en tranches verticales) est une approche de conception logicielle qui consiste Ã  organiser le code par fonctionnalitÃ©s verticales plutÃ´t que par couch...", "content": "Lâ€™architecture Vertical Slice (ou architecture en tranches verticales) est une approche de conception logicielle qui consiste Ã  organiser le code par fonctionnalitÃ©s verticales plutÃ´t que par couches techniques. Autrement dit, chaque fonctionnalitÃ© de lâ€™application est implÃ©mentÃ©e de bout en bout dans une tranche (un slice) comprenant tous les aspects nÃ©cessaires : interface utilisateur, logique mÃ©tier et accÃ¨s aux donnÃ©es. Chaque tranche verticale est autonome et correspond Ã  un cas dâ€™utilisation distinct, ce qui favorise un dÃ©veloppement modulaire centrÃ© sur les fonctionnalitÃ©s. En .NET, cette approche sâ€™accompagne souvent du patron CQRS (Command Query Responsibility Segregation) : on sÃ©pare les opÃ©rations de lecture (Query) de celles dâ€™Ã©criture (Command) et on les traite diffÃ©remment. La bibliothÃ¨que MediatR est frÃ©quemment utilisÃ©e pour mettre en Å“uvre ce style â€“ elle agit comme un mÃ©diateur qui envoie chaque commande ou requÃªte vers son gestionnaire (handler) dÃ©diÃ©, permettant de dÃ©coupler lâ€™Ã©metteur de la requÃªte de son traitement.Avantages de lâ€™architecture Vertical Slice LisibilitÃ© et cohÃ©sion par fonctionnalitÃ© : Le code de chaque fonctionnalitÃ© est regroupÃ© au mÃªme endroit, ce qui le rend plus facile Ã  comprendre et Ã  maintenir quâ€™une base de code oÃ¹ les Ã©lÃ©ments dâ€™une mÃªme fonctionnalitÃ© sont dispersÃ©s entre plusieurs couches. Cette forte cohÃ©sion interne Ã  la tranche amÃ©liore la lisibilitÃ© : on peut se concentrer sur un use case Ã  la fois sans Ãªtre noyÃ© dans la complexitÃ© globale. DÃ©couplage des fonctionnalitÃ©s : Chaque tranche est indÃ©pendante des autres, ce qui rÃ©duit le couplage du systÃ¨me. Modifier ou ajouter une fonctionnalitÃ© impacte peu (voire pas du tout) les autres modules, minimisant les effets de bord. On obtient ainsi un systÃ¨me plus robuste face aux changements, chaque slice pouvant Ã©voluer isolÃ©ment. FacilitÃ© de test : Puisque la logique de chaque use case est isolÃ©e, on peut tester une tranche verticalement sans dÃ©pendre du reste de lâ€™application. Par exemple, il est simple dâ€™Ã©crire des tests unitaires pour un handler donnÃ© en simulant ses dÃ©pendances (bases de donnÃ©es, services externes, etc.), ce qui amÃ©liore la testabilitÃ© globale. DÃ©veloppement parallÃ¨le accÃ©lÃ©rÃ© : Les Ã©quipes peuvent travailler simultanÃ©ment sur diffÃ©rentes fonctionnalitÃ©s sans se marcher sur les pieds, Ã©tant donnÃ© que chaque slice est autonome. Ceci rÃ©duit Ã©galement les risques de conflits de merge et facilite lâ€™intÃ©gration du code produit par plusieurs dÃ©veloppeurs en parallÃ¨le. En grande Ã©quipe, segmenter le travail par vertical slices permet dâ€™accÃ©lÃ©rer le dÃ©veloppement en divisant le systÃ¨me en sous-problÃ¨mes indÃ©pendants. Ajout de fonctionnalitÃ©s simplifiÃ© : Lâ€™architecture Vertical Slice tend Ã  rendre lâ€™application extensible par addition plutÃ´t que par modification. Lâ€™ajout dâ€™une nouvelle fonctionnalitÃ© se traduit par la crÃ©ation dâ€™une nouvelle tranche (nouvelle requÃªte, nouveau handler, etc.) sans modifier du code existant, ce qui limite le risque de rÃ©gression. Cette propriÃ©tÃ© sâ€™aligne bien avec le principe Open/Closed â€“ le code en place est fermÃ© aux modifications intempestives, tandis que le systÃ¨me est ouvert Ã  de nouvelles extensions fonctionnelles. FlexibilitÃ© dâ€™implÃ©mentation : Chaque tranche peut adopter lâ€™approche technique la plus adaptÃ©e Ã  son cas sans impacter le reste du projet. Par exemple, une fonctionnalitÃ© trÃ¨s simple pourra se contenter dâ€™un traitement procÃ©dural direct (transaction script), tandis quâ€™une autre plus complexe pourrait utiliser des patterns de domaine riche â€“ le choix peut se faire au niveau de la slice elle-mÃªme. De mÃªme, chaque tranche pourrait interagir avec des ressources externes diffÃ©remment (base de donnÃ©es SQL, appel dâ€™un service tiers, etc.) sans casser une â€œarchitecture globaleâ€ figÃ©e. Cette souplesse permet de personnaliser la solution par fonctionnalitÃ© et dâ€™Ã©viter les abstractions ou couches supplÃ©mentaires inutiles au sein dâ€™une slice donnÃ©e.InconvÃ©nients et limites de lâ€™architecture Vertical Slice Duplication de code (anti-DRY)Â : Ã€ force dâ€™isoler chaque fonctionnalitÃ©, on risque de rÃ©pÃ©ter du code commun dans plusieurs slices. Par exemple, des composants dâ€™infrastructure ou des routines similaires peuvent se retrouver dupliquÃ©s dans chaque module fonctionnel. Cette redondance va Ã  lâ€™encontre du principe DRY (Donâ€™t Repeat Yourself) et peut alourdir la maintenance : une correction de bug ou un changement commun doit Ãªtre rÃ©percutÃ© Ã  plusieurs endroits. En adoptant Vertical Slice, on accepte implicitement une certaine duplication en Ã©change dâ€™une meilleure isolation, mais il faut surveiller que cela ne devienne pas ingÃ©rable (dâ€™oÃ¹ le commentaire humoristique Â« Adieu DRY ! Â» souvent Ã©voquÃ© sur le sujet). Fragmentation et vision dâ€™ensemble rÃ©duiteÂ : Un risque de cette approche est de morceler lâ€™application en de nombreux petits silos de code. Si la modularitÃ© est poussÃ©e Ã  lâ€™extrÃªme sans garde-fou, on peut perdre la vue dâ€™ensemble du systÃ¨me. Chaque Ã©quipe ou dÃ©veloppeur pouvant structurer â€œsaâ€ tranche Ã  sa maniÃ¨re, lâ€™architecture globale peut manquer de cohÃ©rence. En lâ€™absence de conventions communes, le code peut diverger dâ€™une tranche Ã  lâ€™autre, rendant la maintenance transversale plus difficile. En outre, il devient dÃ©licat de comprendre le flux applicatif global puisque la logique est Ã©clatÃ©e en morceaux indÃ©pendants. Il est donc crucial de dÃ©finir des standards (structuration interne des slices, conventions de nommage, etc.) pour conserver une certaine homogÃ©nÃ©itÃ©. ComplexitÃ© de gestion des interactions : Lâ€™architecture Vertical Slice fonctionne mieux lorsque les fonctionnalitÃ©s sont bien dÃ©couplÃ©es. Si, en pratique, vos cas dâ€™utilisation ont de fortes interactions ou partagent des processus communs, la gestion peut devenir compliquÃ©e. Par exemple, une transaction mÃ©tier impliquant plusieurs slices distinctes sâ€™avÃ¨re difficile Ã  orchestrer proprement avec ce modÃ¨le. On devra alors multiplier les contournements (appels de slice A vers slice B, utilisation de librairies tierces pour orchestrer du workflow, etc.), ce qui peut complexifier le code et le rendre inmaintenable. En dâ€™autres termes, Vertical Slice nâ€™est pas pensÃ©e pour des fonctionnalitÃ©s Ã©troitement liÃ©es ou fortement interdÃ©pendantes, et forcer ce modÃ¨le dans de tels cas peut conduire Ã  une usine Ã  gaz. Performances et logique transversaleÂ : Dans la mÃªme veine, si une opÃ©ration utilisateur nÃ©cessite de coordonner plusieurs tranches verticales, lâ€™application peut souffrir dâ€™une certaine surcouche. Par exemple, devoir appeler successivement plusieurs handlers MediatR pour rÃ©aliser une action complexe peut introduire du surcoÃ»t (sÃ©rialisation/dÃ©sÃ©rialisation de messages, I/O multiples, etc.). Des scÃ©narios transactionnels touchant plusieurs slices seront difficiles Ã  optimiser. Certes, dans la plupart des applications ce surcoÃ»t restera nÃ©gligeable, mais sur un systÃ¨me Ã  trÃ¨s hautes performances ou trÃ¨s complexe, câ€™est un point Ã  considÃ©rer. Lâ€™architecture Vertical Slice est surtout plÃ©biscitÃ©e pour des projets modulaires plutÃ´t que pour de grosses transactions multi-domaines â€“ lorsquâ€™on tente de lâ€™appliquer Ã  des contextes quâ€™elle gÃ¨re mal (transactions distribuÃ©es, rÃ¨gles globales), on se heurte Ã  ses limites.En rÃ©sumÃ©, Vertical Slice amÃ©liore la lisibilitÃ© locale et la sÃ©paration des prÃ©occupations par fonctionnalitÃ©, au prix potentiel de duplications et dâ€™une fragmentation du codebase. Ces inconvÃ©nients ne sont pas rÃ©dhibitoires, mais requiÃ¨rent une vigilance et possiblement la mise en place de patterns complÃ©mentaires (par exemple un service commun pour factoriser du code partagÃ© si nÃ©cessaire, ou un cadre dâ€™architecture global minimal pour guider les devs).Quand utiliser lâ€™architecture Vertical Slice ?Comme tout choix architectural, lâ€™approche Vertical Slice est plus ou moins adaptÃ©e selon le contexte. Voici des situations oÃ¹ son usage est recommandÃ©, et dâ€™autres oÃ¹ il lâ€™est moins :ScÃ©narios recommandÃ©s Applications modulaires ou microservicesÂ : Si votre application peut Ãªtre dÃ©coupÃ©e en modules indÃ©pendants (par exemple par domaine mÃ©tier ou bounded context), lâ€™architecture Vertical Slice sâ€™aligne naturellement. Chaque module ou microservice peut correspondre Ã  un ensemble de slices cohÃ©rents. On parle dâ€™ailleurs souvent de modular monolith pour dÃ©signer un monolithe structurÃ© en tranches verticales, qui offre une maintenabilitÃ© et une comprÃ©hension aisÃ©e du code. Dans ces architectures modulaires, chaque slice (ou ensemble de slices) reprÃ©sente un composant du systÃ¨me, facilement isolable pour le dÃ©veloppement, le dÃ©ploiement ou les tests. APIs REST simples ou projets de petite envergureÂ : Pour une petite application, un MVP ou un service avec des cas dâ€™utilisation simples, adopter dâ€™emblÃ©e une architecture hexagonale ou Clean complÃ¨te peut Ãªtre surdimensionnÃ©. Dans ces cas, lâ€™approche Vertical Slice apporte de la simplicitÃ©. On Ã©vite de crÃ©er plÃ©thore de couches et dâ€™abstractions alors que ce nâ€™est pas nÃ©cessaire pour un petit projet. En dâ€™autres termes, si vos besoins sont limitÃ©s et bien circonscrits, Â« bricoler quelque chose rapidement Â» en sÃ©parant par fonctionnalitÃ©s est tout Ã  fait raisonnable, et formaliser une couche de domaine complÃ¨te serait une perte de temps et dâ€™Ã©nergie. Vous irez plus vite tout en gardant une bonne organisation par features. DÃ©veloppement de nouvelles fonctionnalitÃ©s isolÃ©esÂ : MÃªme au sein dâ€™un projet existant, vous pouvez utiliser Vertical Slice pour des modules trÃ¨s spÃ©cifiques ou autonomes. Par exemple, lâ€™ajout dâ€™une petite API indÃ©pendante dans un grand systÃ¨me peut se faire sous forme de slice verticale sans impacter lâ€™architecture globale. De maniÃ¨re gÃ©nÃ©rale, si une fonctionnalitÃ© nâ€™a quasiment pas de dÃ©pendance sur le reste du systÃ¨me, la dÃ©velopper comme une unitÃ© verticale permet de la livrer rapidement et de faÃ§on propre. Ã‰quipes multiples sur des domaines diffÃ©rentsÂ : Si vous avez plusieurs Ã©quipes ou dÃ©veloppeurs qui travaillent en parallÃ¨le sur des sous-domaines diffÃ©rents, leur attribuer des vertical slices diffÃ©rentes peut rÃ©duire la collision de leurs travaux. Chaque Ã©quipe peut Ã©voluer dans â€œsonâ€ pÃ©rimÃ¨tre avec moins de risques de modifier du code utilisÃ© par les autres. Dans un contexte Agile, cela facilite la livraison continue de features indÃ©pendantes. (Il faut toutefois, comme mentionnÃ©, veiller Ã  garder une cohÃ©rence globale via des guidelines communes.)ScÃ©narios moins adaptÃ©s Grand monolithe avec logique mÃ©tier partagÃ©eÂ : Si votre application forme un monolithe trÃ¨s complexe avec de nombreuses rÃ¨gles mÃ©tier transverses, une architecture strictement Vertical Slice risque de montrer ses limites. Par exemple, dans un systÃ¨me bancaire monolithique, des rÃ¨gles de gestion comme â€œun client ne peut pas dÃ©passer tel dÃ©couvertâ€ sâ€™appliquent Ã  plusieurs fonctionnalitÃ©s. Les implÃ©menter sÃ©parÃ©ment dans chaque tranche conduirait Ã  de la duplication et possiblement des incohÃ©rences. Dans ce genre de contexte, il est souvent prÃ©fÃ©rable dâ€™unifier la logique mÃ©tier centrale dans un modÃ¨le commun (par exexemple, via une couche Domaine partagÃ©e, comme le propose Clean Architecture). Lâ€™architecture â€œpropreâ€ et dÃ©rivÃ©es (Hexagonale, etc.) sont le fruit de dÃ©cennies dâ€™expÃ©rience pour bÃ¢tir des solutions complexes de grande taille, que ce soit en monolithe ou en microservices. Elles apportent un cadre rigoureux pour garantir la cohÃ©rence de la logique mÃ©tier Ã  travers tout le systÃ¨me. Ã€ lâ€™inverse, une approche Vertical Slice pure serait hasardeuse ici, car chaque slice devrait rÃ©implÃ©menter ou appeler ces rÃ¨gles communes, augmentant les risques dâ€™erreur. FonctionnalitÃ©s fortement couplÃ©es entre ellesÂ : Si vos cas dâ€™utilisation interagissent Ã©troitement, quâ€™ils doivent se coordonner ou partager beaucoup de donnÃ©es, une sÃ©paration stricte par slices peut Ãªtre artificielle. Par exemple, une opÃ©ration complexe pourrait nÃ©cessiter lâ€™enchaÃ®nement de plusieurs slices (ce qui revient Ã  simuler une transaction rÃ©partie). Ce genre dâ€™opÃ©ration est pÃ©nible Ã  rÃ©aliser proprement sans couche de service ou orchestrateur commun. Une architecture classique en couches permettrait ici de centraliser cette orchestration dans un service mÃ©tier, lÃ  oÃ¹ Vertical Slice ne fournit pas de rÃ©ponse Ã©vidente. En bref, plus vos fonctionnalitÃ©s sont interdÃ©pendantes, moins lâ€™architecture en tranches verticales est indiquÃ©e. Besoins de rÃ©utilisation du code mÃ©tierÂ : Dans certains projets, on cherche Ã  construire une bibliothÃ¨que ou un noyau de composants rÃ©utilisables (par exemple, un moteur de calcul utilisÃ© par plusieurs applications diffÃ©rentes). Dans ce cas, isoler complÃ¨tement chaque feature nâ€™est pas souhaitable : on veut au contraire factoriser le cÅ“ur commun. Vertical Slice, qui cloisonne le code par use case, nâ€™est pas orientÃ©e vers la mutualisation du code. Si lâ€™un de vos objectifs est de rÃ©utiliser une partie significative de la logique dans dâ€™autres contextes ou dâ€™exposer un modÃ¨le de domaine cohÃ©rent, une architecture du style Clean/DDD sera plus adaptÃ©e. Equipe dÃ©butante ou hÃ©tÃ©rogÃ¨ne sans guidelinesÂ : Enfin, il faut noter quâ€™une Ã©quipe peu expÃ©rimentÃ©e ou sans discipline pourrait dÃ©river avec une architecture Vertical Slice. Parce quâ€™elle impose moins de structure prÃ©dÃ©finie quâ€™une architecture en couches, chaque dÃ©veloppeur pourrait Ãªtre tentÃ© dâ€™organiser Â« Ã  sa sauce Â» sa slice. Sans concertation, le code risque de manquer dâ€™uniformitÃ© et la maintenance deviendra difficile. Pour une Ã©quipe junior, une architecture plus classique (Clean, couches MVC, etc.) sert parfois de filet de sÃ©curitÃ© grÃ¢ce Ã  son cadre clair. Cela ne disqualifie pas Vertical Slice, mais souligne lâ€™importance dâ€™un leadership technique pour guider sa mise en place dans un contexte dâ€™Ã©quipe moins expÃ©rimentÃ©e.En somme, lâ€™architecture Vertical Slice brille dans les contextes modulaires, Ã©volutifs et indÃ©pendants, ou lorsque lâ€™on veut aller vite sur des fonctionnalitÃ©s ciblÃ©es. En revanche, dÃ¨s quâ€™une forte mutualisation ou des invariants globaux sont nÃ©cessaires, il faut soit lâ€™adapter (en combinant avec dâ€™autres approches), soit envisager une architecture diffÃ©rente plus appropriÃ©e.Comparaison avec le Clean ArchitectureIl est frÃ©quent dâ€™opposer Vertical Slice Ã  le Clean Architecture (architecture â€œpropreâ€ dâ€™aprÃ¨s Robert C. Martin), car ces deux approches structurent le code trÃ¨s diffÃ©remment. En rÃ©alitÃ©, elles poursuivent des objectifs communs (modularitÃ©, maintenabilitÃ©) mais via des principes distincts. Comparons-les selon quelques axes clÃ©s :Principes fondamentauxLe Clean Architecture (ainsi que les architectures couches classiques, hexagonale, oignon, etc.) sâ€™articule autour de la sÃ©paration des prÃ©occupations horizontale et du respect de la dÃ©pendance inverse. Elle vise Ã  rendre le cÅ“ur du logiciel indÃ©pendant des dÃ©tails dâ€™implÃ©mentation. Par exemple, la logique mÃ©tier ne doit dÃ©pendre ni dâ€™un framework particulier, ni de la base de donnÃ©es, ni de lâ€™UI. ConcrÃ¨tement, on retrouve dans Clean Architecture des couches bien dÃ©finies (EntitÃ©s du domaine, Cas dâ€™usage, Interfaces dâ€™accÃ¨s aux donnÃ©es, etc.), avec la rÃ¨gle que les dÃ©pendances pointent vers lâ€™intÃ©rieur du cercle (vers le domaine). Lâ€™accent est mis sur la stabilitÃ© du modÃ¨le mÃ©tier : il sâ€™agit de protÃ©ger les rÃ¨gles de gestion des changements technologiques ou des caprices de lâ€™interface.Lâ€™architecture Vertical Slice, de son cÃ´tÃ©, est guidÃ©e par la cohÃ©rence fonctionnelle. Son principe central est de grouper le code par fonctionnalitÃ© mÃ©tier de maniÃ¨re autonome. Chaque slice traite une et une seule fonctionnalitÃ© et embarque tout ce quâ€™il faut pour la rÃ©aliser. Lâ€™idÃ©e sous-jacente est dâ€™obtenir une forte cohÃ©sion interne (tout le code liÃ© Ã  un cas dâ€™utilisation est assemblÃ©) et une faible dÃ©pendance externe (peu de liens avec dâ€™autres parties du systÃ¨me). On couple â€œverticalementâ€ le long dâ€™un flux fonctionnel, plutÃ´t quâ€™horizontalement par type de composant. En adoptant Vertical Slice, on accepte un certain cloisonnement par feature, au bÃ©nÃ©fice dâ€™une flexibilitÃ© locale et dâ€™une simplicitÃ© de comprÃ©hension par cas dâ€™usage.En rÃ©sumÃ©, Clean Architecture met lâ€™emphase sur le domaine et les abstractions stables, alors que Vertical Slice met lâ€™emphase sur les features et leur isolation mutuelle. Clean cherche Ã  Ã©liminer le couplage technique (UI, DB, frameworks) vis-Ã -vis du mÃ©tier, tandis que Vertical Slice cherche Ã  Ã©liminer le couplage fonctionnel entre les diffÃ©rentes parties du systÃ¨me.Organisation du code et du projetSi on regarde lâ€™organisation concrÃ¨te dâ€™un projet .NET dans chaque approche, la diffÃ©rence saute aux yeux. Clean Architecture propose typiquement de structurer la solution en couches ou couches concentriques. On peut avoir par exemple des projets ou folders sÃ©parÃ©s pour : le domaine (entitÃ©s et interfaces), les cas dâ€™application (use cases ou services applicatifs), lâ€™interface (contrÃ´leurs API, UI) et lâ€™infrastructure (implÃ©mentations des dÃ©pÃ´ts, accÃ¨s BD, services externes). Chaque couche a une responsabilitÃ© spÃ©cifique et des dÃ©pendances restreintes (p.ex. lâ€™infrastructure dÃ©pend du domaine pour implÃ©menter ses interfaces, mais lâ€™inverse nâ€™est pas vrai). Cette organisation par couches apporte de la clartÃ© sur le rÃ´le de chaque classe, mais peut introduire de la verbositÃ© (de nombreux projets et fichiers pour une seule feature). Il faut naviguer entre plusieurs dossiers pour suivre le fil dâ€™une fonctionnalitÃ© donnÃ©e.Vertical Slice, Ã  lâ€™opposÃ©, organise le code par regroupement vertical. On va crÃ©er un dossier (ou un namespace) par fonctionnalitÃ© ou par module mÃ©tier, et y placer tous les Ã©lÃ©ments liÃ©s : contrÃ´leur API ou endpoint correspondant, classes de commande/requÃªte, handler MÃ©diatR, modÃ¨les spÃ©cifiques, etc.. Par exemple, au lieu dâ€™avoir un dossier Controllers avec tous les contrÃ´leurs de lâ€™application, on aura un dossier Features contenant des sous-dossiers par fonctionnalitÃ© : Produit, Commande, Client, etc., et Ã  lâ€™intÃ©rieur de chacun, les fichiers pour crÃ©er un produit, mettre Ã  jour un produit, etc. Chaque slice peut ainsi avoir ses propres modÃ¨les ou services internes sâ€™il en a besoin, sans impacter les autres slices. Cette organisation par feature facilite la localisation du code â€“ pour toucher Ã  une fonctionnalitÃ©, on sait exactement oÃ¹ aller â€“ mais rend plus difficile la rÃ©utilisation dâ€™une classe dâ€™une slice Ã  lâ€™autre (puisquâ€™a priori on Ã©vite de le faire). En pratique, il est courant quâ€™un projet Vertical Slice nâ€™ait quâ€™un ou deux assemblages (ex: un projet Web et Ã©ventuellement un projet pour les contrats), lÃ  oÃ¹ une Clean Architecture en comporte plusieurs pour sÃ©parer les couches. Cela rÃ©duit la complexitÃ© initiale (moins de projets .NET Ã  configurer), au prix dâ€™une sÃ©paration moins nette entre logique mÃ©tier et dÃ©tails techniques.Couplage et dÃ©pendancesLa Clean Architecture excelle Ã  rÃ©duire le couplage entre le domaine et les dÃ©tails dâ€™implÃ©mentation. GrÃ¢ce Ã  lâ€™inversion des dÃ©pendances, le cÅ“ur mÃ©tier ne â€œconnaÃ®tâ€ pas la base de donnÃ©es, ni le framework web utilisÃ©. Cela permet, par exemple, de changer de base de donnÃ©es ou de technologie dâ€™UI sans toucher au domaine (en thÃ©orie tout au moins). Le couplage est ainsi contrÃ´lÃ© et dirigÃ© : les couches haut-niveau dÃ©pendent de couches bas-niveau abstraites (interfaces), jamais lâ€™inverse. En revanche, les fonctionnalitÃ©s dans Clean Architecture sont couplÃ©es via le domaine commun. Par exemple, deux use cases diffÃ©rents vont manipuler la mÃªme classe Produit ou Commande. Cela garantit une logique uniforme, mais signifie aussi que ces use cases sont indirectement reliÃ©s : toute modification dans la structure dâ€™une entitÃ© du domaine peut impacter de multiples fonctionnalitÃ©s. Le couplage fonctionnel nâ€™est pas Ã©liminÃ©, il est centralisÃ© dans le domaine.Lâ€™approche Vertical Slice, de son cÃ´tÃ©, cherche Ã  minimiser le couplage entre fonctionnalitÃ©s au profit dâ€™une forte cohÃ©sion interne Ã  chaque fonctionnalitÃ©. IdÃ©alement, chaque slice a ses propres entitÃ©s ou modÃ¨les et nâ€™interagit pas directement avec les autres slices. Le couplage technique (ex: appel base de donnÃ©es) nâ€™est pas nÃ©cessairement inversÃ© comme en Clean Architecture â€“ une tranche peut trÃ¨s bien appeler directement un ORM ou une requÃªte SQL si câ€™est le plus simple â€“ mais ce choix nâ€™affecte que cette tranche prÃ©cise. En rÃ©duisant la portÃ©e dâ€™influence du code, Vertical Slice fait pencher la balance vers une multitude de petits couplages locaux plutÃ´t quâ€™un grand couplage central. Le risque est bien sÃ»r dâ€™introduire du couplage dupliquÃ© (plusieurs slices dÃ©pendant des mÃªmes concepts implÃ©mentÃ©s en parallÃ¨le), dâ€™oÃ¹ lâ€™importance de bien dÃ©limiter les frontiÃ¨res. On peut rÃ©sumer en disant que Clean Architecture vise un faible couplage â€œverticalâ€ (technique), tandis que Vertical Slice vise un faible couplage â€œhorizontalâ€ (fonctionnel). Les deux ne sont pas incompatibles, mais lâ€™accent nâ€™est pas mis au mÃªme endroit.ScalabilitÃ© du code et de lâ€™architectureLorsquâ€™on parle de scalabilitÃ© ici, on entend la capacitÃ© de lâ€™architecture Ã  supporter la croissance de lâ€™application (plus de fonctionnalitÃ©s, plus de dÃ©veloppeurs, plus de complexitÃ©) tout en restant maintenable.Du cÃ´tÃ© de Clean Architecture, la structure en couches apporte une rigueur apprÃ©ciable pour les projets de grande envergure. Câ€™est une architecture Ã©prouvÃ©e pour organiser des solutions complexes avec de nombreuses rÃ¨gles mÃ©tier et de gros Ã©quipes. En sÃ©parant nettement le domaine, les cas dâ€™utilisation et les dÃ©tails techniques, on peut faire Ã©voluer chacun de ces aspects indÃ©pendamment. Clean Architecture favorise la factorisation du code commun, ce qui Ã©vite lâ€™effet boule de neige lors de changements globaux. Par exemple, si une rÃ¨gle mÃ©tier impacte plusieurs cas dâ€™utilisation, on la codera une fois dans le domaine (ou un service commun) plutÃ´t que de devoir la rÃ©pliquer. Ainsi, quand la complexitÃ© augmente, lâ€™architecture en couches offre un cadre qui encaisse la montÃ©e en volume (avec cependant le compromis dâ€™une complexitÃ© interne plus Ã©levÃ©e et dâ€™une courbe dâ€™apprentissage pour les nouveaux dÃ©veloppeurs). On note aussi que Clean Architecture amÃ©liore la maintenabilitÃ© et lâ€™Ã©volutivitÃ© du systÃ¨me sur le long terme, car les prÃ©occupations sont bien isolÃ©es â€“ ce qui peut justifier son investissement initial dans des projets appelÃ©s Ã  grossir.Du cÃ´tÃ© de Vertical Slice, la scalabilitÃ© se joue diffÃ©remment. Ajouter de nouvelles fonctionnalitÃ©s est trÃ¨s simple (comme Ã©voquÃ©, on ajoute du code neuf sans impacter lâ€™existant), ce qui donne une impression de facilitÃ© pour faire croÃ®tre le pÃ©rimÃ¨tre fonctionnel. De plus, plusieurs Ã©quipes peuvent contribuer en parallÃ¨le sur des slices diffÃ©rentes, ce qui scale bien humainement (moins de collisions de code). Beaucoup de projets web ou API croissent initialement plus vite avec ce modÃ¨le quâ€™avec une architecture hexagonale plus formelle. Cependant, plus lâ€™application grossit, plus on accumule de slices â€“ et les risques mentionnÃ©s de duplication ou dâ€™incohÃ©rence peuvent augmenter exponentiellement. Sans discipline, une grosse base de code en Vertical Slices peut devenir difficile Ã  maintenir si chaque tranche a implÃ©mentÃ© sa version de la rÃ©alitÃ©. En outre, la gestion des fonctionnalitÃ©s transverses (par ex. logging commun, transactions multi-slices, rÃ¨gles de validation globales) peut devenir ardue. En pratique, Vertical Slice peut tout Ã  fait sâ€™appliquer Ã  de grands systÃ¨mes, mais souvent en combinaison avec dâ€™autres patterns pour encadrer la complexitÃ©. Par exemple, on peut trÃ¨s bien imaginer un grand monolithe modulable oÃ¹ chaque module est un ensemble de vertical slices, tout en conservant un schÃ©ma de base commun et des infrastructures partagÃ©es. Dâ€™ailleurs, les architectes recommandent souvent de ne pas opposer frontalement Clean Architecture et Vertical Slice, mais dâ€™envisager dâ€™associer les deux dans un mÃªme projet. Par exemple, on peut structurer son code en vertical slices au sein de chaque couche dâ€™un modÃ¨le hexagonal (features folders + interfaces de repo, etc.), ou utiliser Vertical Slice pour la partie application tandis que le domaine reste centralisÃ©. Ces deux approches ne sont pas mutuellement exclusives et peuvent se complÃ©ter pour obtenir un systÃ¨me Ã  la fois modulaire et consistant.En synthÃ¨se, Clean Architecture offre une colonne vertÃ©brale solide pour les systÃ¨mes complexes et de long terme, tandis que Vertical Slice apporte de la lÃ©gÃ¨retÃ© et de la flexibilitÃ© pour le dÃ©veloppement orientÃ© fonctionnalitÃ©.Le tableau ci-dessous rÃ©sume quelques diffÃ©rences :Voici une version restructurÃ©e, plus fluide et symÃ©trique pour chaque point : Principe directeur :Clean Architecture repose sur une organisation par couches avec des dÃ©pendances stables orientÃ©es vers le domaine. Vertical Slice se structure par fonctionnalitÃ©s, avec une forte cohÃ©sion autour de chaque cas dâ€™usage. Organisation du code :Clean Architecture rÃ©partit le code en couches sÃ©parÃ©es (domaine, application, infrastructure). Vertical Slice regroupe tout le code nÃ©cessaire Ã  une fonctionnalitÃ© dans un mÃªme ensemble (commande, handler, modÃ¨les, etc.). Couplage :Clean Architecture vise un dÃ©couplage fort vis-Ã -vis des dÃ©tails techniques, mais centralise les fonctionnalitÃ©s autour dâ€™un modÃ¨le commun. Vertical Slice rÃ©duit le couplage entre fonctionnalitÃ©s, en favorisant une autonomie locale, au prix potentiel dâ€™une duplication. Ã‰chelle et complexitÃ© :Clean Architecture convient bien aux applications complexes ou dâ€™envergure, avec une structure rigide mais robuste Ã  long terme. Vertical Slice est plus adaptÃ© aux projets modulaires ou aux fonctionnalitÃ©s isolÃ©es, avec un dÃ©marrage rapide mais qui demande un encadrement au fil de la croissance.Il nâ€™y a pas de Â« gagnant Â» absolu : le choix dÃ©pend du contexte de votre projet. Lâ€™important est de comprendre ces diffÃ©rences pour appliquer le bon dosage. La section suivante illustre concrÃ¨tement la structure Vertical Slice dans un projet .NET pour mieux ancrer ces concepts.Exemple de projet Vertical Slice en .NET (C#)Pour rendre les choses plus concrÃ¨tes, prenons un exemple dâ€™application .NET utilisant lâ€™architecture Vertical Slice. Imaginons une API pour gÃ©rer des commandes (Orders) dans un systÃ¨me e-commerce. Nous allons voir comment structurer le code par fonctionnalitÃ©s, et comment sâ€™articule lâ€™utilisation de MediatR, des Commands et Queries.Structure par fonctionnalitÃ©sSupposons que notre API offre deux opÃ©rations principales pour les commandes : CrÃ©er une nouvelle commande (Create Order) et Obtenir les dÃ©tails dâ€™une commande (Get Order Details). En Vertical Slice, on va crÃ©er deux tranches sÃ©parÃ©es pour ces deux cas dâ€™utilisation. La structure du projet pourrait ressembler Ã  ceci :ğŸ“¦ MonProjet.APIâ””â”€â”€ Features â””â”€â”€ Orders â”œâ”€â”€ CreateOrder â”‚ â”œâ”€â”€ CreateOrderCommand.cs // Commande pour crÃ©er une commande â”‚ â””â”€â”€ CreateOrderHandler.cs // Handler pour traiter la crÃ©ation â””â”€â”€ GetOrderDetails â”œâ”€â”€ GetOrderDetailsQuery.cs // RequÃªte pour obtenir les dÃ©tails â””â”€â”€ GetOrderDetailsHandler.cs // Handler pour traiter la requÃªteDans ce schÃ©ma, tout ce qui concerne â€œCreate Orderâ€ vit dans son propre dossier (sous Orders/CreateOrder), et pareil pour â€œGet Order Detailsâ€. On pourrait Ã©galement avoir des sous-dossiers par agrÃ©gat ou entitÃ© principale (ici Orders regroupe les features liÃ©es aux commandes). Lâ€™idÃ©e est quâ€™en ouvrant le dossier dâ€™une fonctionnalitÃ©, on retrouve toutes les piÃ¨ces du puzzle pour cette fonctionnalitÃ©, plutÃ´t que de devoir chercher dans un dossier Controllers, puis un dossier Services, puis Repository, etc.Command, Query et Handler avec MediatRVoyons maintenant Ã  quoi ressemblent les classes Ã  lâ€™intÃ©rieur dâ€™une slice. Nous allons crÃ©er un Query pour la lecture des dÃ©tails dâ€™une commande, et une Command pour la crÃ©ation dâ€™une commande. Nous utiliserons les interfaces de MediatR (IRequest&lt;T&gt; et IRequestHandler&lt;TRequest, TResponse&gt;) pour dÃ©finir nos requÃªtes/commandes et leurs gestionnaires.Exemple : Lire les dÃ©tails dâ€™une commande (Query)// DTO (Data Transfer Object) reprÃ©sentant le rÃ©sultat renvoyÃ© au clientpublic record OrderDto(int OrderId, string ProductName, decimal TotalAmount);// RequÃªte de lecture pour obtenir les dÃ©tails d'une commande spÃ©cifiquepublic record GetOrderDetailsQuery(int OrderId) : IRequest&lt;OrderDto&gt;;// Handler associÃ© Ã  la requÃªte GetOrderDetailsQuerypublic class GetOrderDetailsHandler : IRequestHandler&lt;GetOrderDetailsQuery, OrderDto&gt;{ private readonly IOrderRepository _repository; public GetOrderDetailsHandler(IOrderRepository repository) { _repository = repository; } public async Task&lt;OrderDto&gt; Handle(GetOrderDetailsQuery query, CancellationToken cancellationToken) { // RÃ©cupÃ©rer la commande depuis la base de donnÃ©es (ou autre source) var order = await _repository.GetById(query.OrderId); if (order is null) { return null; // ou lever une exception NotFound, selon les besoins } // Mapper les donnÃ©es de la commande vers le DTO de sortie return new OrderDto(order.Id, order.ProductName, order.TotalAmount); }}Dans cet extrait, GetOrderDetailsQuery est une simple classe (ici un record) qui porte les paramÃ¨tres nÃ©cessaires (lâ€™Id de la commande). Elle implÃ©mente IRequest&lt;OrderDto&gt; indiquant quâ€™elle attend une rÃ©ponse de type OrderDto. Le GetOrderDetailsHandler contient la logique pour traiter la requÃªte : typiquement, il va chercher la commande dans un dÃ©pÃ´t (IOrderRepository) et convertir le rÃ©sultat en DTO. GrÃ¢ce Ã  MediatR, ce handler sera automatiquement appelÃ© lorsque nous enverrons un objet GetOrderDetailsQuery via le mÃ©diateur.Exemple : CrÃ©er une nouvelle commande (Command)// Commande pour crÃ©er une nouvelle commandepublic record CreateOrderCommand(int ProductId, int Quantity) : IRequest&lt;OrderDto&gt;;// Handler associÃ© Ã  la commande CreateOrderCommandpublic class CreateOrderHandler : IRequestHandler&lt;CreateOrderCommand, OrderDto&gt;{ private readonly IOrderRepository _repository; // on peut imaginer qu'on utilise un dÃ©pÃ´t ou un service de domaine public CreateOrderHandler(IOrderRepository repository) { _repository = repository; } public async Task&lt;OrderDto&gt; Handle(CreateOrderCommand command, CancellationToken cancellationToken) { // Logique mÃ©tier simplifiÃ©e: crÃ©er l'entitÃ© Order et l'enregistrer var newOrder = new Order { ProductId = command.ProductId, Quantity = command.Quantity, // ... (autres initialisations, calcul du total etc.) }; await _repository.Add(newOrder); // Retourner un DTO reprÃ©sentant la commande crÃ©Ã©e return new OrderDto(newOrder.Id, newOrder.ProductName, newOrder.TotalAmount); }}Ici, CreateOrderCommand encapsule les informations nÃ©cessaires pour crÃ©er une commande (par ex. identifiant du produit, quantitÃ©). Le CreateOrderHandler effectue la crÃ©ation : dans une vraie application, il pourrait appeler des rÃ¨gles de domaine (vÃ©rifier le stock, calculer le montant, etc.) et utiliser le dÃ©pÃ´t pour sauvegarder la commande en base. Ã€ la fin, il retourne un OrderDto avec les dÃ©tails de la nouvelle commande. Remarquez que toute la logique spÃ©cifique Ã  â€œcrÃ©er une commandeâ€ est confinÃ©e dans ce handler â€“ si demain une rÃ¨gle change (par ex. limiter la quantitÃ© maximale), on viendra lâ€™implÃ©menter ici sans impacter dâ€™autres features.IntÃ©gration dans un contrÃ´leur ou un endpoint : GrÃ¢ce Ã  MediatR, nos controllers restent trÃ¨s simples. Par exemple, un contrÃ´leur Web API pour crÃ©er une commande pourrait ressembler Ã  :[ApiController][Route(\"api/[controller]\")]public class OrdersController : ControllerBase{ private readonly IMediator _mediator; public OrdersController(IMediator mediator) { _mediator = mediator; } [HttpPost] public async Task&lt;IActionResult&gt; CreateOrder([FromBody] CreateOrderCommand command) { OrderDto result = await _mediator.Send(command); return CreatedAtAction(\"GetOrder\", new { id = result.OrderId }, result); } [HttpGet(\"{id}\")] public async Task&lt;IActionResult&gt; GetOrder(int id) { OrderDto resultat = await _mediator.Send(new GetOrderDetailsQuery(id)); return resultat is null ? NotFound() : Ok(result); }}On voit que le contrÃ´leur ne contient quasiment aucune logique : il se contente de transmettre la commande/requÃªte au mÃ©diateur (_mediator.Send(...)) et de retourner la rÃ©ponse appropriÃ©e (ici un code 201 Created ou 200 OK). Toute la â€œvraieâ€ logique est implÃ©mentÃ©e dans nos handlers, ce qui correspond bien Ã  lâ€™esprit Vertical Slice : chaque requÃªte HTTP correspond Ã  un use case gÃ©rÃ© par un handler dÃ©diÃ©. Cette structure rend les controllers minces et faciles Ã  maintenir, et dÃ©place le poids de la logique dans les slices oÃ¹ elle est plus facile Ã  tester.Points Ã  noter dans cet exemple On a un fichier par requÃªte/commande et un par handler, mais on aurait pu regrouper la commande et son handler dans le mÃªme fichier (câ€™est une question de style). Lâ€™important est que la sÃ©paration est faite par cas dâ€™utilisation et non par type dâ€™objet. Les classes OrderDto, CreateOrderCommand, GetOrderDetailsQuery, etc., sont spÃ©cifiques Ã  leur slice. Aucune dâ€™entre elles nâ€™est utilisÃ©e en dehors de la fonctionnalitÃ© en question. Cela garantit que modifier lâ€™une nâ€™impactera pas dâ€™autres parties du systÃ¨me par effet de bord inattendu. Chaque handler pourrait avoir ses propres validations (par exemple, vÃ©rifier que Quantity est positive dans CreateOrderHandler). On pourrait utiliser des Behavior MediatR ou des filtres, mais souvent chaque slice gÃ¨re ses validations soit inline, soit via des composants dÃ©diÃ©s (par exemple un FluentValidation validator par commande). Lâ€™essentiel est que, lÃ  encore, la validation mÃ©tier dâ€™un use case vit Ã  cÃ´tÃ© de ce use case. On peut tout Ã  fait utiliser des patterns de conception Ã  lâ€™intÃ©rieur dâ€™un vertical slice. Par exemple, si la logique de crÃ©ation de commande devenait trÃ¨s complexe, on pourrait introduire une couche de domaine (entitÃ© Order avec des mÃ©thodes, service de domaine, etc.) au sein de cette slice. Vertical Slice nâ€™interdit pas dâ€™Ã©crire du code propre ! Il dit juste : Â« ne le rends pas partagÃ© dâ€™office si ce nâ€™est pas nÃ©cessaire Â». Vous pouvez donc combiner Vertical Slice et principes DDD/Clean Ã  un niveau fin, en crÃ©ant une mini-architecture hexagonale interne Ã  une tranche si le besoin sâ€™en fait sentir.RÃ©flexion et questions ouvertesNous avons explorÃ© ce quâ€™est lâ€™architecture Vertical Slice, ses atouts et ses faiblesses, ainsi que son positionnement vis-Ã -vis de lâ€™architecture Clean. Pour conclure cet article, il convient dâ€™insister quâ€™il nâ€™existe pas de solution universelle â€“ le choix dÃ©pend de votre contexte spÃ©cifique. Voici quelques questions ouvertes pour alimenter votre rÃ©flexion et vous aider Ã  Ã©valuer la pertinence de Vertical Slice pour votre projetÂ : Vos fonctionnalitÃ©s sont-elles rÃ©ellement indÃ©pendantes ? Partagez-vous beaucoup de rÃ¨gles mÃ©tier et de donnÃ©es entre diffÃ©rentes parties de lâ€™application, ou bien pouvez-vous facilement isoler des slices sans crÃ©er de doublons massifs ? Si votre domaine est trÃ¨s connectÃ©, une architecture par couches pourrait mieux convenir. Si au contraire vous pouvez dessiner des frontiÃ¨res nettes, Vertical Slice peut apporter de la clartÃ©. Quelle complexitÃ© anticipez-vous Ã  long terme ? Sâ€™il sâ€™agit dâ€™un petit service ou dâ€™un module simple, Vertical Slice vous fera gagner du temps et de la souplesse. En revanche, pour un produit stratÃ©gique qui va Ã©voluer sur des annÃ©es avec de nombreuses fonctionnalitÃ©s, envisagez-vous que lâ€™absence dâ€™un modÃ¨le central puisse devenir un frein (duplication, incohÃ©rences) ? Faut-il prÃ©voir une combinaison dâ€™approches pour grandir sereinement ? Quelle importance accordez-vous Ã  la rÃ©utilisation et aux abstractions ? PrÃ©fÃ©rez-vous du code dupliquÃ© mais simple et lisible, ou une factorisation poussÃ©e quitte Ã  introduire des couches dâ€™abstraction supplÃ©mentaires ? La premiÃ¨re approche favorise Vertical Slice, la seconde sâ€™aligne plus avec Clean/DDD. En fonction de votre prioritÃ© (rapiditÃ© de dÃ©veloppement vs. rationalisation du code), lâ€™une ou lâ€™autre approche prendra lâ€™avantage. Votre Ã©quipe et votre organisation sont-elles prÃªtes ? ConsidÃ©rez le niveau dâ€™expÃ©rience de vos dÃ©veloppeurs et la structure de votre Ã©quipe. Sont-ils Ã  lâ€™aise pour Ã©voluer sans le filet dâ€™une architecture standard ? Vont-ils suivre des conventions communes pour ne pas que chaque slice devienne un microcosme isolÃ© ? Avez-vous les outils pour documenter et surveiller la cohÃ©rence de lâ€™ensemble ? Lâ€™architecture Vertical Slice demande une certaine discipline dans un contexte dâ€™Ã©quipe pour Ã©viter lâ€™effet Â« tour de Babel Â» oÃ¹ chacun code dans son coin. Ã€ lâ€™inverse, une Ã©quipe responsable et bien synchronisÃ©e pourra prospÃ©rer grÃ¢ce Ã  la libertÃ© quâ€™elle offre.En rÃ©pondant Ã  ces questions, vous serez en mesure de peser le pour et le contre de lâ€™architecture Vertical Slice dans votre cas particulier. Nâ€™hÃ©sitez pas Ã  expÃ©rimenter Ã  petite Ã©chelle, voire Ã  combiner des approches (comme structurer votre code en slices tout en conservant un noyau de domaine commun pour les invariants critiques). Lâ€™important est de choisir une architecture qui sert au mieux les besoins de votre projet et de votre Ã©quipe â€“ que ce soit Vertical Slice, Clean Architecture, une combinaison des deux, ou toute autre variante architecturale. AprÃ¨s tout, une architecture nâ€™est rÃ©ussie que si elle vous permet de livrer un logiciel de qualitÃ©, maintenable et Ã©volutif, dans les dÃ©lais et avec le sourire de lâ€™Ã©quipe ğŸ˜Š.ğŸ‘‰ Pour approfondir la rÃ©flexion et dÃ©couvrir un point de vue Ã©clairant sur le sujet, je vous recommande vivement la vidÃ©o de Milan JovanoviÄ‡." }, { "title": "Changements de licences dans lâ€™Ã©cosystÃ¨me .NET â€“ quelles implications ?", "url": "/posts/changements-licences-ecosysteme-dotnet/", "categories": "outil-developpement", "tags": "", "date": "2025-06-15 19:00:00 -0400", "snippet": "IntroductionÂ : une vague de changements inattendueLâ€™Ã©cosystÃ¨me .NET a rÃ©cemment Ã©tÃ© secouÃ© par plusieurs changements de licence touchant plusieurs bibliothÃ¨ques couramment adoptÃ©es par les dÃ©velopp...", "content": "IntroductionÂ : une vague de changements inattendueLâ€™Ã©cosystÃ¨me .NET a rÃ©cemment Ã©tÃ© secouÃ© par plusieurs changements de licence touchant plusieurs bibliothÃ¨ques couramment adoptÃ©es par les dÃ©veloppeurs. Des composants autrefois entiÃ¨rement open source adoptent dÃ©sormais des modÃ¨les commerciaux ou des licences restrictives. Du moteur de cache Redis au framework de tests FluentAssertions, en passant par la bibliothÃ¨que de mapping AutoMapper, le mÃ©diateur MediatR ou encore le bus de messages MassTransit, ces Ã©volutions soulÃ¨vent des questions. Pourquoi ces projets changent-ils de licence, quels impacts pour les dÃ©veloppeurs et architectes, et comment la communautÃ© rÃ©agit-elle ? Cet article propose un tour dâ€™horizon de ces changements, avec un ton Ã  la fois informatif et Ã©ditorial, pour inviter Ã  la rÃ©flexion sur notre utilisation des dÃ©pendances externes.RedisÂ : de lâ€™open source Ã  RSAL, puis retour aux sourcesDepuis son lancement, Redis Ã©tait publiÃ© sous une licence permissive BSD. Cependant, Redis Labs (lâ€™entreprise derriÃ¨re Redis) a introduit en 2024 une nouvelle licence pour certains modules complÃ©mentaires de Redis, appelÃ©e Redis Source Available License (RSAL). ConcrÃ¨tement, seuls des modules comme RedisSearch ou RedisJSON sont concernÃ©s, pas le cÅ“ur de Redis qui reste sous BSD. La RSAL permet de consulter le code source mais restreint lâ€™utilisation commerciale en mode SaaS : il est toujours gratuit de les utiliser en interne ou de les intÃ©grer dans un produit dÃ©ployÃ© sur site, mais proposer un service cloud basÃ© sur ces modules nÃ©cessite dÃ©sormais une licence commerciale. Lâ€™objectif affichÃ© Ã©tait de protÃ©ger Redis des gÃ©ants du cloud (Amazon Web Services, notamment) qui proposaient Redis en service managÃ© sans contribuer au projet.Cette dÃ©cision a eu des rÃ©percussions contrastÃ©es. Pour les dÃ©veloppeurs utilisant Redis dans des projets open source ou des applications internes, le changement Ã©tait mineur. En revanche, pour les entreprises offrant Redis dans leurs solutions cloud, cela impliquait de nouvelles obligations contractuelles et possiblement des coÃ»ts supplÃ©mentaires. Certaines startups ont dÃ» examiner de prÃ¨s leur conformitÃ© vis-Ã -vis de la RSAL pour Ã©viter tout risque juridique. Ã€ plus large Ã©chelle, on anticipait des coÃ»ts accrus pour les fournisseurs de services Redis-as-a-Service, et lâ€™on a vu Ã©merger lâ€™idÃ©e de remplacer les modules concernÃ©s par des alternatives open source ou dÃ©veloppÃ©es maison. Dâ€™autres acteurs du cloud, comme AWS, ont envisagÃ© de se tourner vers des forks de Redis ou des solutions alternatives propriÃ©taires, tÃ©moignant de lâ€™impact stratÃ©gique de ce changement de licence.La communautÃ© open source, de son cÃ´tÃ©, a trÃ¨s mal accueilli ce virage. Le passage dâ€™une licence BSD Ã  un modÃ¨le Â«Â source disponibleÂ Â» a Ã©tÃ© perÃ§u comme une atteinte aux principes du logiciel libre. DÃ¨s lors, des forks communautaires ont vu le jour â€“ par exemple le projet Valkey soutenu par la Linux Foundation, visant Ã  fournir une version de Redis pleinement open source en rÃ©action Ã  la RSAL. Face Ã  ces tensions et Ã  un Ã©cho largement nÃ©gatif, Redis Labs a finalement fait marche arriÃ¨re en 2025Â : RedisÂ 8 est â€œredevenuâ€ open source, sous licence AGPLv3 (une licence approuvÃ©e par lâ€™OSI). Ce choix de lâ€™AGPLv3 garantit que Redis reste un logiciel libre tout en imposant, ironiquement, des contraintes similaires aux objectifs initiaux â€“ lâ€™AGPL oblige toute offre SaaS basÃ©e sur Redis 8 Ã  publier son propre code source modifiÃ©, ce qui dissuade lâ€™exploitation sans contribution. Le retour de Salvatore Sanfilippo (alias @antirez, crÃ©ateur de Redis) dans lâ€™Ã©quipe en novembreÂ 2024 a dâ€™ailleurs coÃ¯ncidÃ© avec ce changement dâ€™orientation. Il sâ€™est rÃ©joui publiquement de voir Redis renouer avec ses racines open sourceÂ : Â«Â Je suis heureux que Redis soit Ã  nouveau un logiciel open source, sous les termes de la licence AGPLv3Â Â». La communautÃ©, initialement froissÃ©e, sâ€™est dite rassurÃ©e par ce geste dâ€™ouverture et de rÃ©conciliation aprÃ¨s la pÃ©riode de turbulence. Redis 8 apporte en outre de nouvelles fonctionnalitÃ©s et optimisations notables, preuve que lâ€™innovation continue tout en revenant Ã  un modÃ¨le plus ouvert.Fluent AssertionsÂ : une bibliothÃ¨que de tests qui devient payanteAutre changement majeur dans lâ€™Ã©cosystÃ¨me .NETÂ : Fluent Assertions, lâ€™une des bibliothÃ¨ques les plus populaires pour lâ€™Ã©criture dâ€™assertions fluides en tests unitaires, a basculÃ© vers un modÃ¨le commercial. Depuis la version 8.0.0, sortie fin 2024, Fluent Assertions nâ€™est plus pleinement open source : il est dÃ©sormais distribuÃ© sous une licence commerciale, ce qui signifie que les utilisateurs commerciaux doivent acquÃ©rir une licence pour lâ€™utiliser au-delÃ  de la version 7. En dâ€™autres termes, la version 7.0.0 (et antÃ©rieures) restait sous licence ouverte (Apache 2.0/MIT auparavant), mais toute mise Ã  jour ultÃ©rieure nÃ©cessite un achat pour un usage en entreprise. Ce tournant a fait lâ€™effet dâ€™une petite bombe dans la communautÃ© des testeurs .NET, habituÃ©s depuis des annÃ©es Ã  la gratuitÃ© de cet outil essentiel.La rÃ©action de la communautÃ© a Ã©tÃ© mitigÃ©e. Beaucoup de dÃ©veloppeurs ont exprimÃ© leur dÃ©ception et leur dÃ©sir de conserver une solution open source pour leurs tests. TrÃ¨s rapidement, la rÃ©ponse communautaire sâ€™est organisÃ©e : un projet alternatif nommÃ© AwesomeAssertions a Ã©tÃ© crÃ©Ã©, sous licence ApacheÂ 2.0, visant Ã  reproduire les fonctionnalitÃ©s phares de Fluent Assertions tout en restant libre. Ce fork communautaire illustre la volontÃ© dâ€™une partie des dÃ©veloppeurs de sâ€™affranchir dâ€™une contrainte commerciale imposÃ©e a posteriori. Par ailleurs, pour ceux qui souhaitaient continuer Ã  utiliser Fluent Assertions sans frais, une solution immÃ©diate a consistÃ© Ã  geler la version de la bibliothÃ¨que Ã  la derniÃ¨re Ã©dition gratuite. En fixant par exemple la dÃ©pendance NuGet Ã  la version 7.0.0 dans le fichier projet, on sâ€™assure de ne pas passer involontairement Ã  une version ultÃ©rieure payante. De nombreux projets ont adoptÃ© cette stratÃ©gie de rester sur la derniÃ¨re version open source connue, Ã©vitant ainsi toute entorse lÃ©gale.Ce changement soudain de licence a Ã©galement alimentÃ© un dÃ©bat plus large. Dâ€™un cÃ´tÃ©, il pose la question de la soutenabilitÃ© des projets open source : les mainteneurs de Fluent Assertions ont sans doute estimÃ© quâ€™un modÃ¨le payant Ã©tait nÃ©cessaire pour continuer Ã  faire Ã©voluer la bibliothÃ¨que. De lâ€™autre, certains membres de la communautÃ© ont critiquÃ© la maniÃ¨re de faire, qualifiant ce mouvement de â€œbrutalâ€. Des contributeurs bÃ©nÃ©voles se sont interrogÃ©s sur le fait que leur code, apportÃ© sous une licence ouverte, se retrouve du jour au lendemain dans un produit commercial â€“ une situation inconfortable et source de ressentiment. Fluent Assertions nâ€™est pas un cas isolÃ© dans lâ€™historique .NET (on se souvient dâ€™IdentityServer4 devenu payant en son temps, ou plus rÃ©cemment de la controverse autour de Moq et SponsorLink), mais lâ€™ampleur de son utilisation a rendu ce changement particuliÃ¨rement sensible. Il a en tout cas servi de catalyseur, incitant dÃ©veloppeurs et architectes Ã  reconsidÃ©rer la confiance aveugle mise dans les dÃ©pendances externes gratuites.AutoMapper et MediatRÂ : le choix dâ€™une double licence Ã©quilibrÃ©eTrois mois aprÃ¨s Fluent Assertions, lâ€™actualitÃ© des licences a de nouveau fait les gros titres dans la communautÃ© .NET. AutoMapper (outil de mapping objet/objet trÃ¨s rÃ©pandu) et MediatR (implÃ©mentation populaire du patron mÃ©diateur) vont Ã  leur tour emprunter la voie de la monÃ©tisation. Leur auteur principal, Jimmy Bogard, a annoncÃ© dÃ©but 2025 sa dÃ©cision de prendre un Â«Â virage commercialÂ Â» afin dâ€™assurer le succÃ¨s Ã  long terme de ces projets. Toutefois, contrairement Ã  un passage pur et simple Ã  une licence payante exclusive, Bogard opte pour un modÃ¨le de double licence (Â« dual license Â»). Cette approche vise Ã  concilier les deux mondes â€“ open source et commercial â€“ en fonction des profils dâ€™utilisateurs.ConcrÃ¨tement, AutoMapper et MediatR resteront gratuits pour un grand nombre dâ€™usages. Jimmy Bogard a exprimÃ© sa volontÃ© de conserver la gratuitÃ© pour : les dÃ©veloppeurs et projets purement open source, les individus, Ã©tudiants et hobbyistes qui utilisent ces outils Ã  des fins non lucratives, les organismes Ã  but non lucratif et les associations, les startups ou petites entreprises en dessous dâ€™un certain seuil de revenus ou de financement, mÃªme les usages en environnement non productif (par exemple, dÃ©veloppement, test, CI).En somme, ceux qui Â«Â bricolent pour le fun ou lâ€™intÃ©rÃªt gÃ©nÃ©ralÂ Â» ne devraient pas Ãªtre pÃ©nalisÃ©s. En revanche, les entreprises Ã  but lucratif qui exploitent AutoMapper et MediatR dans le cadre dâ€™activitÃ©s commerciales seront invitÃ©es Ã  acquÃ©rir une licence payante. Ce sont principalement ces utilisateurs professionnels, tirant profit business de la bibliothÃ¨que, qui porteront le financement du projet. Bogard explique en filigrane une question simple : Â«Â qui devrait payer pour assurer la pÃ©rennitÃ© des outils open source dont tout le monde dÃ©pendÂ ?Â Â» â€“ et sa rÃ©ponse est de ne pas faire payer les dÃ©veloppeurs individuels, mais bien les entreprises qui en retirent de la valeur.Au moment de lâ€™annonce, les dÃ©tails prÃ©cis (tarifs, modalitÃ©s exactes des licences) ne sont pas encore finalisÃ©s. Jimmy Bogard souhaite proposer un modÃ¨le le plus simple et transparent possible, probablement sous forme de forfaits par entreprise plutÃ´t quâ€™une tarification par utilisateur qui alourdirait la gestion. Il a insistÃ© sur le fait que rien ne changera Ã  court terme pour les utilisateurs actuels et quâ€™il communiquera en toute transparence sur lâ€™Ã©volution du modÃ¨le commercial. Cette dÃ©marche mesurÃ©e et expliquÃ©e a Ã©tÃ© plutÃ´t bien reÃ§ue par la communautÃ©, surtout en comparaison dâ€™autres cas plus abrupts. Beaucoup de dÃ©veloppeurs comprennent la rÃ©alitÃ© Ã  laquelle il fait faceÂ : son travail sur ces bibliothÃ¨ques nâ€™Ã©tait plus sponsorisÃ© depuis quâ€™il est indÃ©pendant, et continuer bÃ©nÃ©volement Ã  un tel niveau de qualitÃ© et de support nâ€™Ã©tait plus viable. En choisissant une double licence non punitive et en respectant les utilisateurs open source, Jimmy Bogard offre un exemple de transition rÃ©flÃ©chie qui, si elle se concrÃ©tise comme annoncÃ©, pourrait bien servir de modÃ¨le positif. La communautÃ© a saluÃ© sa transparence et son respect des utilisateurs historiques, soulignant que ce genre de changement â€“ lorsquâ€™il est bien communiquÃ© et justifiÃ© â€“ peut Ãªtre compris et acceptÃ©.MassTransit v9Â : une transition encadrÃ©e vers le modÃ¨le commercialAutre pilier de lâ€™Ã©cosystÃ¨me .NET, MassTransit (framework de messaging distribuÃ© alternatif Ã  NServiceBus) a lui aussi annoncÃ© un tournant important. PrÃ©vue pour la fin 2025, la version MassTransit 9 sera publiÃ©e sous une licence commerciale et non plus open source. Jusquâ€™Ã  la version 8 incluse, MassTransit Ã©tait distribuÃ© gratuitement sous licence ApacheÂ 2.0, cumulant les contributions et les utilisateurs satisfaits. Pourquoi ce changement ? Les mainteneurs expliquent que MassTransit est devenu au fil des ans une infrastructure critique pour de nombreuses entreprises (finance, santÃ©, logistique, etc.), avec plus de 30 packages NuGet formant un Ã©cosystÃ¨me complet. Son succÃ¨s a gÃ©nÃ©rÃ© des besoins croissants â€“ en termes de support, de corrections, de nouvelles fonctionnalitÃ©s â€“ quâ€™une Ã©quipe purement bÃ©nÃ©vole ne peut plus assumer facilement. Pour accÃ©lÃ©rer le dÃ©veloppement et offrir le niveau de support attendu par les utilisateurs en entreprise, un financement pÃ©renne est nÃ©cessaire. Dâ€™oÃ¹ la dÃ©cision de passer en commercial, afin de pouvoir allouer des ressources Ã  plein temps au projet et garantir un support de niveau enterprise.Les mainteneurs de MassTransit ont toutefois pris soin dâ€™accompagner cette transition de maniÃ¨re responsable. Ils ont clairement annoncÃ© que MassTransit v8 restera open source, maintenu sous sa licence actuelle (Apache 2.0). Le code existant ne disparaÃ®t donc pas du giron libre, et les projets qui utilisent v8 peuvent continuer Ã  le faire sans frais. Mieux, lâ€™Ã©quipe continuera Ã  publier des correctifs de sÃ©curitÃ© et bugs critiques sur v8 pendant une pÃ©riode de transition, pour ne pas lÃ©ser les utilisateurs qui resteraient sur lâ€™ancienne version. En parallÃ¨le, MassTransit v9 deviendra un produit commercial : toutes les nouveautÃ©s, amÃ©liorations de performance et fonctionnalitÃ©s Ã  venir seront rÃ©servÃ©es Ã  cette version sous licence payante. Des plans de support adaptÃ©s Ã  la taille des organisations seront proposÃ©s, y compris des tarifs pour les Ã©diteurs de logiciels indÃ©pendants ou les consultants qui intÃ¨grent MassTransit dans des solutions pour leurs clients. En Ã©change, les clients bÃ©nÃ©ficieront dâ€™un support expert, de garanties de stabilitÃ© (SLA) et de lâ€™assurance dâ€™un dÃ©veloppement actif soutenu financiÃ¨rement.Le calendrier annoncÃ© laisse le temps de sâ€™adapterÂ : une prÃ©version de MassTransit 9 sera disponible Q3 2025 pour les early adopters, mais la sortie officielle sous licence commerciale nâ€™interviendra quâ€™en Q1 2026. Jusquâ€™Ã  fin 2026, la v8 continuera dâ€™Ãªtre maintenue, puis elle sera probablement figÃ©e une fois la transition terminÃ©e. Cette progression graduelle offre aux Ã©quipes utilisatrices plusieurs options stratÃ©giques : rester sur v8 si les nouvelles fonctionnalitÃ©s ne sont pas indispensables, prÃ©parer un budget pour passer Ã  v9 lorsque nÃ©cessaire, ou Ã©valuer dâ€™autres solutions de messagerie open source avant lâ€™Ã©chÃ©ance. Globalement, la communautÃ© a accueilli cette annonce avec un mÃ©lange de comprÃ©hension et de rÃ©signation. Certes, personne ne se rÃ©jouit de voir un outil gratuit devenir payant, mais beaucoup reconnaissent le professionnalisme de la dÃ©marche MassTransit. La version open source nâ€™est pas â€œsabotÃ©eâ€, elle reste disponible et fonctionnelle, et la communication proactive de lâ€™Ã©quipe a permis dâ€™anticiper le changement plutÃ´t que de le subir. Des voix dans la communautÃ© soulignent que MassTransit a donnÃ© lâ€™exemple dâ€™une transition transparente, avec versionnage clair (v8 vs v9) et respect des utilisateurs existants â€“ une approche qui contraste avec dâ€™autres projets au changement plus abrupt.Impacts potentiels sur les projets .NETPour les dÃ©veloppeurs et architectes .NET, ces changements de licence ne sont pas quâ€™un sujet de discussion abstrait â€“ ils peuvent avoir des implications concrÃ¨tes sur les projets en cours ou Ã  venir. Voici quelques impacts Ã  considÃ©rerÂ : ConformitÃ© lÃ©galeÂ : intÃ©grer une dÃ©pendance dont la licence a changÃ© sans sâ€™y conformer peut mettre un projet en situation dâ€™infraction. Il est impÃ©ratif de suivre de prÃ¨s les licences de nos packages NuGet. Par exemple, continuer Ã  mettre Ã  jour Fluent Assertions vers la v8+ dans une application commerciale sans acheter la licence constituerait une violation explicite des nouvelles conditions. De mÃªme, utiliser Redis avec des modules RSAL dans un service SaaS sans accord commercial pourrait exposer Ã  des poursuites. Certaines licences open source virales comme lâ€™AGPL exigent aussi de distribuer son propre code source en cas dâ€™utilisation rÃ©seau â€“ un point Ã  ne pas ignorer si lâ€™on envisage RedisÂ 8 AGPLv3 dans un produit propriÃ©taire. En somme, les chefs de projet doivent intÃ©grer la vÃ©rification de licence dans leur gouvernance (Ã  lâ€™instar de la vÃ©rification technique), pour Ã©viter les mauvaises surprises juridiques. CoÃ»ts additionnelsÂ : lâ€™impact financier est Ã©videmment au premier plan des prÃ©occupations. Des outils autrefois gratuits peuvent dÃ©sormais engendrer des coÃ»ts non prÃ©vus dans le budget. Cela peut affecter la rentabilitÃ© dâ€™un produit ou le coÃ»t de dÃ©veloppement dâ€™un projet. Par exemple, une entreprise qui faisait massivement usage de Fluent Assertions ou envisageait MassTransit v9 doit budgÃ©ter le prix des licences ou abonnements correspondants. Pour les solutions cloud, on lâ€™a vu avec Redis, devoir payer une licence sur certaines fonctionnalitÃ©s peut augmenter les frais dâ€™exploitation. Cet aspect pousse dâ€™ailleurs Ã  Ã©valuer le ROI (return on investment) de chaque dÃ©pendance : si un outil payant simplifie Ã©normÃ©ment le dÃ©veloppement, son coÃ»t peut Ãªtre justifiÃ© ; Ã  lâ€™inverse, pour un gain marginal, une alternative gratuite ou maison pourrait redevenir plus attractive. StratÃ©gie technique et dÃ©pendancesÂ : ces changements invitent Ã  revisiter nos choix techniques. En tant quâ€™architectes, il faut toujours avoir un plan B pour les composants clÃ©s. DÃ©sormais, plusieurs approches sâ€™offrent aux Ã©quipes confrontÃ©es Ã  une dÃ©pendance devenue payanteÂ : Rester sur la derniÃ¨re version libre : câ€™est la solution court-terme adoptÃ©e par beaucoup suite au changement de Fluent Assertions (verrouiller la versionÂ 7.0.0 pour Ã©viter la 8.0.0 commerciale). Cela permet de gagner du temps sans casser le build, mais câ€™est une solution figÃ©e qui nâ€™apporte plus dâ€™Ã©volutions. Explorer des alternatives open source : souvent, lâ€™Ã©cosystÃ¨me propose dâ€™autres bibliothÃ¨ques au rÃ´le similaire. Par exemple, Shouldly peut remplacer Fluent Assertions dans les tests, AwesomeAssertions a Ã©mergÃ© pour reprendre le flambeau open source, et dâ€™autres mÃ©diateurs ou mappeurs existent en lieu et place de MediatR/AutoMapper. Il faut cependant Ã©valuer le coÃ»t dâ€™apprentissage et de migration vers ces alternatives. Forker ou internaliser la maintenance : si la licence le permet (par exemple, rester indÃ©finiment sur une version Apache/MIT antÃ©rieure), une Ã©quipe pourrait dÃ©cider de forker le projet et de maintenir son propre dÃ©rivÃ© en interne. Câ€™est ce quâ€™ont fait implicitement les initiateurs de AwesomeAssertions ou de Valkey. Cette voie offre une libertÃ© totale, mais elle exige des ressources pour corriger bugs et ajouter des features soi-mÃªme â€“ un engagement lourd que peu dâ€™Ã©quipes peuvent se permettre sur le long terme. Passer au modÃ¨le payant : enfin, il ne faut pas Ã©carter lâ€™option de payer pour la qualitÃ©. Si un outil apporte une vraie valeur ajoutÃ©e et quâ€™aucune alternative nâ€™Ã©gale son efficacitÃ©, souscrire une licence peut Ãªtre le meilleur choix. AprÃ¨s tout, investir dans un composant logiciel nâ€™est pas diffÃ©rent que payer pour un framework propriÃ©taire ou un service cloud : câ€™est une dÃ©cision Ã  justifier par un bÃ©nÃ©fice. Nombre de mainteneurs espÃ¨rent dâ€™ailleurs que les entreprises comprendront quâ€™un logiciel libre a aussi un coÃ»t de dÃ©veloppement, et quâ€™y contribuer financiÃ¨rement via une licence revient Ã  assurer sa pÃ©rennitÃ©. En un mot, chaque Ã©quipe doit dÃ©sormais peser le pour et le contre : continuer gratuitement (avec dâ€™Ã©ventuelles concessions techniques), ou payer pour le confort et le support. Lâ€™important est dâ€™anticiper ces choix en amont, plutÃ´t que de les subir dans lâ€™urgence. RÃ©actions de la communautÃ© et dÃ©bats Ã©mergentsCes changements de cap dans des projets emblÃ©matiques ont naturellement dÃ©clenchÃ© de vifs dÃ©bats dans la communautÃ©. Du cÃ´tÃ© des dÃ©veloppeurs, on a entendu des craintes quant Ã  un possible effet domino : Â«Â Et si demain dâ€™autres bibliothÃ¨ques indispensables faisaient de mÃªmeÂ ?Â Â» Il existe un vÃ©ritable sentiment de â€œtrahisonâ€ chez certains, qui parlent de bait-and-switch (gratuitÃ© initiale suivie dâ€™un changement commercial inattendu). Lâ€™idÃ©e quâ€™un outil open source adoptÃ© massivement puisse changer de licence a posteriori est vÃ©cue comme un manquement Ã  un Â« contrat social Â» implicite. Ce ressentiment a Ã©tÃ© particuliÃ¨rement prononcÃ© pour Fluent Assertions, jugÃ© abrupt dans son annonce (peu de prÃ©avis, une licence perÃ§ue comme chÃ¨re, etc.). De mÃªme, lâ€™Ã©pisode Moq (oÃ¹ une bibliothÃ¨que de mocks avait ajoutÃ© subrepticement du code de collecte dâ€™e-mails pour pousser au sponsoring) a laissÃ© des traces, rendant les dÃ©veloppeurs plus mÃ©fiants. En rÃ©action Ã  chaque annonce, les forums et rÃ©seaux sociaux se sont enflammÃ©s : faut-il continuer Ã  faire confiance aux mainteneurs open source ? Nâ€™est-on pas en train de payer le prix dâ€™une dÃ©pendance aveugle aux gratuits dâ€™hier ?Face Ã  ces inquiÃ©tudes, une partie de la communautÃ© sâ€™est retroussÃ©e les manches pour proposer des solutions alternatives. Nous avons dÃ©jÃ  mentionnÃ© les forks libres comme AwesomeAssertions pour Fluent Assertions, ou Valkey pour Redis. Ces initiatives montrent lâ€™attachement des dÃ©veloppeurs aux valeurs open source. Cependant, elles posent aussi la question de leur viabilitÃ© Ã  long terme : crÃ©er un fork est facile, le maintenir activement dans la durÃ©e lâ€™est moins. Dâ€™autres projets, lorsquâ€™ils ont vu leur licence changer par le passÃ©, ont incitÃ© les utilisateurs Ã  rester sur lâ€™ancienne version stable (par ex. Entity Framework Core quand il a Ã©voluÃ© avec des changements majeurs, certains sont restÃ©s sur la v5 LTS). Bref, la communautÃ© dispose de garde-fous, mais ceux-ci ont leurs limites techniques et humaines.Il ne faudrait pas non plus opposer systÃ©matiquement communautÃ© et mainteneurs, car bien souvent ils partagent le mÃªme but : la pÃ©rennitÃ© du projet. Du point de vue des mainteneurs open source, la dÃ©cision de commercialiser un produit nâ€™est jamais anodine ni facile. Eux aussi font partie de la communautÃ© et en subissent le jugement. Lorsquâ€™un projet atteint une popularitÃ© Ã©norme (des millions de tÃ©lÃ©chargements NuGet) sans modÃ¨le Ã©conomique, le mainteneur bÃ©nÃ©vole peut sâ€™Ã©puiser ou rencontrer des difficultÃ©s Ã  concilier ce travail avec sa vie professionnelle. Jimmy Bogard lâ€™a explicitement dÃ©crit en parlant de ses contributions qui ont chutÃ© aprÃ¨s la fin du sponsoring par son employeur. De mÃªme, lâ€™Ã©quipe de MassTransit a clairement listÃ© les demandes impossibles Ã  satisfaire sans financement (support temps rÃ©el, nouvelles features, corrections rapides). Il y a donc une prise de conscience gÃ©nÃ©rale que les projets open source Ã  fort impact nÃ©cessitent un soutien, dâ€™une maniÃ¨re ou dâ€™une autre.Le dÃ©bat sâ€™est ainsi dÃ©placÃ© vers les modalitÃ©s de cette monÃ©tisation. Ce que la communautÃ© reproche le plus, ce nâ€™est pas tant quâ€™un mainteneur cherche Ã  Ãªtre rÃ©munÃ©rÃ© pour son travail (ce qui est lÃ©gitime), mais la faÃ§on de le faire. Plusieurs facteurs ont Ã©tÃ© identifiÃ©s dans les rÃ©actions : Le manque de transparence ou de prÃ©avis : des changements soudains, annoncÃ©s aprÃ¨s coup, passent trÃ¨s mal. Ã€ lâ€™inverse, annoncer Ã  lâ€™avance (comme MassTransit lâ€™a fait en prÃ©venant pour v9 tout en gardant v8 libre) est vu dâ€™un bon Å“il. Lâ€™ingratitude perÃ§ue : ignorer la contribution de la communautÃ© peut gÃ©nÃ©rer du ressentiment. Par exemple, la licence commerciale de Fluent Assertions a soulevÃ© des questions sur les PR et issues soumises par des bÃ©nÃ©voles par le passÃ©. Une solution parfois proposÃ©e est de â€œgrandfathererâ€ les versions existantes (les laisser libres) et dâ€™appliquer le commercial seulement aux nouvelles fonctionnalitÃ©s majeures â€“ ce que MassTransit a prÃ©cisÃ©ment fait. La justesse du modÃ¨le : toutes les licences payantes ne se valent pas. La communautÃ© a saluÃ© certaines approches jugÃ©es Ã©quilibrÃ©es, comme la double licence dâ€™ImageSharp (libre pour la plupart des usages, payante pour un usage commercial Ã  grande Ã©chelle) ou la dÃ©marche de Jimmy Bogard qualifiÃ©e de non-punitive. En revanche, elle fustige les pratiques jugÃ©es abusives (ex: Moq insÃ©rant du code non consenti, ou une licence trop restrictive sans alternative). Le respect des utilisateurs existants : câ€™est un point-clÃ©. Les projets qui ont su montrer du respect pour leur base dâ€™utilisateurs ont attÃ©nuÃ© les critiques. Lâ€™exemple de MassTransit est souvent citÃ© : laisser v8 open source et ne commercialiser que v9, câ€™est perÃ§u comme une marque de respect envers ceux qui ont construit leurs systÃ¨mes sur lâ€™outil. De mÃªme, Bogard qui dit vouloir Ã©viter de faire payer les petites structures montre quâ€™il nâ€™oublie pas dâ€™oÃ¹ vient la popularitÃ© de ses libs.En rÃ©sumÃ©, la communautÃ© .NET est en pleine discussion sur lâ€™Ã©quilibre entre open source et viabilitÃ© Ã©conomique. Ces dÃ©bats, parfois passionnÃ©s, ont au moins le mÃ©rite de poser les bonnes questions (quâ€™on dÃ©taillera ci-dessous) et de pousser tout le monde Ã  plus de clairvoyance. Mainteneurs comme utilisateurs peuvent y gagnerÂ : les premiers en apprenant Ã  communiquer tÃ´t et honnÃªtement, les seconds en devenant plus conscients de la valeur des outils quâ€™ils utilisent au quotidien.Les bonnes questions Ã  se poser avant dâ€™adopter un outil tiersÃ€ la lumiÃ¨re de ces Ã©vÃ©nements, il devient crucial, pour tout responsable technique, de sâ€™interroger en amont lorsquâ€™il ajoute une nouvelle dÃ©pendance Ã  son projet. Voici quelques questions clÃ©s Ã  considÃ©rerÂ : Quelle est la licence de cet outil, et est-elle compatible avec mon usage ? Est-ce une licence permissive (MIT, BSDâ€¦) ou plus contraignante (LGPL, AGPL, licence commerciale, etc.) ? Mon projet est-il commercial ou open source, et la licence de lâ€™outil est-elle en accord avec cela ? Par exemple, intÃ©grer du code sous AGPL dans une application propriÃ©taire distribuÃ©e Ã  des clients poserait problÃ¨me. Comprendre dÃ¨s le dÃ©part les obligations lÃ©gales (partage du code source, mention de copyright, interdiction dâ€™un usage SaaS sans accordâ€¦) Ã©vite les mauvaises surprises ultÃ©rieures. Le projet est-il soutenu par une organisation ou un modÃ¨le Ã©conomique pÃ©renne ? Sâ€™agit-il dâ€™un one-man-show dÃ©veloppÃ© sur le temps libre, dâ€™un projet gÃ©rÃ© par une fondation, ou dâ€™un produit sponsorisÃ© par une entreprise ? Un projet sans soutien financier clair peut Ãªtre amenÃ© Ã  changer de licence pour survivre â€“ on lâ€™a vu rÃ©cemment. VÃ©rifiez si le mainteneur a mis en place un Patreon, GitHub Sponsors, ou si une sociÃ©tÃ© (par ex. Red Hat avec Entity Framework Core dans le passÃ©, Redis Labs pour Redis, etc.) supporte le dÃ©veloppement. Un indicateur : la prÃ©sence dâ€™une documentation sur la licence, dâ€™une FAQ sur lâ€™usage commercial, ou dâ€™un historique de communication transparente. Sinon, restez attentifs aux annonces de version majeure qui pourraient changer la donne. Quelle est la criticitÃ© de cette dÃ©pendance dans mon architecture ? En dâ€™autres termes, que se passerait-il si demain elle devenait payante ou nâ€™Ã©tait plus maintenue ? Si la rÃ©ponse est Â«Â mon application serait paralysÃ©eÂ Â», il faut prÃ©voir un plan de secours. Cela peut signifier : garder un Å“il sur les alternatives existantes (mÃªme si on ne les utilise pas immÃ©diatement), concevoir son code de maniÃ¨re Ã  pouvoir changer de bibliothÃ¨que relativement aisÃ©ment (abstraction, interfaces, etc.), ou contribuer soi-mÃªme Ã  la maintenance du fork libre en cas de besoin. Ã‰vitez de bÃ¢tir un chÃ¢teau sur une dÃ©pendance dont vous ignorez tout de la soliditÃ© financiÃ¨re ou communautaire. Serions-nous prÃªts Ã  payer pour cet outil le moment venuÂ ? Cette question peut sembler incongrue lorsquâ€™on ne jure que par lâ€™open source gratuit, mais elle mÃ©rite dâ€™Ãªtre posÃ©e honnÃªtement. Si lâ€™outil en question apporte une valeur Ã©norme (gain de temps, fiabilitÃ©, performance) Ã  votre produit, pourriez-vous justifier un coÃ»t annuel de licence dans votre budget ? Par exemple, MassTransit envisage ~4000Â $ par an pour une PME â€“ est-ce un investissement envisageable par rapport au coÃ»t de dÃ©veloppement dâ€™une solution maison Ã©quivalente ? Si la rÃ©ponse est non, il peut Ãªtre prudent de limiter la dÃ©pendance Ã  cet outil dÃ¨s le dÃ©part ou dâ€™explorer des solutions alternatives moins risquÃ©es. Dans le cas contraire, prÃ©voir un Ã©ventuel coÃ»t dans le business plan du projet logiciel montre une approche mature et Ã©vite de se retrouver bloquÃ©. Comment puis-je contribuer Ã  la pÃ©rennitÃ© des outils dont je dÃ©pendsÂ ? Adopter une dÃ©pendance nâ€™est pas un acte anodin : cela crÃ©e une relation de fait avec son mainteneur. Une bonne question Ã  se poser est : ai-je les moyens de soutenir ce projet dâ€™une maniÃ¨re ou dâ€™une autreÂ ? Cela peut passer par du sponsoring financier (beaucoup de bibliothÃ¨ques acceptent les dons ou les sponsors entreprises), par des contributions techniques (pull requests, participation Ã  la documentation, signalement proactif des bugs), ou mÃªme par du mentorat (aider Ã  rÃ©pondre aux issues des autres utilisateurs). Un projet bien soutenu aura moins tendance Ã  chercher un financement drastique. En contribuant, vous investissez dans la santÃ© Ã  long terme de lâ€™outil â€“ et donc dans la sÃ©curitÃ© de votre propre projet. Cette question renverse un peu la perspective : plutÃ´t que de consommer passivement lâ€™open source, quelle part de responsabilitÃ© suis-je prÃªt Ã  endosser pour quâ€™il reste viable ?Lâ€™importance de comprendre licences et modÃ¨les open sourceLes Ã©vÃ©nements rÃ©cents sont riches dâ€™enseignements. Dâ€™abord, ils rappellent que Â«Â open sourceÂ Â» nâ€™est pas synonyme de gratuitÃ© inconditionnelle et Ã©ternelle. Il existe une myriade de licences open source avec des implications juridiques variÃ©es, et il est primordial pour les professionnels du logiciel de sâ€™y intÃ©resser de prÃ¨s. Entre un projet sous MIT qui autorise presque tout, un projet sous GPL/AGPL qui impose la redistribution du code dÃ©rivÃ©, ou un projet Â«Â source availableÂ Â» qui interdit carrÃ©ment certains usages commerciaux, les diffÃ©rences sont majeures. De mÃªme, un code disponible sur GitHub sans licence explicite nâ€™est pas forcÃ©ment libre de droits. Lire et comprendre la licence dâ€™un composant devrait faire partie du travail de base de tout intÃ©grateur logiciel.Ensuite, il faut avoir conscience que derriÃ¨re chaque projet open source se pose la question du modÃ¨le Ã©conomique. Si un outil est crucial pour vous, renseignez-vous sur comment il est maintenu. De nombreux mainteneurs explorent aujourdâ€™hui des modÃ¨les de durabilitÃ©Â : du simple consulting autour du projet, en passant par lâ€™open core (une base gratuite, des fonctionnalitÃ©s avancÃ©es payantes), les offres hÃ©bergÃ©es (SaaS), la double licence open source/commercial, etc.. Aucun modÃ¨le nâ€™est intrinsÃ¨quement mauvais â€“ câ€™est mÃªme sain que des auteurs puissent vivre de leurs crÃ©ations â€“ mais il vaut mieux le savoir Ã  lâ€™avance. Un projet sponsorisÃ© par une grande entreprise a moins de chances de changer subitement de licence (quoiqueâ€¦), tandis quâ€™un projet portÃ© par un mainteneur isolÃ© sans source de revenus est plus susceptible, un jour, dâ€™Ã©voluer vers une formule payante ou dâ€™Ãªtre abandonnÃ©. Lâ€™initiative FOSSED (FOSS Emergency Dashboard) recense par exemple les changements de licence et fournit des conseils sur la marche Ã  suivre. Lâ€™idÃ©e nâ€™est pas de se mÃ©fier de tout, mais dâ€™Ãªtre informÃ©. Un dÃ©veloppeur ou architecte moderne doit ajouter cette corde Ã  son arc : la veille sur les licences et la comprÃ©hension des enjeux Ã©conomiques open source.Enfin, ces changements soulignent le risque dâ€™une dÃ©pendance excessive Ã  des outils externes. Cela ne signifie pas quâ€™il faille rÃ©inventer la roue pour chaque projet â€“ lâ€™usage de bibliothÃ¨ques Ã©prouvÃ©es reste une bonne pratique â€“ mais quâ€™il faut mesurer le niveau de dÃ©pendance. Quand un composant devient central dans votre architecture, demandez-vous comment vous feriez sâ€™il venait Ã  manquer. Lâ€™exemple de Redis lâ€™illustre bien : certaines entreprises ont dÃ©couvert leur vulnÃ©rabilitÃ© quand la licence a changÃ©, et ont dÃ» rÃ©Ã©valuer leur stratÃ©gie technique en urgence. IdÃ©alement, concevez vos systÃ¨mes de maniÃ¨re modulaire pour pouvoir substituer une implÃ©mentation par une autre. Et surtout, gardez un Å“il sur lâ€™Ã©volution de vos dÃ©pendances : abonnez-vous aux newsletters des projets GitHub, lisez les notes de version, suivez les blogs des auteurs. Un changement de licence ne tombe pas du cielÂ : il est souvent prÃ©cÃ©dÃ© de signes avant-coureurs (ralentissement des commits, discussions sur le financement, version majeure Ã  lâ€™horizon, etc.). En restant attentif, on peut anticiper et sâ€™adapter au lieu de subir.ConclusionÂ : vers un Ã©quilibre entre confiance et vigilanceEn conclusion, lâ€™Ã©cosystÃ¨me .NET vit une pÃ©riode charniÃ¨re oÃ¹ la gratuiteÌ dâ€™hier nâ€™est plus garantie pour demain. Cela pousse chaque acteur â€“ du dÃ©veloppeur individuel Ã  lâ€™architecte logiciel en entreprise â€“ Ã  gagner en maturitÃ© sur la gestion des dÃ©pendances. Il ne sâ€™agit pas de rejeter lâ€™open source, bien au contraireÂ : ces outils restent dâ€™une valeur inestimable et la plupart demeurent libres. Mais il sâ€™agit de passer dâ€™une confiance aveugle Ã  une confiance Ã©clairÃ©e. Oui, on peut continuer Ã  adopter des bibliothÃ¨ques externes pour aller plus vite et plus loin, Ã  condition de faire preuve de vigilanceÂ : vÃ©rifier la licence, Ã©valuer la pÃ©rennitÃ©, contribuer quand on le peut, et prÃ©voir un plan B.Ces rÃ©cents changements de licence, sâ€™ils ont pu Ã©branler nos habitudes, auront au moins eu le mÃ©rite de dÃ©clencher une prise de conscience collective. Ils nous invitent Ã  soutenir davantage les mainteneurs open source, Ã  mieux comprendre lâ€™Ã©conomie cachÃ©e derriÃ¨re nos lignes de code, et Ã  refaire de la stratÃ©gie logicielle un exercice complet, intÃ©grant les considÃ©rations techniques et lÃ©gales. Pour un dÃ©veloppeur ou un architecte logiciel, câ€™est un rappel que notre responsabilitÃ© ne sâ€™arrÃªte pas Ã  Ã©crire du code efficace : il faut aussi sâ€™assurer que ce code repose sur des fondations solides et durables, dans tous les sens du terme. Lâ€™open source a un bel avenir, si chacun y met du sien â€“ en connaissances, en contributions ou en financement â€“ afin que lâ€™innovation collaborative rime avec viabilitÃ© Ã  long terme." }, { "title": "Valider les courriels - pourquoi une regex ne suffit plus", "url": "/posts/validation-courriel/", "categories": "outil-developpement", "tags": "", "date": "2025-06-02 19:00:00 -0400", "snippet": "PrÃ©ambuleJe ne pensais pas Ã©crire un article aujourdâ€™hui sur la validation des courriels, mais une rÃ©cente discussion mâ€™a fait changer dâ€™avis. Comme beaucoup, jâ€™ai longtemps utilisÃ© une simple expr...", "content": "PrÃ©ambuleJe ne pensais pas Ã©crire un article aujourdâ€™hui sur la validation des courriels, mais une rÃ©cente discussion mâ€™a fait changer dâ€™avis. Comme beaucoup, jâ€™ai longtemps utilisÃ© une simple expression rÃ©guliÃ¨re (regex) pour vÃ©rifier le format dâ€™une adresse courriel saisie par un utilisateur. AprÃ¨s tout, la forme dâ€™une adresse Ã©lectronique semble facile Ã  valider : on a tous dÃ©jÃ  croisÃ© la fameuse regex ^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$ pour Â« valider Â» un courriel. Pourtant, valider une adresse email de maniÃ¨re fiable est bien plus complexe, et se reposer uniquement sur une regex est une mauvaise idÃ©e.RÃ©cemment, dans sa vidÃ©o Regex for Email Validation? Think Again!, Derek Comartin a expliquÃ© pourquoi cette approche nâ€™est ni fiable ni suffisante. Cela mâ€™a encouragÃ© Ã  approfondir le sujet et Ã  partager les bonnes pratiques modernes en matiÃ¨re de validation des adresses courriel.Les limites des regex pour les adresses courrielValider un email par regex atteint vite ses limites.Voici pourquoi : Une regex ne couvre jamais tous les cas dâ€™adresse valides. La norme RFC 5322 (et suivantes) autorise des variantes dâ€™adresses bien plus larges que nos regex simplifiÃ©es. Par exemple, de nombreux caractÃ¨res spÃ©ciaux sont permis dans la partie locale (avant le @) dâ€™une adresse : !, #, $, %, &amp;, ', _, /, =, ?, ^, `, {, |, } ou ~ peuvent thÃ©oriquement faire partie dâ€™un courriel. Une regex Â« maison Â» oublie souvent dâ€™en tenir compte, ce qui peut conduire Ã  rejeter Ã  tort des emails valides. De mÃªme, certaines adresses spÃ©ciales (contenant des guillemets, des caractÃ¨res non-ASCII, un nom de domaine littÃ©ral en IP, etc.) respectent les standards mais ne passeront pas un filtre regex trop strict. Ã€ lâ€™inverse, une regex peut accepter des adresses invalides. MÃªme une expression rÃ©guliÃ¨re trÃ¨s Ã©laborÃ©e peut laisser passer des adresses qui ne fonctionneront pas en pratique. Un exemple classique : une regex rÃ©putÃ©e â€œparfaiteâ€ autorisait tony@example.com. (avec un point Ã  la fin) comme adresse valide, alors quâ€™aucun serveur de messagerie nâ€™acceptera un courriel se terminant par un point. Autre cas : certaines implÃ©mentations basÃ©es sur le HTML5 considÃ¨rent test@test (sans domaine de second niveau) comme valide cÃ´tÃ© client, alors que cette adresse nâ€™a pas de domaine complet et nâ€™aboutirait pas. En bref, mÃªme les meilleurs patterns peuvent avoir des failles et donner un faux sentiment de sÃ©curitÃ©. Une regex ne dit rien sur lâ€™existence rÃ©elle de lâ€™adresse. Câ€™est le point le plus important. Une adresse peut trÃ¨s bien avoir la bonne forme et Ãªtre acceptÃ©e par toutes vos validations syntaxiques, tout en nâ€™existant pas du tout dans la rÃ©alitÃ© (domaine inexistant ou simplement aucune boÃ®te Ã  ce nom). La regex, aussi sophistiquÃ©e soit-elle, ne peut pas vÃ©rifier que le serveur de mail existe et que la boÃ®te de rÃ©ception est active. En dâ€™autres termes, une validation purement syntaxique nâ€™assure en rien que lâ€™email pourra effectivement recevoir vos messages.En plus de ces limites inhÃ©rentes aux regex, se reposer uniquement sur une validation cÃ´tÃ© client (front-end) est une pratique dangereuse. Tout contrÃ´le JavaScript ou Angular peut Ãªtre contournÃ© par un utilisateur malintentionnÃ©. Si votre application ne valide les courriels quâ€™au niveau du navigateur, vous risquez dâ€™enregistrer en base de donnÃ©es des adresses totalement invalides envoyÃ©es directement Ã  votre serveur. Il est impÃ©ratif de reproduire les validations critiques cÃ´tÃ© serveur, sans se fier exclusivement au contrÃ´le du formulaire cÃ´tÃ© client.Que proposent Angular, .NET et FluentValidation ?Face Ã  la difficultÃ© de dÃ©finir LA regex parfaite, les principaux frameworks et bibliothÃ¨ques ont adoptÃ© des approches pragmatiques â€“ soit en utilisant des patterns larges, soit au contraire en simplifiant la validation. Angular (Front-end) â€“ Le validateur dâ€™email intÃ©grÃ© dâ€™Angular utilise une expression rÃ©guliÃ¨re inspirÃ©e de la spÃ©cification HTML5 (WHATWG) avec quelques ajustements basÃ©s sur les RFC. ConcrÃ¨tement, Angular va vÃ©rifier que lâ€™adresse respecte le format gÃ©nÃ©ral (prÃ©sence dâ€™un @ et dâ€™un point, pas de point au dÃ©but/Ã  la fin de la partie locale, longueur maximale de 254 caractÃ¨res, etc.). Ce choix couvre la majoritÃ© des cas courants tout en Ã©vitant des erreurs grossiÃ¨res. Cependant, mÃªme cette regex â€œamÃ©liorÃ©eâ€ reste une solution gÃ©nÃ©rique : elle peut ne pas satisfaire tous les besoins mÃ©tier, surtout pour des cas extrÃªmes ou des domaines moins standards. Angular permet dâ€™ailleurs de la remplacer par un pattern personnalisÃ© si nÃ©cessaire. En somme, Angular offre une premiÃ¨re couche de validation utile cÃ´tÃ© client, mais nâ€™Ã©limine pas le besoin de validations supplÃ©mentaires cÃ´tÃ© serveur. .NET (Back-end) â€“ Du cÃ´tÃ© .NET, lâ€™Ã©volution est intÃ©ressante. Historiquement, la classe dâ€™attribut EmailAddressAttribute (dans System.ComponentModel.DataAnnotations) utilisait une Ã©norme expression rÃ©guliÃ¨re pour valider les emails (dans les versions .NET Framework 4.x et antÃ©rieures). Cette regex Â« monstrueuse Â» couvrait un grand nombre de cas, mais elle alourdissait la validation et restait imparfaite. Depuis .NET Core, Microsoft a changÃ© dâ€™approche : lâ€™attribut se contente dÃ©sormais de vÃ©rifier que la chaÃ®ne contient un @ qui nâ€™est ni en premiÃ¨re position ni en derniÃ¨re position. En clair, si un texte a au moins un caractÃ¨re avant et aprÃ¨s un @, il passe la validation de base. Par exemple, \"code.maze.com\" sera jugÃ© invalide (pas de @), \"code@maze\" invalide (le @ est en fin de chaÃ®ne aprÃ¨s â€œmazeâ€?), et \"code@[emailÂ protected]\" valide (mÃªme si ce nâ€™est pas une adresse fonctionnelle). Ce choix peut surprendre, mais il est volontairement minimaliste. Lâ€™objectif de .NET ici est juste dâ€™Ã©liminer les entrÃ©es grossiÃ¨rement incorrectes, sans tenter de capturer toutes les subtilitÃ©s du RFC. Microsoft assume que la vÃ©ritable validation doit se faire autrement (via un workflow dâ€™activation, etc.), plutÃ´t que de maintenir une regex complexe et faillible. RÃ©sultat : on Ã©vite de fausses erreurs sur des adresses valides mais un peu atypiques, en attrapant seulement les cas Ã©vidents (absences de @, etc.). FluentValidation (Back-end, .NET) â€“ FluentValidation, une bibliothÃ¨que populaire de validation en .NET, aligne sa stratÃ©gie sur celle de .NET Core. Son validateur intÃ©grÃ© EmailAddress() rÃ©alise par dÃ©faut la mÃªme vÃ©rification simplifiÃ©e : prÃ©sence dâ€™un @ non situÃ© au dÃ©but ni Ã  la fin de la chaÃ®ne. Cette dÃ©marche est assumÃ©e pour coller au comportement dâ€™ASP.NET Core et Ã  lâ€™attribut EmailAddressAttribute. Dans la documentation officielle, les auteurs expliquent que le check est intentionnellement naÃ¯f, car â€œfaire quelque chose dâ€™infaillible est trÃ¨s difficileâ€. Ils recommandent de valider rÃ©ellement lâ€™adresse en envoyant un email de confirmation, la validation logicielle nâ€™ayant pour but que dâ€™attraper les valeurs grossiÃ¨rement erronÃ©es destinÃ©es Ã  lâ€™UI. FluentValidation offre bien la possibilitÃ© dâ€™utiliser lâ€™ancien mode de validation (une regex hÃ©ritÃ©e de .NET 4.x) via EmailAddress(EmailValidationMode.Net4xRegex), mais cette option est dÃ©sormais dÃ©prÃ©ciÃ©e et gÃ©nÃ¨re un avertissement â€“ signe que la validation basÃ©e sur une regex exhaustive nâ€™est plus recommandÃ©e. En pratique, si vous utilisez FluentValidation, le conseil est donc le mÃªme : ne faites quâ€™une validation de format trÃ¨s basique, puis dÃ©lÃ©guez le reste du travail.En rÃ©sumÃ©, les frameworks modernes tendent Ã  assouplir la validation syntaxique des emails plutÃ´t quâ€™Ã  la renforcer outre mesure. Angular propose une regex large couvrant les cas usuels, tandis que .NET/FluentValidation optent pour un contrÃ´le minimal. Lâ€™idÃ©e sous-jacente est la suivante : plutÃ´t que dâ€™essayer de tout valider parfaitement avec du code statique (regex), mieux vaut attraper le 90% dâ€™erreurs Ã©videntes, et traiter le 10% restant via des mÃ©canismes dynamiques.Valider lâ€™existence de lâ€™adresse (sans envoyer de mail)Attraper les erreurs de format, câ€™est bien â€“ mais Ã§a ne suffit pas, on lâ€™a vu. La vraie question est : cette adresse existe-t-elle rÃ©ellement ? Autrefois, certains systÃ¨mes envoyaient un email de confirmation Ã  lâ€™adresse fournie, espÃ©rant quâ€™un message invalide â€œreviendrait Ã  lâ€™expÃ©diteurâ€ si lâ€™adresse nâ€™existe pas. Mauvaise idÃ©e ! Envoyer un courriel juste pour vÃ©rifier quâ€™il arrive peut nuire Ã  votre rÃ©putation : si vous envoyez en masse Ã  des adresses invalides, les serveurs destinataires peuvent vous classer comme spammeur. De plus, vous risquez de froisser vos nouveaux utilisateurs en les obligeant Ã  une confirmation pÃ©nible, ou pire, dâ€™envoyer des emails non sollicitÃ©s. Derek Comartin le mentionnait dans sa vidÃ©o : cette approche nâ€™est pas la meilleure non plus.La solution moderne consiste Ã  dÃ©lÃ©guer la validation rÃ©elle Ã  un service externe spÃ©cialisÃ©. Il existe aujourdâ€™hui des services SaaS de vÃ©rification dâ€™adresses email (NeverBounce, ZeroBounce, Mailfloss, Kickbox et bien dâ€™autres) qui font ce travail de maniÃ¨re efficace sans envoyer de courriel au destinataire. Comment est-ce possible ? Ces services effectuent une sÃ©rie de contrÃ´les techniques : VÃ©rification du domaine : ils examinent si le nom de domaine de lâ€™adresse existe et possÃ¨de des enregistrements MX (serveurs de mail) configurÃ©s. Pas de serveur mail = adresse forcÃ©ment invalide. Ce premier filtre permet dÃ©jÃ  dâ€™Ã©liminer les domaines fantaisistes ou dÃ©sactivÃ©s. RÃ©ponse du serveur de messagerie : le service contacte le serveur de messagerie du domaine via le protocole SMTP (comme sâ€™il allait envoyer un message) et interroge sâ€™il peut accepter lâ€™adresse destinataire donnÃ©e. En simulant cette conversation avec le serveur (un ping de la boÃ®te mail), on peut savoir si lâ€™adresse est reconnue sans envoyer rÃ©ellement de mail. Si le serveur rÃ©pond â€œboÃ®te inexistanteâ€ (code dâ€™erreur 550 par ex.), on sait que lâ€™adresse est invalide. Si au contraire on obtient une rÃ©ponse positive (250 OK), câ€™est bon signe â€“ lâ€™adresse semble valide et prÃªte Ã  recevoir du courrier. DÃ©tection avancÃ©e : ces services ajoutent souvent dâ€™autres filtres utiles, comme lâ€™identification des adresses jetables (fournisseurs de mails temporaires), des spam traps (adresses piÃ¨ges qui ne servent quâ€™Ã  piÃ©ger les spammeurs), ou des adresses ayant un historique dâ€™abus. Par exemple, ZeroBounce signale si une adresse appartient Ã  un domaine connu pour Ãªtre toxique ou si câ€™est un alias â€œcatch-allâ€ qui accepte tous les mails sans garantir la dÃ©livrabilitÃ©. Ce sont des informations importantes pour maintenir la propretÃ© de vos listes de diffusion.Le grand avantage de ces services est quâ€™ils effectuent toutes ces vÃ©rifications en temps rÃ©el via des API, gÃ©nÃ©ralement en quelques centaines de millisecondes, et ce de maniÃ¨re transparente pour lâ€™utilisateur final. Par exemple, vous pouvez intÃ©grer lâ€™API de validation de NeverBounce ou ZeroBounce directement lors de lâ€™inscription de vos utilisateurs : dÃ¨s quâ€™ils soumettent leur adresse, une requÃªte API vÃ©rifie en arriÃ¨re-plan lâ€™adresse et indique si elle est valide, sans que vous ayez Ã  envoyer un seul courriel de test. Si lâ€™adresse sâ€™avÃ¨re invalide, vous pouvez inviter lâ€™utilisateur Ã  la corriger, Ã©vitant ainsi dâ€™enregistrer des contacts erronÃ©s dans votre base de donnÃ©es. Exemple de service SaaS : NeverBounce est lâ€™un de ces outils de validation trÃ¨s fiables et rapides. Il sâ€™intÃ¨gre facilement Ã  vos applications via API et peut vÃ©rifier des listes entiÃ¨res dâ€™emails en quelques minutes. Lors dâ€™un test comparatif, NeverBounce a pu vÃ©rifier 800 adresses en 5 minutes, identifiant correctement plus de 650 emails valides. Ce type de service assure une vÃ©rification complÃ¨te de vos courriels (domaine actif, serveur rÃ©pondant, adresse existante) sans effort de votre part. Lâ€™investissement en vaut la chandelle si la qualitÃ© de vos emails est critique (campagnes marketing, envois transactionnels, etc.), car il en va de votre taux de dÃ©livrabilitÃ© et de votre rÃ©putation dâ€™expÃ©diteur.En utilisant un service de validation externe, le workflow de validation â€œidÃ©alâ€ dâ€™un courriel ressemble Ã  ceci : Validation syntaxique lÃ©gÃ¨re (ex. via un contrÃ´le Angular en front-end, puis une vÃ©rification minimale cÃ´tÃ© serveur avec .NET/FluentValidation) pour attraper les erreurs de frappe grossiÃ¨res immÃ©diatement. Validation dâ€™existence via un service externe pour vÃ©rifier que lâ€™adresse existe et est opÃ©rationnelle (domaine OK, serveur OK, boÃ®te OK), sans envoyer de mail de confirmation. Prise en compte dans votre logique mÃ©tier : par exemple, marquer lâ€™adresse comme â€œvÃ©rifiÃ©eâ€ dans votre base de donnÃ©es ou votre CRM, permettre lâ€™inscription du compte associÃ©, etc. Si le service retourne un statut nÃ©gatif (adresse inexistante, jetable ou autre), dÃ©cider du traitement (demander une autre adresse, alerter lâ€™utilisateur, refuser lâ€™inscription, etc.).Ce processus Ã  plusieurs couches assure une validation robuste tout en offrant une bonne expÃ©rience utilisateur : on ne bloque pas lâ€™internaute sur des dÃ©tails de format exotiques, mais on sâ€™assure en coulisse que lâ€™adresse fournie est exploitable.ConclusionLa validation des adresses courriel ne doit plus se rÃ©sumer Ã  trouver LA regex miracle. Les pratiques ont Ã©voluÃ© : entre les spÃ©cifications extensibles du format email et les enjeux de dÃ©livrabilitÃ©, il est clair quâ€™une simple regex cÃ´tÃ© client ne suffit plus pour garantir la validitÃ© dâ€™une adresse. Il faut adopter une approche en plusieurs Ã©tapes, combinant tolÃ©rance sur le format et fermetÃ© sur lâ€™existence rÃ©elle.En 2025, le bon workflow pour valider un courriel est : âœ… VÃ©rifier lÃ©gÃ¨rement le format (juste assez pour Ã©viter les entrÃ©es absurdes, sans exclure Ã  tort des adresses valides) âœ… VÃ©rifier lâ€™existence via un service externe (sâ€™assurer que le domaine et la boÃ®te sont valides, sans vous faire passer pour un spammeur) âŒ Ne plus sâ€™acharner Ã  Ã©crire des regex compliquÃ©es, ni se fier uniquement Ã  la validation du navigateur. Une regex peut filtrer le gros des erreurs, mais pas toutes, et certainement pas la dÃ©livrabilitÃ© rÃ©elle.En appliquant ces principes, vous amÃ©liorerez la qualitÃ© des donnÃ©es collectÃ©es et rÃ©duirez les problÃ¨mes dâ€™emails invalides (bounces, plaintes, etc.) qui peuvent nuire Ã  votre plateforme.Enfin, nâ€™hÃ©sitez pas Ã  documenter ce choix technologique dans votre registre de dÃ©cisions interne. La maniÃ¨re dont vous validez les courriels est un choix dâ€™architecture applicative important, qui mÃ©rite dâ€™Ãªtre tracÃ© noir sur blanc. Comme le souligne Derek Comartin, garder un log de toutes les dÃ©cisions clÃ©s dâ€™un projet Ã©vite bien des devinettes par la suite, en fournissant le contexte du pourquoi chaque dÃ©cision a Ã©tÃ© prise. Inscrire dans votre registre de dÃ©cisions que â€œla validation des emails se fait via une vÃ©rification syntaxique minimale + service externe X, pour telles raisonsâ€ permettra aux futurs dÃ©veloppeurs de comprendre et de respecter cette approche cohÃ©rente.En somme, valider un courriel aujourdâ€™hui, ce nâ€™est pas chercher la perfection avec une regex interminable, câ€™est mettre en place un processus intelligent : souple sur le format, strict sur lâ€™existence, et toujours documentÃ©. Adoptez ce workflow moderne â€“ vos utilisateurs, votre Ã©quipe et votre serveur mail vous en remercieront." }, { "title": "GUID vs ID auto-incrÃ©mentÃ© - dilemme et solutions en environnement .NET distribuÃ©", "url": "/posts/guid-versus-id/", "categories": "", "tags": "dotnet", "date": "2025-05-19 13:00:00 -0400", "snippet": "PrÃ©ambuleJe me souviens encore dâ€™une rÃ©union mouvementÃ©e avec un DBA, alors que nous discutions de lâ€™architecture dâ€™un nouveau systÃ¨me .NET rÃ©parti sur plusieurs services. Lorsque jâ€™ai osÃ© proposer...", "content": "PrÃ©ambuleJe me souviens encore dâ€™une rÃ©union mouvementÃ©e avec un DBA, alors que nous discutions de lâ€™architecture dâ€™un nouveau systÃ¨me .NET rÃ©parti sur plusieurs services. Lorsque jâ€™ai osÃ© proposer dâ€™utiliser des GUID comme clÃ©s primaires au lieu des habituels identifiants auto-incrÃ©mentÃ©s, jâ€™ai vu des yeux sâ€™Ã©cartiller. Â« Pourquoi changer une recette qui marche ? Â» mâ€™a-t-on lancÃ©. En bon dÃ©veloppeur, jâ€™avais moi-mÃªme toujours apprÃ©ciÃ© la simplicitÃ© des IDs sÃ©quentiels (1, 2, 3, â€¦). Mais dans un environnement distribuÃ© et asynchrone, cette vieille recette commenÃ§ait Ã  montrer ses limites.Dans cet article, je vous propose un retour dâ€™expÃ©rience sur ce choix technique. Nous verrons pourquoi les GUID peuvent sâ€™avÃ©rer pratiques (et comment les utiliser intelligemment), sans occulter les inconvÃ©nients bien rÃ©els qui rendent certains DBA mÃ©fiants. Installez-vous confortablement, on va parler dâ€™identifiants, de .NET/Entity Framework, dâ€™index, de sÃ©curitÃ©, de tests et de compromis astucieux.Le dilemme dans un monde distribuÃ©Identifiants auto-incrÃ©mentÃ©s ou GUID ? Le dÃ©bat est ancien. Dans un monde monolithique, les identifiants sÃ©quentiels (souvent de type int) brillent par leur simplicitÃ©, leur performance, et leur tri naturel. Mais dÃ¨s que le systÃ¨me devient distribuÃ©, asynchrone, ou sâ€™Ã©tend sur plusieurs bases de donnÃ©es, les problÃ¨mes commencent : collisions dâ€™identifiants, coordination nÃ©cessaire pour garantir lâ€™unicitÃ©, complexitÃ© lors des fusions de donnÃ©es ou synchronisations inter-sites.Les GUID (Globally Unique IDentifiers), quant Ã  eux, peuvent Ãªtre gÃ©nÃ©rÃ©s localement par lâ€™application, sans dÃ©pendance Ã  la base de donnÃ©es, tout en assurant une unicitÃ© quasi-absolue (2^128 combinaisons possibles).NouveautÃ©s en .NET 9 : les GUID version 7Depuis la version 9, .NET introduit la mÃ©thode Guid.CreateVersion7(), qui gÃ©nÃ¨re des GUID ordonnables chronologiquement. Cela rÃ©pond Ã  une critique historique des GUID : leur tendance Ã  provoquer de la fragmentation dans les index SQL.Les GUID v7 combinent un horodatage (48 bits) et des bits alÃ©atoires.On peut les utiliser avec :Guid guid7 = Guid.CreateVersion7();Ces nouveaux GUID sont plus efficaces pour les insertions en base de donnÃ©es, car ils rÃ©duisent significativement les fragmentations dâ€™index et offrent des performances comparables aux IDs sÃ©quentiels tout en conservant les avantages des GUID. Bien que EF Core ne les supporte pas encore nativement, il est possible de crÃ©er un gÃ©nÃ©rateur personnalisÃ© pour les utiliser efficacement.Avantages des GUID dans un systÃ¨me distribuÃ© UnicitÃ© globale : pas besoin de coordination entre les bases de donnÃ©es ou les services. GÃ©nÃ©ration cÃ´tÃ© client : utile pour le mode hors-ligne, les tests ou les systÃ¨mes Ã  haute disponibilitÃ©. ScalabilitÃ© : pas de bottleneck sur une table centrale. CompatibilitÃ© avec la rÃ©plication : les GUID sont idÃ©aux pour la rÃ©plication multi-site. Facilite les tests automatisÃ©s : permet de crÃ©er des identifiants prÃ©dictibles et stables entre exÃ©cutions. Uniformisation entre environnements : les mÃªmes identifiants peuvent Ãªtre utilisÃ©s en dev, test, staging ou production.Cas concrets Fusion de deux bases de donnÃ©es : sans GUID, risque de collision dâ€™IDs. Microservices : chaque service peut gÃ©rer ses propres identifiants. Tests automatisÃ©s : avec des GUID prÃ©visibles, les assertions et les fixtures deviennent plus stables. Alimentation entre environnements : les mÃªmes GUID peuvent Ãªtre utilisÃ©s pour des jeux de donnÃ©es identiques, simplifiant les dÃ©ploiements.ImplÃ©mentation en EF Corepublic class Commande { public Guid Id { get; set; } = Guid.NewGuid(); public string Description { get; set; } = \"\";}Pour utiliser un GUID sÃ©quentiel (cÃ´tÃ© SQL Server) :modelBuilder.Entity&lt;Commande&gt;() .Property(c =&gt; c.Id) .HasDefaultValueSql(\"NEWSEQUENTIALID()\");InconvÃ©nients des GUID Taille (16 octets vs 4 pour un int) Moins lisibles pour les humains Risque de fragmentation si non ordonnÃ©s LÃ©gÃ¨re surcharge en performance sur les jointuresDialogue avec les DBAs : trouver le bon Ã©quilibreTous les DBAs ne sont pas rÃ©fractaires au changement. Certains apportent mÃªme des solutions trÃ¨s pertinentes (index non cluster, fill factor adaptÃ©, etc.). Ce quâ€™ils attendent, câ€™est que les choix soient justifiÃ©s techniquement, mesurÃ©s, et documentÃ©s. Un DBA averti sera rassurÃ© de savoir que vous utilisez des GUID v7 plutÃ´t que des GUID alÃ©atoires classiques. Le dialogue avec eux permet de mettre en place une stratÃ©gie gagnant-gagnant.SÃ©curitÃ© des routes HTTP exposÃ©esUn autre avantage souvent oubliÃ© : la sÃ©curitÃ© par lâ€™obscurcitÃ©.Une route comme :GET /api/citoyens/5/enfantsest prÃ©visible. Un utilisateur malveillant peut tester /6, /7, etc., pour tenter dâ€™accÃ©der Ã  des donnÃ©es qui ne lui sont pas destinÃ©es. Câ€™est une attaque de type IDOR (Insecure Direct Object Reference).Avec des GUID :GET /api/citoyens/6f8d2ac1-3b90-4dc5-9543-f490a8f5d8c2/enfantslâ€™exploration devient presque impossible sans information prÃ©alable. Attention : cela ne remplace pas une vÃ©rification des droits, mais cela constitue une couche supplÃ©mentaire de protection, conforme aux bonnes pratiques OWASP. Cela dÃ©courage aussi les attaques automatisÃ©es.ConclusionUtiliser des GUID, ce nâ€™est pas une mode. Câ€™est une rÃ©ponse technique Ã  des besoins trÃ¨s concrets dans les architectures modernes. Il faut Ãªtre conscient des implications (taille, fragmentation, lisibilitÃ©), mais avec les GUID v7, les bonnes pratiques EF Core, une stratÃ©gie de tests solide, et une communication honnÃªte avec vos DBAs, ils peuvent devenir un atout puissant. Et dans un monde oÃ¹ la distribution, lâ€™asynchronisme, les tests dÃ©connectÃ©s, la sÃ©curitÃ© et lâ€™automatisation sont la norme, câ€™est un choix de plus en plus stratÃ©gique.Bon coding, et que vos identifiants restent uniques !" }, { "title": "Redis 8 - Un retour attendu Ã  lâ€™open source avec lâ€™AGPLv3", "url": "/posts/redis-redevient-open-source/", "categories": "outil-developpement", "tags": "", "date": "2025-05-04 19:00:00 -0400", "snippet": "Le 1er mai 2025, Redis a fait un virage majeur en revenant Ã  ses racines open source. Redis 8 est dÃ©sormais disponible sous la licence AGPLv3, approuvÃ©e par lâ€™Open Source Initiative (OSI). Ce chang...", "content": "Le 1er mai 2025, Redis a fait un virage majeur en revenant Ã  ses racines open source. Redis 8 est dÃ©sormais disponible sous la licence AGPLv3, approuvÃ©e par lâ€™Open Source Initiative (OSI). Ce changement intervient aprÃ¨s une pÃ©riode de turbulences sur le plan des licences, que jâ€™avais dÃ©jÃ  abordÃ©e dans un prÃ©cÃ©dent article.ğŸ“œ Une parenthÃ¨se fermÃ©e : de BSD Ã  SSPL, puis Ã  AGPLEn mars 2024, Redis avait troquÃ© sa licence permissive BSD pour un modÃ¨le dual : RSALv2 et SSPLv1. Lâ€™objectif Ã©tait de restreindre lâ€™exploitation commerciale sans contribution Ã©quitable, notamment par les grands fournisseurs cloud.Ce virage avait dÃ©plu Ã  la communautÃ©, provoquant lâ€™apparition de forks comme Valkey, soutenu par la Linux Foundation. En rÃ©ponse Ã  ces tensions, Redis 8 est dÃ©sormais aussi disponible sous AGPLv3, une licence rÃ©ellement open source, tout en conservant les autres options.ğŸ‘¨â€ğŸ’» Le retour de @antirez et les nouveautÃ©s de Redis 8Le retour de Salvatore Sanfilippo (@antirez), crÃ©ateur de Redis, a Ã©tÃ© un moment clÃ©. De retour chez Redis en novembre 2024 comme Ã©vangÃ©liste dÃ©veloppeur, il a participÃ© Ã  la rÃ©orientation du projet. Sous sa houlette, Redis 8 propose de vraies avancÃ©es techniques : Vector Sets : un nouveau type de donnÃ©es orientÃ© pour lâ€™IA et recherche vectorielle. Modules intÃ©grÃ©s directement dans Redis Core : JSON, Time Series, Bloom, Cuckoo, Count-min sketch, Top-K, t-digest, etc. AmÃ©liorations de performance : plus de 30 optimisations, avec jusquâ€™Ã  87â€¯% de gain de vitesse sur certaines commandes et un dÃ©bit doublÃ©.ğŸ’¡ Redis 8 sâ€™aligne donc sur les besoins modernes des dÃ©veloppeurs et des infrastructures.ğŸŒ Une communautÃ© rassurÃ©eLe retour Ã  une licence open source a Ã©tÃ© bien accueilli. Sanfilippo lâ€™a rÃ©sumÃ© simplement : Â« Je suis heureux que Redis soit Ã  nouveau un logiciel open source, sous les termes de la licence AGPLv3. Â»Ce geste tÃ©moigne dâ€™une volontÃ© de renouer avec les principes du logiciel libre, tout en soutenant une innovation continue.ğŸ¯ Ce quâ€™il faut retenir Redis revient Ã  lâ€™open source avec lâ€™AGPLv3. Redis 8 introduit des fonctionnalitÃ©s fortes pour lâ€™IA, des modules clÃ©s et des vraies optimisations. Câ€™est un geste dâ€™ouverture et de rÃ©conciliation avec la communautÃ© des dÃ©veloppeurs.ğŸ“š Pour creuser le sujet : Lâ€™annonce officielle : redis.io/blog/agplv3 Mon analyse du changement de licence de Redis en 2024 : Redis change de licence â€“ Pourquoi Ã§a fait dÃ©bat" }, { "title": "Interrupteurs de fonctionnalitÃ©", "url": "/posts/interrupteur-fonctionnalite/", "categories": "outil-developpement", "tags": "dotnet", "date": "2025-04-21 18:00:00 -0400", "snippet": "Quâ€™est-ce quâ€™un interrupteur de fonctionnalitÃ©Les applications cloud, les microservices et les pratiques DevOps misent sur la vitesse et lâ€™agilitÃ©. Les utilisateurs attendent de la rÃ©activitÃ©, des ...", "content": "Quâ€™est-ce quâ€™un interrupteur de fonctionnalitÃ©Les applications cloud, les microservices et les pratiques DevOps misent sur la vitesse et lâ€™agilitÃ©. Les utilisateurs attendent de la rÃ©activitÃ©, des nouveautÃ©s frÃ©quentes et peu (voire aucun) temps dâ€™arrÃªt.Les interrupteurs de fonctionnalitÃ© sont une technique moderne de dÃ©ploiement permettant de dÃ©coupler lâ€™activation dâ€™une fonctionnalitÃ© de son dÃ©ploiement. Une simple modification de configuration suffit Ã  activer une fonctionnalitÃ© pour certains utilisateurs ou environnements, sans redÃ©marrer ni redÃ©ployer lâ€™application (en thÃ©orie). âš¡ Ils permettent de dissocier le dÃ©ploiement de la mise en production.Cela permet notamment de tester des fonctionnalitÃ©s en production sur des groupes restreints dâ€™utilisateurs ou dâ€™activer graduellement une nouveautÃ©.Mise en place dans une application .NETInstallation des packages NuGet Pour tout projet : dotnet add package Microsoft.FeatureManagement Pour les APIs : dotnet add package Microsoft.FeatureManagement.AspNetCore Configuration minimaleusing Microsoft.FeatureManagement;public class Program { // ... builder.Services.AddFeatureManagement();}Utilisation du service IFeatureManagerUne fois la configuration en place, vous pouvez interroger facilement un interrupteur de fonctionnalitÃ© comme illustrÃ© dans lâ€™exemple suivant :public class MonService{ private readonly IFeatureManager _featureManager; public MonService(IFeatureManager featureManager) { _featureManager = featureManager; } public Task&lt;string&gt; ObtenirMessage() =&gt; _featureManager.IsEnabledAsync(\"NouvelleFonctionnalite\") ? \"Bienvenue avec la nouvelle fonctionnalitÃ©\" : \"Bienvenue\";}Sources de configurationFichier appsettings.json{ \"FeatureManagement\": { \"NouvelleFonctionnalite\": true }}Ou via une section personnalisÃ©e :{ \"MaSectionFlags\": { \"NouvelleFonctionnalite\": true }}services.AddFeatureManagement(Configuration.GetSection(\"MaSectionFlags\"));Variables dâ€™environnementFeatureManagement__NouvelleFonctionnalite=true âœ… Deux underscores requis entre la section et la clÃ©.Azure App Configuration NuGet : dotnet add package Microsoft.Extensions.Configuration.AzureAppConfiguration Configuration :config.AddAzureAppConfiguration(options =&gt;{ options.Connect(Environment.GetEnvironmentVariable(\"ConnectionString\")) .ConfigureRefresh(refresh =&gt; refresh.Register(\"Settings:Sentinel\", refreshAll: true) .SetCacheExpiration(TimeSpan.FromMinutes(1))) .UseFeatureFlags();});Cas dâ€™utilisationActivation conditionnelle dâ€™un contrÃ´leur ou endpoint[FeatureGate(FeatureFlags.NouvelleFonctionnalite)][ApiController][Route(\"api/[controller]\")]public class ExempleController : ControllerBase{ [HttpGet] [FeatureGate(FeatureFlags.NouvelleFonctionnalite)] public IActionResult Get() =&gt; Ok(\"AccÃ¨s autorisÃ©\");}Composant Blazor (WebAssembly)@if (estFonctionnaliteActive){ &lt;div&gt;La fonctionnalitÃ© est active.&lt;/div&gt;}[Inject] public required IFeatureManager FeatureManager { get; set; }private bool estFonctionnaliteActive;protected override async Task OnInitializedAsync(){ estFonctionnaliteActive = await FeatureManager.IsEnabledAsync(nameof(FeatureFlags.NouvelleFonctionnalite)); // ...}Redirection conditionnelle (ex. : vers une page 404)&lt;NotFound&gt; &lt;LayoutView Layout=\"@typeof(MainLayout)\"&gt; &lt;PageNonTrouvee /&gt; &lt;/LayoutView&gt;&lt;/NotFound&gt;Autres possibilitÃ©sLes interrupteurs ne se limitent pas Ã  un simple Ã©tat activÃ© ou dÃ©sactivÃ©. GrÃ¢ce aux filtres, il est possible dâ€™introduire des comportements plus dynamiques et contextuels. Interrupteurs avancÃ©s - Microsoft permet dâ€™ajouter des filtres personnalisÃ©s comme une activation basÃ©e sur un utilisateur, une rÃ©gion, etc. Interrupteurs personnalisÃ©s - Vous pouvez crÃ©er vos propres filtres.Bonnes pratiquesUtiliser un enum pour Ã©viter les chaÃ®nes magiquespublic enum FeatureFlags{ NouvelleFonctionnalite, FonctionnaliteB}await _featureManager.IsEnabledAsync(nameof(FeatureFlags.NouvelleFonctionnalite));Faire le mÃ©nage rÃ©guliÃ¨rement des interrupteursLes interrupteurs de fonctionnalitÃ© sont par nature temporaires. Une fois quâ€™une fonctionnalitÃ© est pleinement dÃ©ployÃ©e ou quâ€™un test A/B est terminÃ©, il est essentiel de retirer les interrupteurs associÃ©s du code et de la configuration.Laisser des interrupteurs inactifs ou non utilisÃ©s peut nuire Ã  la lisibilitÃ©, alourdir la maintenance, et introduire de lâ€™incertitude dans les comportements attendus de lâ€™application. ğŸ’¡ Astuce : tenez une liste des interrupteurs actifs avec leur date de crÃ©ation et lâ€™objectif visÃ©. Cela vous aidera Ã  planifier leur retrait Ã  temps.Comportement personnalisÃ© lorsquâ€™une action est dÃ©sactivÃ©ePar dÃ©faut, un contrÃ´leur ou action dÃ©sactivÃ© retourne un HTTP 404.Vous pouvez crÃ©er une classe personnalisÃ©e qui implÃ©mente IDisabledFeaturesHandler pour gÃ©rer un autre type de rÃ©ponse. ğŸ‘‰ Voir la documentation.ConclusionLes interrupteurs de fonctionnalitÃ© sont un levier puissant pour amÃ©liorer lâ€™agilitÃ© des Ã©quipes de dÃ©veloppement, rÃ©duire les risques liÃ©s aux dÃ©ploiements et faciliter lâ€™expÃ©rimentation de nouvelles fonctionnalitÃ©s. Leur intÃ©gration dans une application .NET est relativement simple grÃ¢ce au support natif offert par Microsoft.FeatureManagement.Cependant, comme tout outil, leur utilisation doit Ãªtre encadrÃ©e par des bonnes pratiques : Ã©viter lâ€™accumulation dâ€™interrupteurs obsolÃ¨tes, documenter leur usage, et rÃ©flÃ©chir Ã  une stratÃ©gie de configuration adaptÃ©e Ã  lâ€™Ã©chelle du projet (locale, via App Configuration, ou centralisÃ©e).Adopter les interrupteurs de fonctionnalitÃ©, câ€™est se donner les moyens de livrer plus rapidement, de maniÃ¨re plus sÃ»re, et avec une meilleure maÃ®trise du changement. Commencez petit, expÃ©rimentez, et adaptez votre approche Ã  vos besoins rÃ©els. Lâ€™essentiel est de rester intentionnel dans lâ€™usage que vous en faites." }, { "title": "Introduction Ã  K6", "url": "/posts/introduction-k6/", "categories": "outil-developpement", "tags": "k6, essais", "date": "2025-04-14 20:00:00 -0400", "snippet": "Quâ€™est-ce que k6 ?k6 est un outil open-source de test de charge et de performance, conÃ§u pour aider les dÃ©veloppeurs Ã  Ã©valuer la fiabilitÃ© de leurs systÃ¨mes, notamment les API, microservices et si...", "content": "Quâ€™est-ce que k6 ?k6 est un outil open-source de test de charge et de performance, conÃ§u pour aider les dÃ©veloppeurs Ã  Ã©valuer la fiabilitÃ© de leurs systÃ¨mes, notamment les API, microservices et sites web. DÃ©veloppÃ© par Grafana Labs, k6 permet de simuler des comportements utilisateur rÃ©alistes et dâ€™identifier les potentielles faiblesses avant le dÃ©ploiement en production.Pourquoi utiliser k6 ? Si vous Ãªtes habituÃ© Ã  Apache JMeter, k6 vous apportera une approche plus moderne et dÃ©veloppeur-friendly avec des tests Ã©crits en JavaScript. ğŸ™Œ Son CLI facilite son intÃ©gration dans les pipelines dâ€™intÃ©gration continue. CapacitÃ© Ã  gÃ©nÃ©rer des rapports dÃ©taillÃ©s sur les performances, aidant Ã  identifier les goulets dâ€™Ã©tranglement et les points Ã  optimiser.Types dâ€™essais de charge ğŸ”¼ Test de montÃ©e en charge (Ramp-up test) - Augmente progressivement le nombre dâ€™utilisateurs pour voir Ã  quel moment lâ€™application commence Ã  ralentir. ğŸ’¥ Test de stress (Stress test) - Envoie plus de requÃªtes que la capacitÃ© normale pour voir comment lâ€™application rÃ©agit sous la surcharge. â³ Test dâ€™endurance (Soak test) -Â VÃ©rifie si lâ€™application reste stable aprÃ¨s plusieurs heures/jours sous charge continue. âš¡ Test de pointe (Spike test) -Â Simule une augmentation soudaine du trafic pour voir si le systÃ¨me peut absorber les pics de charge.La liste ne sâ€™arrÃªte pas lÃ  ! Il existe Ã©galement dâ€™autres types de tests complÃ©mentaires, comme :Â  ğŸ§¨ Tests de chaos et de rÃ©silience (Chaos and Resilience Testing) -Â Simulent des pannes ou des conditions extrÃªmes pour Ã©valuer la capacitÃ© dâ€™un systÃ¨me Ã  rÃ©sister aux dÃ©faillances et Ã  se rÃ©tablir automatiquement. ğŸ–§ Tests dâ€™infrastructure (Infrastructure Testing) -Â VÃ©rifient la performance et la fiabilitÃ© des composants sous-jacents tels que les serveurs, les bases de donnÃ©es, les rÃ©seaux et le stockage cloud. Ils aident Ã  identifier les goulets dâ€™Ã©tranglement et Ã  optimiser les ressources.Â  ğŸŒ Tests de performance du navigateur (Browser Performance Testing) -Â Mesurent la vitesse dâ€™affichage et le temps de chargement des pages web du point de vue de lâ€™utilisateur final. Ces tests permettent dâ€™optimiser les performances cÃ´tÃ© client et dâ€™amÃ©liorer lâ€™expÃ©rience utilisateur. Outils populaires : Lighthouse et k6 Browser.Â  ğŸ’¡ En combinant ces diffÃ©rents types de tests, on sâ€™assure dâ€™une application robuste, performante et rÃ©siliente, prÃªte Ã  affronter toutes les conditions !Il est essentiel dâ€™Ã©valuer le retour sur investissement (ROI) avant de multiplier les tests : toutes les applications nâ€™ont pas besoin de tests de charge, de rÃ©silience ou dâ€™infrastructure avancÃ©s. Lâ€™important est dâ€™adapter la stratÃ©gie de test aux risques et aux exigences de lâ€™application pour Ã©viter des efforts inutiles.Mise en place de k6k6 est multiplateforme et compatible avec Windows, Linux et macOS. Pour lâ€™installer, utilisez le gestionnaire de paquets adaptÃ© Ã  votre systÃ¨me dâ€™exploitation.Sur Windows :winget install k6 --source winget ğŸ’¡Notons Ã©galement quâ€™il est possible dâ€™exÃ©cuter k6 depuis un conteneur.Lancer un simple test de charge sur une API RESTFaire un test de charge sur une API REST (ex: https://example.com/api/products) pour simuler 10 utilisateurs virtuels pendant 30 secondes.Script test.js :import http from 'k6/http';import { sleep, check } from 'k6';export const options = { vus: 10, // 10 utilisateurs virtuels duration: '30s', // pendant 30 secondes};export default function () { const res = http.get('https://example.com/api/products'); check(res, { 'status est 200': (r) =&gt; r.status === 200, 'rÃ©ponse contient produits': (r) =&gt; r.body.includes('product'), }); sleep(1);}ExÃ©cution du script :k6 run test.jsğŸ™Œ Tu verras des statistiques en temps rÃ©el comme le nombre de requÃªtes rÃ©ussies, les Ã©checs, le temps de rÃ©ponse moyen, etc.Comprendre les rÃ©sultats dâ€™exÃ©cution de k6Lorsquâ€™un test de charge est exÃ©cutÃ© avec k6, un rapport dÃ©taillÃ© est gÃ©nÃ©rÃ© en console, affichant plusieurs indicateurs clÃ©s de performance. Ces mÃ©triques permettent dâ€™Ã©valuer la stabilitÃ©, la rapiditÃ© et la robustesse du systÃ¨me testÃ©.Â  Â RÃ©sumÃ© des mÃ©triques principalesÂ Ã€ la fin dâ€™un test, k6 affiche un tableau de rÃ©sultats contenant les indicateurs suivants :Â  Â  http_reqs : Nombre total de requÃªtes HTTP effectuÃ©es pendant le test.Â  http_req_duration : Temps moyen de rÃ©ponse des requÃªtes HTTP, gÃ©nÃ©ralement mesurÃ© en millisecondes.Â  http_req_failed : Pourcentage de requÃªtes ayant Ã©chouÃ©.Â  vus (Virtual Users) : Nombre dâ€™utilisateurs simultanÃ©s simulÃ©s Ã  un instant donnÃ©.Â  iterations : Nombre total dâ€™itÃ©rations de script exÃ©cutÃ©es.Analyse des temps de rÃ©ponseLâ€™un des Ã©lÃ©ments les plus critiques est le http_req_duration, qui mesure le temps de rÃ©ponse des requÃªtes.k6 fournit des valeurs utiles comme :Â  Moyenne (avg) : Temps de rÃ©ponse moyen sur lâ€™ensemble des requÃªtes.Â  MÃ©diane (med) : Le temps de rÃ©ponse qui sÃ©pare la moitiÃ© des requÃªtes les plus rapides des plus lentes.Â  95e percentile (p(95)) : 95% des requÃªtes ont eu un temps de rÃ©ponse infÃ©rieur Ã  cette valeur.Â  Maximum (max) : Le temps de rÃ©ponse le plus Ã©levÃ© observÃ©.Â  Â Ces donnÃ©es aident Ã  identifier des problÃ¨mes comme des pics de latence ou des temps de rÃ©ponse anormalement longs.VÃ©rification des Ã©checs et des erreursÂ Lâ€™indicateur http_req_failed permet de voir si certaines requÃªtes ont Ã©chouÃ© (timeouts, erreurs serveur, etc.). Un taux Ã©levÃ© peut indiquer une saturation du backend ou une mauvaise configuration de lâ€™application.Â  Â InterprÃ©tation des tendances et optimisations possiblesÂ  Si le temps de rÃ©ponse mÃ©dian est bas, mais le max est Ã©levÃ©, cela signifie que certaines requÃªtes subissent des latences importantes, peut-Ãªtre dues Ã  un goulot dâ€™Ã©tranglement.Â  Un taux dâ€™Ã©chec Ã©levÃ© peut indiquer un problÃ¨me de scalabilitÃ© ou un manque de ressources cÃ´tÃ© serveur.Â  Si la charge CPU ou mÃ©moire du serveur augmente rapidement, il pourrait Ãªtre utile de mettre en place un autoscaling ou dâ€™optimiser les requÃªtes et le caching.Â ConclusionDans un contexte oÃ¹ les performances applicatives sont de plus en plus scrutÃ©es, k6 sâ€™impose comme un outil moderne, lÃ©ger et puissant pour rÃ©aliser des essais de charge adaptÃ©s aux rÃ©alitÃ©s dâ€™aujourdâ€™hui. Que ce soit pour valider la scalabilitÃ© dâ€™une API, dÃ©tecter des goulets dâ€™Ã©tranglement, ou sâ€™assurer que lâ€™expÃ©rience utilisateur reste fluide en pÃ©riode de pointe, k6 offre une solution accessible aussi bien aux dÃ©veloppeurs quâ€™aux Ã©quipes DevOps.GrÃ¢ce Ã  sa syntaxe en JavaScript, son intÃ©gration facile dans les pipelines CI/CD, et la richesse des mÃ©triques fournies, il devient un excellent alliÃ© pour intÃ©grer les tests de performance dÃ¨s les premiÃ¨res Ã©tapes du cycle de dÃ©veloppement â€” un des piliers des pratiques modernes comme le Shift Left Testing.Il est cependant essentiel de rappeler que tous les projets ne nÃ©cessitent pas le mÃªme niveau de rigueur en matiÃ¨re de tests de performance. Adapter les types dâ€™essais (stress, endurance, montÃ©e en chargeâ€¦) aux enjeux mÃ©tier, Ã  lâ€™infrastructure cible et aux attentes des utilisateurs permet dâ€™optimiser lâ€™effort tout en maximisant le retour sur investissement.En rÃ©sumÃ©, que vous souhaitiez prÃ©venir les mauvaises surprises en production ou simplement renforcer la rÃ©silience de vos systÃ¨mes, k6 est un excellent point de dÃ©part pour professionnaliser vos essais de charge. Et comme toujours : testez tÃ´t, testez souvent, et testez intelligemment." }, { "title": "Redis change de licence ! Quelles rÃ©percussions pour les dÃ©veloppeurs et les entreprises ?", "url": "/posts/redis-change-licence/", "categories": "outil-developpement", "tags": "", "date": "2025-04-06 20:00:00 -0400", "snippet": "Depuis son lancement, Redis sâ€™est imposÃ© comme lâ€™un des outils de base de donnÃ©es en mÃ©moire les plus populaires. Que ce soit pour le caching, la gestion de files dâ€™attente ou mÃªme lâ€™implÃ©mentation...", "content": "Depuis son lancement, Redis sâ€™est imposÃ© comme lâ€™un des outils de base de donnÃ©es en mÃ©moire les plus populaires. Que ce soit pour le caching, la gestion de files dâ€™attente ou mÃªme lâ€™implÃ©mentation de structures de donnÃ©es complexes, Redis a Ã©tÃ© un choix de prÃ©dilection pour de nombreuses entreprises et dÃ©veloppeurs. Cependant, un changement de licence soulÃ¨ve des questions : quelles seront les implications pour ceux qui utilisent Redis au quotidien ? Et comment les alternatives comme Garnet de Microsoft pourraient-elles sâ€™intÃ©grer dans cet Ã©cosystÃ¨me en pleine mutation ?Changement de licence : Quâ€™est-ce qui a changÃ© ?Redis, historiquement disponible sous licence BSD, a modifiÃ© les conditions dâ€™utilisation de certains modules via la Redis Source Available License (RSAL). Ce changement vise Ã  limiter lâ€™utilisation des modules spÃ©cifiques Ã  Redis dans des solutions commerciales fournies en tant que service (Software-as-a-Service). Lâ€™objectif principal derriÃ¨re ce changement est de protÃ©ger Redis des grands acteurs du cloud, tels quâ€™Amazon Web Services, qui bÃ©nÃ©ficiaient des modules Redis sans contribuer en retour au projet ou Ã  son dÃ©veloppement (Microsoft soutient lâ€™utilisation de Redis Ã  travers plusieurs initiatives).Voici quelques points clÃ©s Ã  retenir : RSAL sâ€™applique uniquement Ã  certains modules Redis (comme RedisSearch ou RedisJSON), mais pas au cÅ“ur de Redis, qui reste sous licence BSD. Les entreprises peuvent toujours utiliser ces modules gratuitement pour une utilisation interne ou dans des produits on-premise. Cependant, la vente de services SaaS basÃ©s sur ces modules nÃ©cessite dÃ©sormais une licence commerciale.RÃ©percussions pour les dÃ©veloppeurs et les entreprisesPour les dÃ©veloppeurs travaillant sur des projets open source ou des applications internes, le changement est relativement mineur. Toutefois, pour les entreprises qui intÃ¨grent Redis dans des services cloud commercialisÃ©s, cela peut entraÃ®ner des coÃ»ts supplÃ©mentaires et des obligations contractuelles. Les startups et les PME utilisant Redis dans des environnements cloud doivent maintenant Ã©valuer attentivement leur conformitÃ© Ã  la nouvelle licence.Impacts potentiels : CoÃ»ts accrus : Les entreprises SaaS pourraient voir leurs frais augmenter si elles choisissent dâ€™intÃ©grer des modules Redis sous RSAL dans leurs solutions. Alternative Ã  considÃ©rer : Les projets peuvent chercher Ã  remplacer certains modules Redis par des solutions open source alternatives ou Ã  dÃ©velopper leurs propres implÃ©mentations. Ã‰cosystÃ¨me cloud : Les grands fournisseurs cloud, comme AWS, peuvent se tourner vers des forks de Redis ou encourager lâ€™adoption de solutions alternatives comme ElastiCache, leur version propriÃ©taire du service Redis.Garnet : Une alternative prometteuse ?En parallÃ¨le de ce changement de licence, de nouveaux outils Ã©mergent, notamment Garnet de Microsoft, qui pourrait devenir une alternative sÃ©rieuse Ã  Redis dans certains cas dâ€™usage. Garnet est un projet conÃ§u pour Ãªtre une base de donnÃ©es distribuÃ©e ultra-rapide, optimisÃ©e pour les applications cloud natives, et offrant des fonctionnalitÃ©s qui rivalisent avec Redis.Il se distingue par : Une intÃ©gration native avec Azure : Garnet est conÃ§u pour tirer parti de lâ€™infrastructure Azure, ce qui en fait un choix naturel pour les entreprises dÃ©jÃ  investies dans cet Ã©cosystÃ¨me. Une approche axÃ©e sur la performance : Tout comme Redis, Garnet est conÃ§u pour des temps de rÃ©ponse trÃ¨s bas, ce qui le rend idÃ©al pour des applications nÃ©cessitant des donnÃ©es en temps rÃ©el. FlexibilitÃ© des licences : Microsoft pourrait proposer une politique de licence plus flexible, qui pourrait Ãªtre attrayante pour les entreprises concernÃ©es par la RSAL de Redis. CompatibilitÃ© avec le contrat Redis : Garnet est entiÃ¨rement compatible avec le contrat Redis, facilitant ainsi son adoption pour les entreprises qui utilisent dÃ©jÃ  cette technologie.ConclusionLe changement de licence de Redis reprÃ©sente une Ã©volution significative dans la gestion des outils open source dans le cloud. Si la RSAL peut offrir une meilleure protection pour Redis Labs contre les gÃ©ants du cloud, elle pousse Ã©galement les entreprises et dÃ©veloppeurs Ã  rÃ©Ã©valuer leur dÃ©pendance Ã  ces outils. Les alternatives comme Garnet de Microsoft, ainsi que dâ€™autres bases de donnÃ©es en mÃ©moire, vont probablement gagner en popularitÃ© Ã  mesure que lâ€™Ã©cosystÃ¨me continue dâ€™Ã©voluer.Pour les dÃ©veloppeurs et entreprises, lâ€™essentiel est de suivre de prÃ¨s ces changements et dâ€™Ã©valuer les coÃ»ts et les bÃ©nÃ©fices de chaque solution pour leur propre architecture. Le choix de rester avec Redis, dâ€™explorer Garnet ou mÃªme dâ€™adopter une autre solution dÃ©pendra largement des besoins spÃ©cifiques et des contraintes budgÃ©taires de chaque projet.Ce changement de paradigme autour de Redis reflÃ¨te un mouvement plus large dans lâ€™Ã©cosystÃ¨me open source oÃ¹ de plus en plus de projets cherchent Ã  protÃ©ger leur modÃ¨le Ã©conomique tout en continuant Ã  offrir des solutions robustes pour les utilisateurs finaux.ğŸ’¡DÃ©couvrez en vidÃ©o lâ€™impact du changement de licence de Redis." }, { "title": "AmÃ©lioration des performances en utilisant JsonSerializerOptions en singleton", "url": "/posts/performances-jsonserializeroptions-singleton/", "categories": "", "tags": "dotnet", "date": "2025-03-20 20:00:00 -0400", "snippet": "IntroductionRÃ©cemment, jâ€™ai donnÃ© une formation interne oÃ¹ je partageais diffÃ©rentes astuces pour amÃ©liorer les performances dâ€™applications en .NET. Parmi ces astuces, une technique particuliÃ¨remen...", "content": "IntroductionRÃ©cemment, jâ€™ai donnÃ© une formation interne oÃ¹ je partageais diffÃ©rentes astuces pour amÃ©liorer les performances dâ€™applications en .NET. Parmi ces astuces, une technique particuliÃ¨rement efficace concernait lâ€™utilisation de JsonSerializerOptions en singleton.Historiquement, la sÃ©rialisation dâ€™objets en .NET a souvent Ã©tÃ© rÃ©alisÃ©e avec le NuGet Newtonsoft. Toutefois, depuis lâ€™introduction de System.Text.Json en .NET Core 3.0, une alternative plus performante et mieux intÃ©grÃ©e au runtime est devenue disponible. Bien que System.Text.Json soit gÃ©nÃ©ralement plus rapide que Newtonsoft, son efficacitÃ© peut Ãªtre compromise par une mauvaise gestion des options de sÃ©rialisation (JsonSerializerOptions). Lorsque ces options sont recrÃ©Ã©es Ã  chaque appel au lieu dâ€™Ãªtre rÃ©utilisÃ©es, cela peut entraÃ®ner une augmentation notable des allocations mÃ©moire et une dÃ©gradation des performances.Dans cet article, nous allons dÃ©montrer lâ€™impact de lâ€™utilisation dâ€™une instance singleton de JsonSerializerOptions par rapport Ã  la crÃ©ation dâ€™options Ã  chaque appel. Nous allons utiliser BenchmarkDotNet pour comparer les deux mÃ©thodes et mettre en Ã©vidence les gains potentiels en matiÃ¨re de performances.ContexteLâ€™analyseur CA1869 recommande dâ€™utiliser une instance de JsonSerializerOptions en singleton pour optimiser les performances. Cela est particuliÃ¨rement pertinent lorsquâ€™une configuration personnalisÃ©e est requise, comme lâ€™utilisation de PropertyNamingPolicy ou PropertyNameCaseInsensitive.Configuration de lâ€™analyse de performancesPour mesurer les performances, nous utilisons le code suivant qui compare deux mÃ©thodes : SerializeSansSingleton() : CrÃ©e une nouvelle instance de JsonSerializerOptions Ã  chaque sÃ©rialisation. SerializeAvecSingleton() : RÃ©utilise une instance singleton prÃ©configurÃ©e (JsonSerializerOptions.Web) pour chaque sÃ©rialisation.Voici le code complet :using BenchmarkDotNet.Attributes;using BenchmarkDotNet.Columns;using BenchmarkDotNet.Order;using System.Text.Json;namespace TestBenchmarkDotnet.TestJsonSerializerOptions;[MemoryDiagnoser][Orderer(SummaryOrderPolicy.FastestToSlowest)][RankColumn][HideColumns(Column.Error, Column.Median, Column.RatioSD, Column.StdDev)]public class BenchmarkJsonSerializerOptions{ private readonly TestModel _testModel = new() { Id = 1, Nom = \"Test\", Description = \"This is a test description\", EstActif = true }; [Params(1, 100)] public int Iteration { get; set; } [Benchmark(Baseline = true)] public string SerializeSansSingleton() { var json = \"\"; for (var i = 0; i &lt; Iteration; i++) { var options = new JsonSerializerOptions { PropertyNamingPolicy = JsonNamingPolicy.CamelCase, PropertyNameCaseInsensitive = true }; json = JsonSerializer.Serialize(_testModel, options); } return json; } [Benchmark] public string SerializeAvecSingleton() { var json = \"\"; for (var i = 0; i &lt; Iteration; i++) { json = JsonSerializer.Serialize(_testModel, JsonSerializerOptions.Web); } return json; }}public class TestModel{ public int Id { get; set; } public required string Nom { get; set; } public required string Description { get; set; } public bool EstActif { get; set; }}RÃ©sultats des benchmarksLâ€™exÃ©cution des benchmarks a produit les rÃ©sultats suivants :| MÃ©thode | ItÃ©rations | DurÃ©e moyenne | Ratio | Allocations Gen0 | MÃ©moire allouÃ©e ||------------------------|------------|---------------|-------|------------------|-----------------|| SerializeAvecSingleton | 1 | 205.6 ns | 0.24 | 0.0219 | 184 B || SerializeSansSingleton | 1 | 861.3 ns | 1.00 | 0.0429 | 390 B || SerializeAvecSingleton | 100 | 21,592.5 ns | 0.32 | 2.1973 | 18400 B || SerializeSansSingleton | 100 | 67,384.9 ns | 1.00 | 4.1504 | 39035 B |Analyse des rÃ©sultatsLes rÃ©sultats montrent clairement quâ€™en utilisant une instance singleton de JsonSerializerOptions : La mÃ©thode est environ 4 fois plus rapide pour une seule itÃ©ration. Les allocations mÃ©moire sont rÃ©duites de plus de 50 %, ce qui est significatif pour des traitements en masse.Lâ€™approche utilisant JsonSerializerOptions.Web rÃ©utilise la configuration en singleton, ce qui permet dâ€™Ã©viter la crÃ©ation dâ€™objets inutiles et amÃ©liore ainsi les performances de faÃ§on considÃ©rable.ConclusionLâ€™utilisation dâ€™une instance singleton pour JsonSerializerOptions est une bonne pratique qui amÃ©liore non seulement les performances en termes de temps dâ€™exÃ©cution, mais rÃ©duit Ã©galement les allocations mÃ©moire. Cela est particuliÃ¨rement bÃ©nÃ©fique dans les scÃ©narios oÃ¹ les opÃ©rations de sÃ©rialisation sont rÃ©pÃ©tÃ©es de maniÃ¨re intensive.Lâ€™analyseur CA1869 dans .NET fournit des recommandations qui mÃ©ritent dâ€™Ãªtre suivies pour Ã©viter les piÃ¨ges courants en matiÃ¨re de performances.En rÃ©sumÃ©, si votre application effectue des sÃ©rialisations frÃ©quentes avec des configurations spÃ©cifiques, envisagez dâ€™utiliser une instance singleton pour vos JsonSerializerOptions. ğŸ’¡ Astuces supplÃ©mentaires : Pour vous assurer que lâ€™analyseur CA1869 est actif dans votre projet, vous pouvez lâ€™ajouter dans votre fichier .editorconfig comme suit dotnet_diagnostic.CA1869.severity = warning." }, { "title": "Veille technologique - Pourquoi et comment rester Ã  jour", "url": "/posts/veille-technologique/", "categories": "", "tags": "", "date": "2025-02-26 18:00:00 -0500", "snippet": "La veille technologique est un processus continu de surveillance et dâ€™analyse des Ã©volutions dans un domaine technologique donnÃ©. Elle consiste Ã  collecter, analyser et exploiter des informations s...", "content": "La veille technologique est un processus continu de surveillance et dâ€™analyse des Ã©volutions dans un domaine technologique donnÃ©. Elle consiste Ã  collecter, analyser et exploiter des informations sur les nouvelles tendances, les innovations, les Ã©volutions des standards et les meilleures pratiques afin dâ€™anticiper les changements et dâ€™adapter ses stratÃ©gies en consÃ©quence.ğŸ“Œ Pourquoi faire de la veille technologique ? Rester Ã  jour â€“ La technologie Ã©volue rapidement, surtout dans des domaines comme le cloud, lâ€™intelligence artificielle ou le dÃ©veloppement logiciel. Anticiper les Ã©volutions â€“ Identifier les tendances Ã©mergentes pour prendre de lâ€™avance sur la concurrence. AmÃ©liorer la prise de dÃ©cision â€“ Adopter les bonnes technologies en connaissance de cause. Optimiser les performances â€“ Tirer parti des nouvelles pratiques et outils pour amÃ©liorer la productivitÃ© et la qualitÃ©. RÃ©duire les risques â€“ Ã‰viter dâ€™investir dans des technologies obsolÃ¨tes ou inadaptÃ©es.ğŸ” Comment faire une veille efficace ?âœ… Suivre des sources fiables : Blogs spÃ©cialisÃ©s, sites dâ€™Ã©diteurs (Microsoft, AWS, Google Cloud), communautÃ©s GitHub, newsletters techniques, etc.âœ… Participer aux Ã©vÃ©nements : ConfÃ©rences (Microsoft Ignite, .NET Conf), meetups, webinaires.âœ… Tester les nouvelles technologies : ExpÃ©rimenter avec des Proof of Concepts (PoC).âœ… Ã‰changer avec la communautÃ© : Forums, LinkedIn, X/Twitter et Discord sont des mines dâ€™informations.âœ… Automatiser sa veille : Utiliser des outils comme Feedly, GitHub Trending et Twitter Lists.ğŸ“¡ OÃ¹ je fais ma veille technologique ?ğŸ¥ Mes chaÃ®nes YouTube favoritesYouTube est une ressource incontournable pour apprendre et explorer de nouvelles technologies.Voici quelques crÃ©ateurs de contenu que je suis : Nick Chapsas - ChaÃ®ne YouTube Milan JovanoviÄ‡ - ChaÃ®ne YouTube CodeOpinion (Derek Comartin) - ChaÃ®ne YouTube Shawn Wildermuth - ChaÃ®ne YouTube IAmTimCorey (Tim Corey) - ChaÃ®ne YouTube Zoran Horvat - ChaÃ®ne YouTube Fireship - ChaÃ®ne YouTube Patrick God - ChaÃ®ne YouTube Microsoft Visual Studio - ChaÃ®ne YouTube Microsoft Azure Developers - ChaÃ®ne YouTube Microsoft Developer - ChaÃ®ne YouTube Christopher Okhravi - ChaÃ®ne YouTube TechWorld with Nana - ChaÃ®ne YouTubeğŸ‘¥ Experts et crÃ©ateurs de contenu Ã  suivreJe suis Ã©galement plusieurs experts et crÃ©ateurs de contenu sur des plateformes comme X, LinkedIn et dâ€™autres rÃ©seaux sociaux. Ils partagent rÃ©guliÃ¨rement des insights et des ressources sur le dÃ©veloppement et les technologies Microsoft. Nick Chapsas : LinkedIn Milan JovanoviÄ‡ : LinkedIn Dave Callan : LinkedIn Oleg Kyrylchuk : LinkedIn Saeed Esmaeelinejad : LinkedIn Stefan ÄokiÄ‡ : LinkedIn Nikola KneÅ¾eviÄ‡ LinkedIn Mukesh Murugan : LinkedIn Nick Cosentino : LinkedIn Microsoft Visual Studio (Showcase) : LinkedIn GitHub (Entreprise) : LinkedIn Brigit Murtaugh : LinkedIn Rhea Patel : LinkedInğŸ“š Articles et blogs Ã  suivreJâ€™utilise Feedly pour suivre et organiser mes lectures.Voici une liste de blogs et articles que je consulte rÃ©guliÃ¨rement : Scott Hanselmanâ€™s Blog : Blog Thomas Claudius Huber : Blog Cezary PiÄ…tek Blog : Blog Youâ€™ve Been Haacked : Blog Jimmy Bogard : Blog Shawn Wildermuthâ€™s Blog : Blog Martin Fowler : Blog Andrew Lock - .NET Escapades : Blog ploeh blog : Blog Flavio Copes : Blog Rick Strahlâ€™s Web Log : Blog dotNetTips.com : Blog Dev Leader : Blog Jon Skeetâ€™s coding : Blog CodeOpinion : Blog Visual Studio Blog : Blog Azure DevOps Blog : Blog Visual Studio Code - Code Editing. Redefined. : Blog Microsoft for Developers : Blog PowerShell Team : Blog Windows Blog : Blog Docker : Blog AWS News Blog : Blog Engineering@Microsoft : Blog AWS Architecture Blog : Blog Engineering at Meta : Blog Netflix TechBlog - Medium : Blog freeCodeCamp Programming Tutorials: Python, JavaScript, Git &amp; More : Blog Dropbox Tech Blog : Blog Engineering Blog : Blog Stack Overflow Blog : Blog The Airbnb Tech Blog - Medium : Blog The latest from GitHubâ€™s engineering team - The GitHub Blog : Blog Pinterest Engineering Blog - Medium : Blog Stripe Blog : Blog Spotify Engineering : Blog Postman Blog : Blog Engineering at Slack : Blog Discord Inc. Press Releases : BlogEn conclusionCette liste nâ€™est pas figÃ©e et Ã©volue en fonction de mes intÃ©rÃªts et des tendances du marchÃ©. Lâ€™objectif est de rester Ã  jour sur les meilleures pratiques et les innovations technologiques, notamment dans lâ€™Ã©cosystÃ¨me .NET et le cloud." }, { "title": "Mon parcours et conseils pour la relÃ¨ve", "url": "/posts/parcours-conseils-releve/", "categories": "", "tags": "", "date": "2025-02-13 18:58:00 -0500", "snippet": "RÃ©cemment, jâ€™ai eu lâ€™opportunitÃ© de participer Ã  un webinaire destinÃ© aux jeunes souhaitant dÃ©couvrir les mÃ©tiers des technologies de lâ€™information. Cet Ã©change a Ã©tÃ© lâ€™occasion de rÃ©pondre Ã  plusi...", "content": "RÃ©cemment, jâ€™ai eu lâ€™opportunitÃ© de participer Ã  un webinaire destinÃ© aux jeunes souhaitant dÃ©couvrir les mÃ©tiers des technologies de lâ€™information. Cet Ã©change a Ã©tÃ© lâ€™occasion de rÃ©pondre Ã  plusieurs questions sur mon parcours, mon rÃ´le en tant quâ€™architecte logiciel, et les opportunitÃ©s dans le domaine des TI. Voici un rÃ©sumÃ© des Ã©changes qui, je lâ€™espÃ¨re, pourront inspirer celles et ceux qui envisagent une carriÃ¨re en informatique.Pourquoi est-il important pour moi de participer Ã  JeunesExplo ?En tant quâ€™architecte logiciel et responsable de lâ€™axe Â« Pratiques logicielles et modernisation Â» au sein du Centre dâ€™Excellence Azure chez Cofomo QuÃ©bec, je crois fermement Ã  lâ€™importance de transmettre mon expÃ©rience et dâ€™inspirer la nouvelle gÃ©nÃ©ration. Participer Ã  JeunesExplo, câ€™est une occasion unique dâ€™Ã©changer avec des jeunes curieux et ambitieux, de leur montrer la diversitÃ© des carriÃ¨res en TI, et de les encourager Ã  explorer ce secteur en pleine transformation.Lâ€™informatique est omniprÃ©sente dans nos vies et impacte tous les secteurs : santÃ©, finance, jeux vidÃ©o, Ã©ducation, administration publiqueâ€¦ Il est donc essentiel de faire dÃ©couvrir ces opportunitÃ©s et de dÃ©montrer que chacun peut y trouver sa place.Mon mÃ©tier : Architecte logicielUne journÃ©e typique dans mon travailChaque journÃ©e commence par une mÃªlÃ©e quotidienne (daily scrum) avec mon Ã©quipe. Câ€™est un moment clÃ© oÃ¹ chacun partage sur quoi il travaille, Ã©voque ses avancÃ©es et mentionne dâ€™Ã©ventuels blocages nÃ©cessitant de lâ€™aide. Cette synchronisation rapide permet dâ€™assurer une bonne coordination et de dÃ©celer les obstacles dÃ¨s le dÃ©but de la journÃ©e.Ensuite, nous passons en revue les prioritÃ©s pour nous assurer que nos objectifs restent bien alignÃ©s avec les attentes du projet et des parties prenantes.Mon rÃ´le au quotidien implique : La conception dâ€™architectures logicielles robustes et Ã©volutives, en tenant compte des aspects de performance, de sÃ©curitÃ© et de maintenabilitÃ©. La collaboration avec les dÃ©veloppeurs pour rÃ©soudre des problÃ¨mes techniques, effectuer des revues de code et accompagner lâ€™adoption de nouvelles technologies. Des tÃ¢ches stratÃ©giques, telles que la prÃ©paration de prÃ©sentations, la coordination avec dâ€™autres Ã©quipes ou encore lâ€™analyse de performance, afin de garantir une expÃ©rience utilisateur optimale dans nos applications cloud.Cette approche me permet dâ€™assurer Ã  la fois un suivi opÃ©rationnel efficace et une vision globale pour lâ€™Ã©volution des projets.Les outils que jâ€™utiliseMon travail au quotidien repose sur plusieurs outils, qui mâ€™aident Ã  concevoir des solutions performantes, collaborer avec mon Ã©quipe et assurer un haut niveau de qualitÃ© dans nos livrables.Voici quelques-uns de mes incontournables : Visual Studio â€“ Mon IDE principal pour dÃ©velopper en .NET. Il offre un environnement riche pour Ã©crire du code, exÃ©cuter des tests unitaires, profiler les performances et tirer parti de fonctionnalitÃ©s avancÃ©es comme IntelliSense, lâ€™intÃ©gration Git et Github Copilot. Azure DevOps â€“ UtilisÃ© pour la gestion des pipelines CI/CD, le suivi des tÃ¢ches via les tableaux Kanban, et la collaboration sur le code avec les pull requests. Il nous permet dâ€™automatiser les dÃ©ploiements et dâ€™assurer un dÃ©veloppement fluide et structurÃ©. BenchmarkDotNet â€“ Indispensable pour mesurer et comparer les performances du code .NET. Je lâ€™utilise notamment pour analyser lâ€™impact des optimisations et valider les choix architecturaux avant leur mise en place. Windows Terminal â€“ Mon terminal par dÃ©faut pour exÃ©cuter des commandes et scripts. Stryker.NET â€“ Un outil que jâ€™apprÃ©cie pour les essais de mutation, permettant dâ€™amÃ©liorer la robustesse de nos tests unitaires et de sâ€™assurer que notre code est rÃ©ellement testÃ© en profondeur.Quelle formation pour devenir architecte logiciel ?Le parcours typique passe par un baccalaurÃ©at en informatique ou en gÃ©nie logiciel, mais ce nâ€™est pas une voie obligatoire. Jâ€™ai moi-mÃªme suivi un parcours atypique, obtenant un diplÃ´me dâ€™Ã©tudes collÃ©giales en informatique au CÃ©gep de Matane.Lâ€™essentiel dans ce domaine, câ€™est lâ€™auto-formation, la pratique sur des projets concrets, et la capacitÃ© Ã  apprendre continuellement. Les certifications jouent un rÃ´le clÃ© dans plusieurs spÃ©cialisations, bien que leur importance varie selon le domaine. Certaines branches, comme la cybersÃ©curitÃ© ou lâ€™administration cloud, en font un vÃ©ritable atout pour valider ses compÃ©tences, tandis que dâ€™autres, comme le dÃ©veloppement logiciel, mettent davantage lâ€™accent sur lâ€™expÃ©rience et les rÃ©alisations concrÃ¨tes.Comment jâ€™ai su que lâ€™informatique Ã©tait ma voie ?En secondaire 5, jâ€™ai dÃ©couvert la programmation grÃ¢ce Ã  un cours en Visual Basic 6. TrÃ¨s vite, jâ€™ai Ã©tÃ© captivÃ© par le fait de pouvoir Ã©crire du code et voir un programme prendre vie sous mes yeux. Chaque nouvel exercice me motivait Ã  en apprendre davantage et Ã  expÃ©rimenter par moi-mÃªme.Vers la fin du cours, jâ€™ai dÃ©veloppÃ© un tic-tac-toe oÃ¹ lâ€™on pouvait jouer contre une intelligence artificielle basique. Câ€™Ã©tait un petit projet, mais il mâ€™a permis de mieux comprendre la logique derriÃ¨re le dÃ©veloppement de logiciels et dâ€™alimenter encore plus ma curiositÃ© pour le domaine.Mon intÃ©rÃªt pour la matiÃ¨re sâ€™est traduit par une belle reconnaissance : jâ€™ai reÃ§u le mÃ©ritas en informatique cette annÃ©e-lÃ . Ce prix a renforcÃ© ma confiance et confirmÃ© que je voulais continuer dans cette voie.Les opportunitÃ©s dans les TILes secteurs dâ€™activitÃ©s en informatique sont nombreux : Technologies de lâ€™information : dÃ©veloppement dâ€™applications, cybersÃ©curitÃ©, gestion dâ€™infrastructures. Finance : plateformes de services bancaires, gestion des transactions. SantÃ© : logiciels de suivi mÃ©dical, gestion des dossiers patients. Ã‰ducation : outils pÃ©dagogiques numÃ©riques, plateformes dâ€™apprentissage en ligne. Jeux vidÃ©o : moteurs de jeu, intelligence artificielle. Secteur public : modernisation des services gouvernementaux.Les employeurs potentiels sont tout aussi variÃ©s : entreprises privÃ©es, startups, gouvernement, secteur autonome (freelance, consulting)â€¦La diversitÃ© dans le secteur des TIBien que les hommes restent majoritaires, la mixitÃ© progresse grÃ¢ce Ã  des initiatives encourageant la participation des femmes en technologie. Des programmes de mentorat, des bourses et des Ã©vÃ©nements dÃ©diÃ©s contribuent Ã  rendre le domaine plus inclusif.Lâ€™informatique gagne Ã  Ãªtre plus diversifiÃ©e, car la diversitÃ© des perspectives stimule lâ€™innovation et la crÃ©ativitÃ©.Conseils pour les futurs professionnels des TI DÃ©veloppe ta curiositÃ© : lâ€™informatique Ã©volue vite, il faut aimer apprendre. RÃ©seaute : cherche un mentor, participe Ã  des Ã©vÃ©nements, connecte-toi avec des professionnels sur LinkedIn. Travaille sur des projets personnels : crÃ©e des applications, contribue Ã  des projets open source, participe Ã  des hackathons. Familiarise-toi avec des technologies modernes : cloud, .NET, DevOpsâ€¦ Obtiens des certifications : elles sont un atout pour valider tes compÃ©tences.Avec dÃ©termination et persÃ©vÃ©rance, il est possible de rÃ©ussir, mÃªme en suivant un chemin non conventionnel !ConclusionLâ€™informatique est un domaine en constante Ã©volution, offrant des opportunitÃ©s variÃ©es et stimulantes dans de nombreux secteurs. Que ce soit par le dÃ©veloppement logiciel, lâ€™architecture des systÃ¨mes, la cybersÃ©curitÃ© ou encore lâ€™intelligence artificielle, chacun peut y trouver un chemin qui correspond Ã  ses intÃ©rÃªts et ambitions.Mon parcours mâ€™a appris que la curiositÃ©, lâ€™apprentissage continu et la persÃ©vÃ©rance sont des Ã©lÃ©ments clÃ©s pour rÃ©ussir. Il nâ€™existe pas de parcours unique : certains passent par lâ€™universitÃ©, dâ€™autres, comme moi, empruntent des chemins diffÃ©rents. Ce qui compte, câ€™est la passion et lâ€™envie dâ€™Ã©voluer.Si tu es intÃ©ressÃ© par les technologies et que tu veux bÃ¢tir des solutions qui auront un impact, câ€™est un domaine fait pour toi. Lance-toi, explore, expÃ©rimente, et surtout, ne sous-estime jamais ta capacitÃ© Ã  apprendre et Ã  progresser." }, { "title": "Scripts utilitaires", "url": "/posts/scripts-utilitaires/", "categories": "outil-developpement", "tags": "dotnet", "date": "2025-02-03 19:00:00 -0500", "snippet": "Les scripts suivants sont conÃ§us pour simplifier et automatiser diverses tÃ¢ches courantes liÃ©es Ã  lâ€™administration dâ€™une machine de dÃ©veloppement.Ils couvrent un Ã©ventail dâ€™opÃ©rations allant de la ...", "content": "Les scripts suivants sont conÃ§us pour simplifier et automatiser diverses tÃ¢ches courantes liÃ©es Ã  lâ€™administration dâ€™une machine de dÃ©veloppement.Ils couvrent un Ã©ventail dâ€™opÃ©rations allant de la gestion de fichiers Ã  la maintenance de dÃ©pÃ´ts Git, en passant par lâ€™optimisation des performances de copie et le nettoyage de rÃ©pertoires volumineux.Ces outils sont particuliÃ¨rement utiles pour gÃ©rer des environnements de dÃ©veloppement complexes et maintenir une machine propre et performante.CrÃ©er un fichier de taille exacteCe script permet de crÃ©er un fichier dâ€™une taille exacte, ce qui peut Ãªtre utile pour tester des systÃ¨mes de fichiers ou des transferts de donnÃ©es.Voici un exemple pour crÃ©er un fichier texte dâ€™un 1 Mo :$file = New-Object -TypeName System.IO.FileStream -ArgumentList \"D:\\TestFile.txt\", \"Create\", \"ReadWrite\"$file.SetLength(1Mb)$file.Close()Ce fichier peut Ãªtre utilisÃ© pour des tests de performance ou pour vÃ©rifier le comportement de systÃ¨mes de fichiers avec des fichiers de taille spÃ©cifique. Notez toutefois que ce fichier nâ€™aura pas de contenu valide ou de structure particuliÃ¨re, ce qui peut le rendre inappropriÃ© pour certaines applications nÃ©cessitant des donnÃ©es significatives.Copier efficacement avec RobocopyRobocopy, un outil intÃ©grÃ© Ã  Windows, est souvent plus performant que lâ€™Explorateur de fichiers pour copier des fichiers, surtout lorsquâ€™il sâ€™agit de gros volumes de donnÃ©es. Lâ€™Explorateur de fichiers Windows ne tire pas pleinement parti de tous les cÅ“urs du processeur, ce qui peut le rendre moins efficace pour les transferts de fichiers volumineux ou complexes. De plus, Robocopy utilise les ressources de maniÃ¨re plus optimale, offrant ainsi des vitesses de copie amÃ©liorÃ©es et une gestion des erreurs plus robuste, ce qui le rend particuliÃ¨rement adaptÃ© pour les tÃ¢ches de grande envergure.Voici un exemple dâ€™utilisation :robocopy \"C:\\repertoireSource\" \"D:\\repertoireCible\" /E /r:0 /w:0Dans cet exemple : /E - copie les sous-rÃ©pertoires, y compris les rÃ©pertoires vides. /r:0 - fixe le nombre de tentatives de reprise Ã  zÃ©ro, Ã©vitant ainsi les retards dus aux erreurs. /w:0 - dÃ©finit le temps dâ€™attente entre les tentatives Ã  zÃ©ro, ce qui accÃ©lÃ¨re encore le processus.Supprimer un rÃ©pertoire avec un chemin trop longLâ€™Explorateur de fichiers Windows peut rencontrer des problÃ¨mes lorsquâ€™il sâ€™agit de supprimer des rÃ©pertoires dont le chemin est trop long. En effet, Windows a une limite de 260 caractÃ¨res pour les chemins de fichiers, ce qui peut rendre la suppression de tels rÃ©pertoires compliquÃ©e directement depuis lâ€™interface graphique.Pour contourner cette limitation, vous pouvez utiliser le script suivant avec Robocopy, qui permet de supprimer le contenu dâ€™un rÃ©pertoire mÃªme si le chemin est trop long :robocopy \"C:\\RepertoireVide\" \"C:\\RepertoireASupprimer\" /purgeCela peut Ãªtre particuliÃ¨rement pratique pour effectuer le mÃ©nage des rÃ©pertoires node_modules, qui ont souvent des chemins de fichiers trÃ¨s longs en raison de la structure de leurs dÃ©pendances.Nettoyer vos rÃ©fÃ©rentiels Git de vos branches fusionnÃ©esCe script automatise le nettoyage des branches fusionnÃ©es dans plusieurs rÃ©fÃ©rentiels Git. Il parcourt tous les rÃ©pertoires du dossier courant, identifie ceux contenant un dÃ©pÃ´t Git, synchronise les branches distantes, puis supprime les branches locales fusionnÃ©es, tout en prÃ©servant les branches principales comme master et main. Câ€™est un outil pratique pour maintenir des rÃ©fÃ©rentiels propres et Ã©viter lâ€™encombrement avec des branches inutilisÃ©es.Exemple :.\\cleanbranch.ps1Supprimer les rÃ©pertoires consommant beaucoup dâ€™espaceIl est frÃ©quent que certains rÃ©pertoires tels que TestResults, StrykerOutput, node_modules, bin, et obj occupent une grande quantitÃ© dâ€™espace disque. Il est donc utile dâ€™effectuer un nettoyage rÃ©gulier pour libÃ©rer de lâ€™espace.Par dÃ©faut, le script cible ces rÃ©pertoires qui sont connus pour consommer beaucoup dâ€™espace. Vous pouvez soit confirmer la suppression de ces rÃ©pertoires par dÃ©faut, soit sÃ©lectionner manuellement ceux que vous souhaitez supprimer. Une fois les rÃ©pertoires choisis, le script sâ€™occupe de les supprimer ainsi que tout leur contenu, en explorant Ã©galement les sous-rÃ©pertoires de maniÃ¨re rÃ©cursive.Exemple :.\\cleanFolder.ps1Mettre Ã  jour des dÃ©pÃ´ts GitCe script met Ã  jour tous les dÃ©pÃ´ts Git dans un rÃ©pertoire en exÃ©cutant git pull pour chacun. Il liste les sous-dossiers, affiche un message en vert pour chaque mise Ã  jour, et effectue la synchronisation avec le dÃ©pÃ´t distant.Exemple :.\\updateAll.ps1ConclusionEn intÃ©grant ces scripts utilitaires Ã  votre processus de travail, vous pouvez considÃ©rablement simplifier la gestion de votre machine de dÃ©veloppement. Ils vous permettront dâ€™optimiser les performances de votre environnement, de maintenir vos projets propres et organisÃ©s, et de gagner du temps en automatisant des processus rÃ©pÃ©titifs.Pour accÃ©der Ã  tous ces scripts, consultez le rÃ©fÃ©rentiel complet sur GitHub.ğŸ’¡Note : Utilisez Windows Terminal avec PowerShell Core pour Ã©viter des problÃ¨mes dâ€™encodage de caractÃ¨res.Nâ€™hÃ©sitez pas Ã  explorer et personnaliser ces outils en fonction de vos besoins spÃ©cifiques !" }, { "title": "Transition de Fluent Assertions vers une licence commerciale", "url": "/posts/changement-licence-fluentassertions/", "categories": "outil-developpement", "tags": "dotnet, fluentassertions", "date": "2025-01-15 11:00:00 -0500", "snippet": "Fluent Assertions, une des bibliothÃ¨ques .NET les plus populaires pour Ã©crire des assertions fluides et expressives dans les tests unitaires, a rÃ©cemment franchi une Ã©tape importante en changeant d...", "content": "Fluent Assertions, une des bibliothÃ¨ques .NET les plus populaires pour Ã©crire des assertions fluides et expressives dans les tests unitaires, a rÃ©cemment franchi une Ã©tape importante en changeant de modÃ¨le de licence. Depuis la version 8.0.0, la bibliothÃ¨que est soumise Ã  une licence commerciale, ce qui signifie que les utilisateurs commerciaux doivent dÃ©sormais acheter une licence pour continuer Ã  lâ€™utiliser.Une rÃ©action communautaire : la naissance dâ€™AwesomeAssertionsCe changement a suscitÃ© des rÃ©actions mitigÃ©es dans la communautÃ© des dÃ©veloppeurs. Beaucoup ont exprimÃ© leur dÃ©sir de continuer Ã  utiliser une solution open source. En rÃ©ponse Ã  ces besoins, un projet communautaire nommÃ© AwesomeAssertions a Ã©tÃ© lancÃ©. Ce projet vise Ã  offrir une alternative open source, sous licence Apache 2.0, permettant de bÃ©nÃ©ficier des fonctionnalitÃ©s clÃ©s de Fluent Assertions sans les contraintes liÃ©es Ã  une licence commerciale.Maintenir Fluent Assertions sous licence open sourcePour les dÃ©veloppeurs qui souhaitent continuer Ã  utiliser Fluent Assertions sans adopter la nouvelle licence commerciale, il est possible de verrouiller la version Ã  la 7.0.0, la derniÃ¨re version disponible sous lâ€™ancienne licence open source.Voici comment procÃ©der dans votre fichier de projet (.csproj) :&lt;PackageReference Include=\"FluentAssertions\" Version=\"[7.0.0]\" /&gt;Cela garantit que votre projet reste sur la version 7.0.0, Ã©vitant ainsi toute mise Ã  jour involontaire vers une version soumise Ã  la nouvelle licence.Alternatives disponiblesLâ€™Ã©cosystÃ¨me .NET propose plusieurs autres bibliothÃ¨ques et solutions pour Ã©crire des assertions dans les tests unitaires : Fluent Assertions 8.0.0 et versions ultÃ©rieures : Pour ceux qui nâ€™ont pas de problÃ¨me avec le modÃ¨le commercial, ces versions offrent des fonctionnalitÃ©s avancÃ©es, avec un support prioritaire et des garanties additionnelles. AwesomeAssertions : Ce projet communautaire en cours de dÃ©veloppement sous licence Apache 2.0 offre une alternative open source aux versions commerciales de Fluent Assertions. Shouldly : Une bibliothÃ¨que dâ€™assertions connue pour ses messages dâ€™erreur lisibles et informatifs en cas dâ€™Ã©chec dâ€™une assertion, tout en fournissant une syntaxe simple et intuitive. TUnit : Un framework de test moderne et performant pour .NET 8 et versions ultÃ©rieures. Bien quâ€™il soit principalement conÃ§u comme un framework de test complet, il inclut des fonctionnalitÃ©s dâ€™assertion. Assertions natives : Utiliser des assertions telles que Assert.Equal ou Assert.True directement via le framework de test par dÃ©faut. En savoir plusPour ceux qui souhaitent mieux comprendre le contexte, Nick Chapsas a publiÃ© une vidÃ©o analysant ce changement et ses implications.ConclusionLe passage de Fluent Assertions Ã  une licence commerciale marque un tournant dans lâ€™Ã©cosystÃ¨me .NET. Bien que cela puisse crÃ©er des contraintes pour certains utilisateurs, des alternatives comme AwesomeAssertions ou Shouldly offrent des solutions adaptÃ©es. Chaque Ã©quipe doit Ã©valuer ses besoins en termes de licence, de support et de fonctionnalitÃ©s pour choisir la meilleure option.ğŸ’¡ Restez Ã  lâ€™affÃ»t, je devrais republier bientÃ´t pour partager lâ€™orientation que nous aurons choisie." }, { "title": "Aide-mÃ©moire - Code propre", "url": "/posts/aide-memoire-code-propre/", "categories": "bonne-pratique", "tags": "", "date": "2025-01-14 19:00:00 -0500", "snippet": "Lâ€™objectif de tout dÃ©veloppeur est de produire du code qui soit non seulement fonctionnel, mais aussi propre, facile Ã  lire et Ã  maintenir. Un code propre permet non seulement dâ€™amÃ©liorer la collab...", "content": "Lâ€™objectif de tout dÃ©veloppeur est de produire du code qui soit non seulement fonctionnel, mais aussi propre, facile Ã  lire et Ã  maintenir. Un code propre permet non seulement dâ€™amÃ©liorer la collaboration entre les membres de lâ€™Ã©quipe, mais aussi dâ€™assurer la pÃ©rennitÃ© du projet Ã  long terme. Chaque choix de conception que vous faites peut avoir un impact significatif sur la maintenabilitÃ© de votre code, câ€™est pourquoi il est crucial de bien comprendre ces principes pour Ã©lever la qualitÃ© de vos projets.Le code propreLe code est propre sâ€™il peut Ãªtre compris facilement par toutes les personnes qui auront Ã  travailler avec celui-ci. Un code propre peut Ãªtre lu et amÃ©liorÃ© aisÃ©ment par un autre dÃ©veloppeur que son auteur dâ€™origine. Avec la comprÃ©hensibilitÃ© viennent la lisibilitÃ©, la possibilitÃ© de changement, lâ€™extensibilitÃ© et la maintenabilitÃ©.ğŸ’¡Note : Lâ€™importance est dâ€™en prendre connaissance plus comme des orientations communes que comme des rÃ¨gles Ã  respecter Ã  tout prix.RÃ¨gles gÃ©nÃ©rales Suivez les conventions de codage C# Suivez les conventions gÃ©nÃ©rales dâ€™affectation de noms Pensez SOLID Pensez KISS (Keep it simple stupid). Plus simple est toujours mieux. RÃ©duisez au maximum la complexitÃ©. Suivez-la rÃ¨gle du boy-scouts (The Boy Scout Rule). Toujours laisser un endroit dans un Ã©tat meilleur que celui oÃ¹ vous lâ€™avez trouvÃ©. Cherchez toujours la cause premiÃ¨re dâ€™un problÃ¨me.RÃ¨gles de conception Conservez les donnÃ©es configurables Ã  des niveaux Ã©levÃ©s. PrÃ©fÃ©rez le polymorphisme Ã  if/else ou switch/case. SÃ©parez le code multithread. EmpÃªchez la surconfigurabilitÃ© (over-configurability). Utilisez lâ€™injection de dÃ©pendance. Suivez-la loi de DÃ©mÃ©ter. Une classe ne doit connaÃ®tre que ses dÃ©pendances directes.Conseils de comprÃ©hension ÃŠtre cohÃ©rent. Si vous faites quelque chose dâ€™une certaine maniÃ¨re, suivez ce patron par la suite. Utilisez des variables explicatives. Encapsulez les conditions aux cas limites (boundary conditions). Les conditions aux cas limites sont difficiles Ã  suivre. Mettez le traitement pour eux en un seul endroit. PrÃ©fÃ©rez les objets de valeur (value objects) dÃ©diÃ©s au type primitif. Ã‰vitez les dÃ©pendances logiques. Nâ€™Ã©crivez pas de mÃ©thodes qui fonctionnent correctement en fonction de quelque chose dâ€™autre dans la mÃªme classe. Ã‰vitez les conditions nÃ©gatives.Conventions de nommage Choisissez des noms descriptifs et sans ambiguÃ¯tÃ©. Utilisez le franÃ§ais, sauf si les directives de la compagnie indiquent de coder en anglais. Il nâ€™est cependant pas nÃ©cessaire de traduire des termes techniques spÃ©cifiques au framework ou un patron de conception tel quâ€™un â€œrepositoryâ€. Faire une distinction significative. Utilisez des noms prononÃ§ables. Utilisez des noms consultables. Remplacez les nombres magiques par des constantes nommÃ©es. Ã‰vitez les encodages. Nâ€™ajoutez pas de prÃ©fixes ni dâ€™informations de type. Ã‰vitez de mettre les acronymes tels que â€œXMLâ€ en majuscule, favorisez le format â€œXmlâ€.RÃ¨gles pour les fonctions Petites. Font quâ€™une chose. Utilisez des noms descriptifs. PrÃ©fÃ©rez moins dâ€™arguments. Nâ€™ont pas dâ€™effets secondaires. Nâ€™utilisez pas dâ€™arguments de type flags. Divisez la mÃ©thode en plusieurs mÃ©thodes indÃ©pendantes qui peuvent Ãªtre appelÃ©es depuis le client sans lâ€™indicateur.RÃ¨gles pour les commentaires Essayez toujours de vous expliquer en code. Ne soyez pas redondant. Nâ€™ajoutez pas de bruit Ã©vident. Nâ€™utilisez pas dâ€™accolade fermante pour commenter. Ne commentez pas le code. Retirez-le simplement. Utilisez comme explication lâ€™intention. Utilisez comme clarification du code. Utilisez comme avertissement des consÃ©quences.Structure du code source SÃ©parez les concepts verticalement. Le code associÃ© doit apparaÃ®tre verticalement dense. DÃ©clarez les variables proches de leur utilisation. Les fonctions dÃ©pendantes doivent Ãªtre proches. Des fonctions similaires doivent Ãªtre proches. Placez les fonctions dans le sens descendant. Gardez les lignes courtes. Nâ€™utilisez pas lâ€™alignement horizontal. Utilisez un saut de ligne pour associer des Ã©lÃ©ments liÃ©s et dissocier des Ã©lÃ©ments faiblement liÃ©s. Ne cassez pas lâ€™indentation.Objets et structures de donnÃ©es Masquez la structure interne. PrÃ©fÃ©rez les structures de donnÃ©es. Ã‰vitez les structures hybrides (moitiÃ© objet et moitiÃ© donnÃ©es). Devrait Ãªtre petit. Fait quâ€™une chose. Petit nombre de variables. La classe de base ne doit rien savoir de ses dÃ©rivÃ©s. Vaut mieux avoir plusieurs fonctions que de passer du code dans une fonction pour sÃ©lectionner un comportement. PrÃ©fÃ©rez les mÃ©thodes non statiques aux mÃ©thodes statiques.Essais Une assertion logique par test Lisible Rapide IndÃ©pendant RÃ©pÃ©tableCode smells RigiditÃ© - Le logiciel est difficile Ã  changer. Un petit changement provoque une cascade de changements. FragilitÃ© - Le logiciel tombe en panne Ã  de nombreux endroits en raison dâ€™un seul changement. ImmobilitÃ© - Vous ne pouvez pas rÃ©utiliser des parties du code dans dâ€™autres - projets en raison des risques impliquÃ©s et des efforts importants. ComplexitÃ© inutile. RÃ©pÃ©tition inutile. OpacitÃ©. Le code est difficile Ã  comprendre.ğŸ’¡Note : Au besoin, rÃ©fÃ©rez-vous Ã  cet article complet pour plus de dÃ©tails.ConclusionEn conclusion, adopter des pratiques de code propre nâ€™est pas seulement une bonne pratique, mais une nÃ©cessitÃ© dans le dÃ©veloppement logiciel moderne. En intÃ©grant les principes Ã©voquÃ©s dans cet article, vous favoriserez non seulement la lisibilitÃ© et la maintenabilitÃ© de votre code, mais vous contribuerez Ã©galement Ã  lâ€™efficacitÃ© de votre Ã©quipe. Un code bien structurÃ© permet de rÃ©duire les erreurs et de faciliter les mises Ã  jour futures, tout en rendant lâ€™intÃ©gration de nouveaux dÃ©veloppeurs plus fluide. En fin de compte, investir dans la propretÃ© du code est un investissement dans la durabilitÃ© de vos projets, garantissant ainsi leur succÃ¨s Ã  long terme. Rappelez-vous que la quÃªte dâ€™un code propre est un processus continu qui demande rÃ©flexion, pratique et engagement.RÃ©fÃ©rences https://www.perforce.com/blog/sca/what-code-quality-overview-how-improve-code-quality https://gist.github.com/wojteklu/73c6914cc446146b8b533c0988cf8d29" }, { "title": "Introduction aux Testcontainers", "url": "/posts/introduction-testcontainers/", "categories": "outil-developpement", "tags": "essais, testcontainers", "date": "2024-12-12 19:00:00 -0500", "snippet": "Les systÃ¨mes logiciels modernes sâ€™attaquent Ã  des problÃ¨mes complexes en utilisant une multitude de technologies et dâ€™outils. Rarement un systÃ¨me logiciel fonctionne de maniÃ¨re isolÃ©e; il interagit...", "content": "Les systÃ¨mes logiciels modernes sâ€™attaquent Ã  des problÃ¨mes complexes en utilisant une multitude de technologies et dâ€™outils. Rarement un systÃ¨me logiciel fonctionne de maniÃ¨re isolÃ©e; il interagit gÃ©nÃ©ralement avec des bases de donnÃ©es, des systÃ¨mes de messagerie, des fournisseurs de cache, et de nombreux autres services tiers. Dans ce marchÃ© hautement concurrentiel, le temps de mise en marchÃ© est crucial. Les entreprises souhaitent lancer leur produit rapidement, obtenir des retours et itÃ©rer en consÃ©quence. Pour atteindre cette agilitÃ©, il est essentiel dâ€™avoir un processus solide dâ€™intÃ©gration et de dÃ©ploiement continus (CI/CD). Un Ã©lÃ©ment clÃ© de ce processus sont les tests automatisÃ©s, qui garantissent le bon fonctionnement de lâ€™application.Les tests unitaires permettent de vÃ©rifier la logique mÃ©tier et les dÃ©tails dâ€™implÃ©mentation en isolant les services externes, mais la majeure partie du code applicatif rÃ©side dans lâ€™intÃ©gration avec ces services. Pour avoir une confiance totale dans notre application, il est essentiel dâ€™Ã©crire des tests dâ€™intÃ©gration en plus des tests unitaires, afin dâ€™assurer la fonctionnalitÃ© complÃ¨te de lâ€™application.Historiquement, les tests dâ€™intÃ©gration sont considÃ©rÃ©s comme difficiles en raison des dÃ©fis liÃ©s Ã  la maintenance dâ€™un environnement de test dâ€™intÃ©gration. La mise en place de tests dâ€™intÃ©gration avec une infrastructure prÃ©configurÃ©e pose plusieurs problÃ¨mes, notamment la nÃ©cessitÃ© de garantir que lâ€™infrastructure est opÃ©rationnelle et que les donnÃ©es sont prÃ©configurÃ©es dans un Ã©tat dÃ©sirÃ©. De plus, lâ€™exÃ©cution de plusieurs pipelines de construction (build) en parallÃ¨le peut interfÃ©rer avec dâ€™autres donnÃ©es de test, entraÃ®nant des tests instables.Face Ã  ces dÃ©fis, certains dÃ©veloppeurs se tournent vers des services en mÃ©moire ou des variations embarquÃ©es des services requis pour les tests dâ€™intÃ©gration. Par exemple, un dÃ©veloppeur pourrait utiliser une base de donnÃ©es en mÃ©moire en remplacement de Microsoft SQL Server. Bien que cela soit une amÃ©lioration par rapport Ã  lâ€™absence de tests dâ€™intÃ©gration, lâ€™utilisation de simulations ou de versions en mÃ©moire prÃ©sente ses propres problÃ¨mes. Les services en mÃ©moire peuvent ne pas avoir toutes les fonctionnalitÃ©s des services en production !Câ€™est dans ce contexte que les Testcontainers entre en jeu, offrant une solution innovante pour les tests dâ€™intÃ©gration avec de vÃ©ritables services, rendant cette tÃ¢che aussi simple que lâ€™Ã©criture de tests unitaires. En utilisant les Testcontainers, les dÃ©veloppeurs peuvent crÃ©er des environnements de test reproduisant fidÃ¨lement leurs systÃ¨mes de production, tout en bÃ©nÃ©ficiant dâ€™une rÃ©troaction rapide et efficace sur leurs modifications.Quâ€™est-ce quâ€™un TestcontainerTestcontainers est une bibliothÃ¨que de test qui offre des API pour faciliter la mise en place de tests dâ€™intÃ©gration avec des services rÃ©els, encapsulÃ©s dans des conteneurs Docker. GrÃ¢ce Ã  Testcontainers, il est possible dâ€™Ã©crire des tests qui interagissent avec le mÃªme type de services que ceux utilisÃ©s en production, Ã©liminant ainsi la nÃ©cessitÃ© dâ€™utiliser des simulations ou des services en mÃ©moire.Un test dâ€™intÃ©gration typique basÃ© sur Testcontainers fonctionne de la maniÃ¨re suivante : Avant les tests :Â  Vous dÃ©marrez vos services requis (bases de donnÃ©es, systÃ¨mes de messagerie, etc.) en utilisant lâ€™API de Testcontainers pour lancer des conteneurs Docker. Configuration : Vous configurez ou mettez Ã  jour la configuration de votre application pour quâ€™elle utilise ces services conteneurisÃ©s. Pendant les tests : Vos tests sâ€™exÃ©cutent en utilisant ces services conteneurisÃ©s, ce qui garantit un environnement de test qui reflÃ¨te fidÃ¨lement la production. AprÃ¨s les tests : Testcontainers sâ€™occupe de dÃ©truire ces conteneurs, que les tests aient rÃ©ussi ou Ã©chouÃ©.La seule exigence pour exÃ©cuter des tests basÃ©s sur Testcontainers est dâ€™avoir un environnement dâ€™exÃ©cution de conteneurs compatible avec lâ€™API Docker. Si vous avez Docker Desktop installÃ© et en cours dâ€™exÃ©cution, tout est prÃªt.Â Pour plus dâ€™informations sur les environnements Docker pris en charge par Testcontainers, vous pouvez consulter la documentation officielle.Quâ€™est-ce que les Testcontainers rÃ¨glent ?Les Testcontainers rÃ©solvent plusieurs problÃ¨mes liÃ©s aux tests dâ€™intÃ©gration en permettant aux dÃ©veloppeurs de tester leur application avec de vÃ©ritables services, ce qui augmente la confiance dans les modifications apportÃ©es au code.Voici quelques points clÃ©s sur ce que Testcontainers amÃ©liore : Infrastructure de test prÃ©configurÃ©e : Avec Testcontainers, il nâ€™est pas nÃ©cessaire dâ€™avoir une infrastructure de test dâ€™intÃ©gration prÃ©alablement configurÃ©e. Lâ€™API de Testcontainers fournit automatiquement les services nÃ©cessaires avant lâ€™exÃ©cution des tests. Cela signifie que la dÃ©finition de lâ€™infrastructure se trouve directement Ã  cÃ´tÃ© du code de test, facilitant ainsi la maintenance et la comprÃ©hension. Isolation des donnÃ©es : Testcontainers Ã©limine les problÃ¨mes de conflit de donnÃ©es, mÃªme lorsque plusieurs pipelines de construction (build) sâ€™exÃ©cutent en parallÃ¨le. Chaque pipeline fonctionne avec un ensemble de services isolÃ©s, ce qui prÃ©vient les interfÃ©rences entre les tests. ExÃ©cution depuis lâ€™IDE : Les dÃ©veloppeurs peuvent exÃ©cuter leurs tests dâ€™intÃ©gration directement depuis leur IDE, tout comme pour les tests unitaires, sans avoir Ã  pousser les modifications et attendre que lâ€™intÃ©gration continue (CI) exÃ©cute les tests. Cela amÃ©liore le flux de travail et accÃ©lÃ¨re le cycle de rÃ©troaction. Nettoyage automatique : AprÃ¨s lâ€™exÃ©cution des tests, Testcontainers sâ€™occupe automatiquement de la suppression des conteneurs, ce qui simplifie la gestion des ressources et rÃ©duit le risque dâ€™accumulation de services inutilisÃ©s sur le systÃ¨me. CompatibilitÃ© avec plusieurs langages : Testcontainers peut Ãªtre utilisÃ© avec de nombreux langages de programmation populaires, notamment .NET, Java, Go, NodeJS, Rust et Python, avec davantage de supports de langages Ã  venir.En rÃ©sumÃ©, Testcontainers fournit une solution pratique et efficace pour les tests dâ€™intÃ©gration, permettant aux Ã©quipes de dÃ©veloppement de gagner en confiance et en agilitÃ© dans leur processus de dÃ©veloppement.DÃ©monstrationConclusionEn conclusion, nous avons examinÃ© les dÃ©fis inhÃ©rents aux tests dâ€™intÃ©gration, en mettant en lumiÃ¨re les limitations de lâ€™utilisation de mocks ou de services en mÃ©moire, qui peuvent entraÃ®ner des incohÃ©rences entre les environnements de test et de production. Testcontainers offre une solution puissante Ã  ces problÃ¨mes, permettant aux dÃ©veloppeurs de rÃ©aliser des tests dâ€™intÃ©gration avec de vÃ©ritables services dans des conteneurs Docker isolÃ©s. Cela renforce non seulement la fiabilitÃ© des tests, mais streamline Ã©galement le processus de test, permettant un retour dâ€™information plus rapide et une plus grande confiance dans les modifications de code.Pour des informations supplÃ©mentaires et une documentation dÃ©taillÃ©e sur la mise en Å“uvre de Testcontainers dans vos projets, nâ€™hÃ©sitez pas Ã  visiter Testcontainers." }, { "title": "Optimisation des Expressions RÃ©guliÃ¨res avec les GÃ©nÃ©rateurs de Sources dans .NET", "url": "/posts/regex-generateurs-sources-fluentvalidation/", "categories": "", "tags": "dotnet, fluentvalidation, source-generator", "date": "2024-12-01 19:00:00 -0500", "snippet": "Avec .NET, les gÃ©nÃ©rateurs de sources pour les expressions rÃ©guliÃ¨res (Regex Source Generators) permettent dâ€™optimiser la crÃ©ation et lâ€™exÃ©cution des expressions rÃ©guliÃ¨res en gÃ©nÃ©rant du code sour...", "content": "Avec .NET, les gÃ©nÃ©rateurs de sources pour les expressions rÃ©guliÃ¨res (Regex Source Generators) permettent dâ€™optimiser la crÃ©ation et lâ€™exÃ©cution des expressions rÃ©guliÃ¨res en gÃ©nÃ©rant du code source au moment de la compilation. Ces gÃ©nÃ©rateurs, introduits Ã  partir de .NET 7, utilisent lâ€™attribut [GeneratedRegex] pour dÃ©finir des expressions rÃ©guliÃ¨res de maniÃ¨re dÃ©clarative.Lâ€™objectif est dâ€™amÃ©liorer les performances en Ã©liminant les Ã©tapes coÃ»teuses de compilation des expressions rÃ©guliÃ¨res au moment de lâ€™exÃ©cution. Au lieu de crÃ©er une instance de Regex dynamique, le gÃ©nÃ©rateur produit un code prÃ©compilÃ©, offrant ainsi une solution plus rapide et adaptÃ©e aux scÃ©narios exigeants. ğŸ’¡ Pour plus de dÃ©tails, consultez la documentation officielle : Regular Expression Source Generators.FluentValidation et les GÃ©nÃ©rateurs de SourcesOn peut lÃ©gitimement se demander si FluentValidation utilise les gÃ©nÃ©rateurs de sources pour optimiser lâ€™instruction Matches(\"^[a-zA-Z0-9]*$\");. Pour explorer cette question, jâ€™ai conÃ§u un benchmark comparant deux implÃ©mentations de validation : lâ€™une utilisant des expressions rÃ©guliÃ¨res classiques et lâ€™autre tirant parti des gÃ©nÃ©rateurs de sources avec [GeneratedRegex].Benchmark ComparatifVoici le code du benchmark :using BenchmarkDotNet.Attributes;using BenchmarkDotNet.Columns;using BenchmarkDotNet.Order;using FluentValidation;using System.Text.RegularExpressions;namespace TestBenchmarkDotnet.TestRegexFluentValidation;[MemoryDiagnoser][Orderer(SummaryOrderPolicy.FastestToSlowest)][RankColumn][HideColumns(Column.Error, Column.Median, Column.RatioSD, Column.StdDev)]public partial class BenchmarkRegexFluentValidation{ private ValidatorSansGeneratedRegex _validatorSansGeneratedRegex; private ValidatorAvecGeneratedRegex _validatorAvecGeneratedRegex; private TestModel _instance; [GlobalSetup] public void GlobalSetup() { _validatorSansGeneratedRegex = new(); _validatorAvecGeneratedRegex = new(); _instance = new() { Value = \"TestString123\" }; } [Benchmark(Baseline = true)] public void ValidationSansGeneratedRegex() =&gt; _ = _validatorSansGeneratedRegex.Validate(_instance).IsValid; [Benchmark] public void ValidationAvecGeneratedRegex() =&gt; _ = _validatorAvecGeneratedRegex.Validate(_instance).IsValid; public class TestModel { public required string Value { get; set; } } public class ValidatorSansGeneratedRegex : AbstractValidator&lt;TestModel&gt; { public ValidatorSansGeneratedRegex() { RuleFor(x =&gt; x.Value).Matches(\"^[a-zA-Z0-9]*$\"); } } public partial class ValidatorAvecGeneratedRegex : AbstractValidator&lt;TestModel&gt; { public ValidatorAvecGeneratedRegex() { RuleFor(x =&gt; x.Value).Matches(MyRegex()); } // L'option `RegexOptions.Compiled` est ignorÃ©e par le gÃ©nÃ©rateur et donc non nÃ©cessaire. [GeneratedRegex(@\"^[a-zA-Z0-9]*$\")] private static partial Regex MyRegex(); }}Analyse des rÃ©sultats| Method | Mean | Ratio | Rank | Gen0 | Allocated | Alloc Ratio ||----------------------------- |---------:|------:|-----:|-------:|----------:|------------:|| ValidationAvecGeneratedRegex | 157.3 ns | 0.67 | 1 | 0.0715 | 600 B | 1.00 || ValidationSansGeneratedRegex | 234.7 ns | 1.00 | 2 | 0.0715 | 600 B | 1.00 | Temps dâ€™exÃ©cution :Lâ€™utilisation des gÃ©nÃ©rateurs de sources rÃ©duit le temps dâ€™exÃ©cution de la validation dâ€™environ 33 %, passant de 234,7 ns Ã  157,3 ns. Cette amÃ©lioration est due Ã  lâ€™optimisation apportÃ©e par le code prÃ©compilÃ© gÃ©nÃ©rÃ© au moment de la compilation. Allocations mÃ©moire :Les allocations mÃ©moire restent identiques dans les deux cas (600 B). Cela montre que lâ€™amÃ©lioration des performances est principalement due Ã  la rÃ©duction des calculs nÃ©cessaires pour traiter les expressions rÃ©guliÃ¨res. Classement :Le benchmark classe clairement la validation avec gÃ©nÃ©rateurs de sources comme la mÃ©thode la plus rapide. Ces rÃ©sultats dÃ©montrent que lâ€™intÃ©gration des gÃ©nÃ©rateurs de sources peut apporter des bÃ©nÃ©fices significatifs en termes de performance, particuliÃ¨rement pour des validations rÃ©pÃ©tÃ©es ou critiques en production.ConclusionLes gÃ©nÃ©rateurs de sources pour les expressions rÃ©guliÃ¨res reprÃ©sentent une avancÃ©e importante dans lâ€™Ã©cosystÃ¨me .NET. En utilisant lâ€™attribut [GeneratedRegex], les dÃ©veloppeurs peuvent bÃ©nÃ©ficier dâ€™amÃ©liorations notables en termes de vitesse dâ€™exÃ©cution, tout en conservant une syntaxe dÃ©clarative et lisible.Bien que FluentValidation ne supporte pas directement les gÃ©nÃ©rateurs de sources dans ses rÃ¨gles comme Matches, lâ€™approche manuelle dÃ©montrÃ©e ici peut Ãªtre utilisÃ©e pour combiner les avantages des deux outils.Les dÃ©veloppeurs souhaitant optimiser leurs applications .NET devraient envisager dâ€™adopter cette technique, notamment dans les scÃ©narios oÃ¹ des expressions rÃ©guliÃ¨res complexes ou souvent utilisÃ©es jouent un rÃ´le clÃ©." }, { "title": "Retour sur Microsoft Ignite 2024", "url": "/posts/microsoft-ignite-2024/", "categories": "", "tags": "dotnet, conference", "date": "2024-12-01 19:00:00 -0500", "snippet": "Lâ€™Ã©vÃ©nement phare de Microsoft a mis lâ€™accent sur des thÃ©matiques majeures : lâ€™intelligence artificielle (IA), la cybersÃ©curitÃ© et la collaboration. Lâ€™Ã©dition de cette annÃ©e a prÃ©sentÃ© des outils e...", "content": "Lâ€™Ã©vÃ©nement phare de Microsoft a mis lâ€™accent sur des thÃ©matiques majeures : lâ€™intelligence artificielle (IA), la cybersÃ©curitÃ© et la collaboration. Lâ€™Ã©dition de cette annÃ©e a prÃ©sentÃ© des outils et solutions destinÃ©s Ã  transformer les entreprises dans un monde numÃ©rique en rapide Ã©volution.1. Azure AI FoundryUne plateforme intÃ©grÃ©e pour concevoir, entraÃ®ner et dÃ©ployer des modÃ¨les dâ€™IA personnalisÃ©s : Permet aux entreprises de personnaliser des modÃ¨les comme GPT pour leurs propres donnÃ©es. Cas pratique : Une banque europÃ©enne a augmentÃ© ses ventes croisÃ©es de 25 % grÃ¢ce Ã  lâ€™analyse de donnÃ©es client en temps rÃ©el.2. Copilot EverywhereUne extension de lâ€™IA dans les outils quotidiens comme Microsoft Teams, Power Platform et des plateformes tierces : RÃ©sumÃ©s automatiques, gÃ©nÃ©ration dâ€™applications low-code et intÃ©gration avec des solutions comme Salesforce. Impact mesurable : Une entreprise a rÃ©duit de 30 % le temps consacrÃ© Ã  des tÃ¢ches administratives.3. Zero Day QuestUn programme de dÃ©tection proactive des vulnÃ©rabilitÃ©s pour renforcer la cybersÃ©curitÃ© mondiale : Collaboration avec des chercheurs en sÃ©curitÃ©, des universitÃ©s et des gouvernements pour dÃ©tecter les menaces. RÃ©sultat : RÃ©duction des incidents liÃ©s aux menaces internes de 40 % dans une sociÃ©tÃ© dâ€™assurance.4. Focus sur lâ€™inclusivitÃ© et lâ€™Ã©thique AccessibilitÃ© accrue dans les outils collaboratifs. Garde-fous intÃ©grÃ©s dans lâ€™IA pour garantir des rÃ©sultats fiables.Microsoft a rÃ©affirmÃ© sa mission : rendre lâ€™IA accessible tout en assurant la sÃ©curitÃ© et la rÃ©silience nÃ©cessaires pour prospÃ©rer dans ce monde numÃ©rique.SQL Server 2025SQL Server 2025 marque une avancÃ©e significative pour intÃ©grer lâ€™IA au cÅ“ur des donnÃ©es dâ€™entreprise : Stockage vectoriel natif et gestion des modÃ¨les IA directement dans le moteur SQL pour rÃ©duire les latences et simplifier lâ€™orchestration. SÃ©curitÃ© renforcÃ©e avec cryptage en temps rÃ©el et conformitÃ© stricte aux normes (GDPR, HIPAA). SQL Server Management Studio (SSMS) 21 Preview : Migration vers Visual Studio 2022, support 64 bits, thÃ¨me sombre et Copilot intÃ©grÃ© pour simplifier la rÃ©daction de requÃªtes T-SQL. Ces innovations offrent des gains en performance, une meilleure gestion des coÃ»ts et une modernisation adaptÃ©e aux besoins des entreprises modernes.Azure DevOps et GitHubMicrosoft a clarifiÃ© sa vision sur lâ€™avenir dâ€™Azure DevOps et GitHub, en renforÃ§ant leur intÃ©gration pour rÃ©pondre Ã  une large diversitÃ© de besoins : GitHub Advanced Security : Disponible dans Azure DevOps pour dÃ©tecter les secrets dans le code et surveiller les dÃ©pendances. GitHub Copilot : AccessibilitÃ© dans Azure DevOps pour des suggestions de code contextuelles et un gain de productivitÃ© accru. Azure Boards et GitHub : IntÃ©gration renforcÃ©e permettant une gestion fluide des Ã©lÃ©ments de travail. Azure Pipelines : Des pipelines CI/CD adaptatifs pour les projets GitHub.Microsoft ne cherche pas Ã  opposer Azure DevOps et GitHub, mais plutÃ´t Ã  renforcer leur complÃ©mentaritÃ©.RÃ©sumÃ© : GitHub et Azure DevOps ne sont pas des concurrents, mais des alliÃ©s, offrant une double offre stratÃ©gique pour simplifier les processus, sÃ©curiser les applications et amÃ©liorer la performance.Azure Kubernetes Service (AKS) et Azure Managed RedisMicrosoft a mis en avant les amÃ©liorations de Azure Kubernetes Service (AKS) et Azure Managed Redis, essentiels pour les applications modernes et lâ€™IA gÃ©nÃ©rative.1. Azure Kubernetes Service (AKS) Fleet Manager : Orchestration centralisÃ©e des clusters Kubernetes sur plusieurs environnements. Auto-provisionnement des nÅ“uds : Allocation dynamique des ressources pour optimiser les coÃ»ts et les performances. Trusted Launch : SÃ©curitÃ© renforcÃ©e avec vÃ©rification de lâ€™intÃ©gritÃ© des workloads critiques.2. Azure Managed Redis OptimisÃ© pour les charges dâ€™IA gÃ©nÃ©rative (GenAI) avec des latences rÃ©duites et une gestion avancÃ©e des donnÃ©es en mÃ©moire. IdÃ©al pour stocker des embeddings, des donnÃ©es contextuelles et des rÃ©sultats de modÃ¨les gÃ©nÃ©ratifs. API simplifiÃ©es pour une intÃ©gration rapide dans les pipelines dâ€™IA.Avec ces outils, Microsoft simplifie la gestion des environnements complexes tout en garantissant scalabilitÃ© et sÃ©curitÃ©.GitHub CopilotGitHub Copilot rÃ©volutionne le dÃ©veloppement avec des fonctionnalitÃ©s clÃ©s : ComplÃ©tion contextuelle avancÃ©e et refactoring automatisÃ© pour simplifier le travail des dÃ©veloppeurs. Suggestions de sÃ©curitÃ© et gÃ©nÃ©ration de documentation intelligente pour amÃ©liorer la qualitÃ© et la fiabilitÃ© du code. PossibilitÃ© dâ€™ajouter des instructions spÃ©cifiques pour guider Copilot.Impact mesurable : Les entreprises comme Accenture rapportent une rÃ©duction de 30 % du temps consacrÃ© aux tÃ¢ches rÃ©pÃ©titives grÃ¢ce Ã  Copilot, permettant aux Ã©quipes de se concentrer sur lâ€™innovation.ConclusionMicrosoft Ignite 2024 a confirmÃ© son engagement envers une transformation numÃ©rique responsable et performante. Entre IA, cybersÃ©curitÃ©, et complÃ©mentaritÃ© entre Azure DevOps et GitHub, lâ€™avenir est clair : des outils puissants, sÃ©curisÃ©s, et accessibles pour toutes les entreprises, quelle que soit leur taille ou leur secteur." }, { "title": "Le mois clÃ© des innovations chez Microsoft", "url": "/posts/innovations-microsoft-2024/", "categories": "", "tags": "conference", "date": "2024-12-01 19:00:00 -0500", "snippet": "Installez-vous confortablement, peut-Ãªtre avec une bonne tasse de cafÃ©, car ce rÃ©sumÃ© regorge dâ€™informations passionnantes et dÃ©taillÃ©es pour explorer les derniÃ¨res innovations.Le mois de novembre ...", "content": "Installez-vous confortablement, peut-Ãªtre avec une bonne tasse de cafÃ©, car ce rÃ©sumÃ© regorge dâ€™informations passionnantes et dÃ©taillÃ©es pour explorer les derniÃ¨res innovations.Le mois de novembre est dÃ©sormais une pÃ©riode clÃ© pour les annonces technologiques, marquÃ©e par deux Ã©vÃ©nements majeurs organisÃ©s par Microsoft : .NET Conf et Microsoft Ignite..NET Conf 2024.NET Conf 2024 a eu lieu du 12 au 14 novembre, est une confÃ©rence annuelle dÃ©diÃ©e Ã  lâ€™Ã©cosystÃ¨me .NET, oÃ¹ les dÃ©veloppeurs dÃ©couvrent les derniÃ¨res innovations et mises Ã  jour autour de .NET.Câ€™est un Ã©vÃ©nement gratuit, entiÃ¨rement en ligne, qui propose des sessions techniques, des dÃ©monstrations, et des prÃ©sentations dâ€™experts.Points clÃ©s de .NET Conf : Focus sur .NET 9 : Mise en avant des performances et des outils de dÃ©veloppement. Multiplateforme : Sessions couvrant le dÃ©veloppement web, mobile, cloud, et plus encore. AccessibilitÃ© : Un incontournable pour ceux qui souhaitent rester Ã  la pointe des meilleures pratiques. ğŸ¥ Astuce : Vous pouvez retrouver toutes les sessions de .NET Conf 2024 sur la playlist officiellee.Microsoft Ignite 2024Microsoft Ignite, quant Ã  lui, a eu lieu du 18 au 22 novembre Ã  Chicago. Il sâ€™agit dâ€™une confÃ©rence majeure pour les professionnels TI et les dÃ©cideurs technologiques, couvrant une gamme plus large de technologies Microsoft, incluant Azure, Microsoft 365, et la cybersÃ©curitÃ©.Points clÃ©s de Microsoft Ignite : DÃ©couverte des innovations majeures dans le cloud, la productivitÃ© et la transformation numÃ©rique. PrÃ©sentation des derniÃ¨res avancÃ©es dans Azure et Microsoft 365. Un lieu stratÃ©gique pour comprendre les enjeux technologiques futurs. ğŸ¥ Astuce : Retrouvez toutes les sessions sur la playlist officielle .Synergie des Ã©vÃ©nementsCes deux Ã©vÃ©nements, organisÃ©s en novembre, crÃ©ent une synergie stratÃ©gique : ils permettent Ã  Microsoft de synchroniser les lancements de produits comme .NET, Visual Studio, et Azure, tout en touchant des audiences complÃ©mentaires.Bien que certains participants trouvent cette concentration dâ€™Ã©vÃ©nements intense, elle permet de faire des annonces clÃ©s au moment opportun pour les dÃ©cisions budgÃ©taires de lâ€™annÃ©e suivante.En combinant ces Ã©vÃ©nements, Microsoft transforme novembre en un mois de dÃ©couvertes technologiques, gÃ©nÃ©rant un enthousiasme considÃ©rable dans la communautÃ© des dÃ©veloppeurs et professionnels TI.Cycle de versionnement en .NETAvant dâ€™analyser les nouveautÃ©s annoncÃ©es, il est crucial de bien comprendre le cycle de versionnement en .NET, car cela permet de situer chaque version dans son contexte de support et dâ€™innovation, et dâ€™orienter ses choix en consÃ©quence.Depuis la sortie de .NET 6, Microsoft a introduit un cycle de versionnement rÃ©gulier qui alterne entre des versions LTS (Long-Term Support) et STS (Standard-Term Support). Ce schÃ©ma vise Ã  offrir un Ã©quilibre entre : La stabilitÃ© pour les applications Ã  long terme. La possibilitÃ© dâ€™accÃ©der rapidement aux nouvelles fonctionnalitÃ©s.DÃ©tails sur les versions : Versions LTS : PubliÃ©es tous les deux ans, elles bÃ©nÃ©ficient de trois ans de support.IdÃ©ales pour les applications en production nÃ©cessitant de la stabilitÃ©. Versions STS : PubliÃ©es entre deux versions LTS, elles incluent les derniÃ¨res avancÃ©es en termes de fonctionnalitÃ©s et de performances.Support limitÃ© Ã  18 mois, encourageant les migrations rapides.Ce modÃ¨le assure un rythme dâ€™innovation rapide tout en offrant aux entreprises la flexibilitÃ© de choisir la version qui correspond le mieux Ã  leurs besoins.Devriez-vous toujours cibler la version la plus rÃ©cente du framework ?Non, il nâ€™est pas toujours nÃ©cessaire dâ€™utiliser la version la plus rÃ©cente du framework. Cela dÃ©pend de plusieurs facteurs liÃ©s Ã  la stabilitÃ©, au support, et aux nouvelles fonctionnalitÃ©s.Ã‰lÃ©ments clÃ©s Ã  prendre en compte : StabilitÃ© et support (LTS vs STS) : Pour les applications nÃ©cessitant stabilitÃ© et support prolongÃ©, privilÃ©giez les versions LTS. Si vous souhaitez bÃ©nÃ©ficier des derniÃ¨res avancÃ©es, une version STS peut Ãªtre envisagÃ©e, malgrÃ© son support plus court. CompatibilitÃ© des dÃ©pendances : Assurez-vous que les bibliothÃ¨ques ou packages de votre projet sont compatibles avant toute mise Ã  jour. Nouvelles fonctionnalitÃ©s : Si une version introduit des optimisations ou amÃ©liorations cruciales pour votre projet, cela peut justifier une migration. Effort de migration : Migrer vers une nouvelle version demande souvent des ajustements dans le code et des tests supplÃ©mentaires. Si la stabilitÃ© est prioritaire, une mise Ã  jour peut Ãªtre moins urgente. ConclusionEn conclusion, il est recommandÃ© de choisir la version qui correspond le mieux aux besoins de votre projet, en privilÃ©giant les LTS pour la stabilitÃ©, mais sans exclure les STS si les nouvelles fonctionnalitÃ©s apportent un rÃ©el bÃ©nÃ©fice.Microsoft transforme le mois de novembre en une pÃ©riode stratÃ©gique pour les dÃ©couvertes technologiques, offrant une vision claire des innovations Ã  venir dans .NET, Azure, et bien dâ€™autres outils.Pour en savoir plus, dÃ©couvrez mon article dÃ©diÃ© Ã  .NET Conf 2024 et celui sur Microsoft Ignite 2024." }, { "title": "Retour sur .NET Conf 2024", "url": "/posts/dotnet-conf-2024/", "categories": "", "tags": "dotnet, conference", "date": "2024-12-01 19:00:00 -0500", "snippet": ".NET 9 regorge de nouveautÃ©s pensÃ©es pour amÃ©liorer la performance, la lisibilitÃ© et offrir des outils modernes aux dÃ©veloppeurs. AprÃ¨s une annÃ©e Ã  explorer les prÃ©versions, jâ€™ai pris le temps de s...", "content": ".NET 9 regorge de nouveautÃ©s pensÃ©es pour amÃ©liorer la performance, la lisibilitÃ© et offrir des outils modernes aux dÃ©veloppeurs. AprÃ¨s une annÃ©e Ã  explorer les prÃ©versions, jâ€™ai pris le temps de synthÃ©tiser les points forts qui pourraient transformer votre faÃ§on de coder.Pour une approche visuelle, je recommande la vidÃ©o rÃ©cente de Nick Chapsas, qui propose un excellent aperÃ§u. Et pour des dÃ©tails techniques approfondis, les articles de Stephen Toub sont une ressource incontournable. âš ï¸ Attention : Il est impossible de couvrir en dÃ©tail toutes les nouveautÃ©s de .NET 9 dans un seul article. Cependant, pour une vue dâ€™ensemble complÃ¨te, je vous encourage Ã  consulter les ressources officielles et les analyses approfondies mentionnÃ©es ici.1. LINQ : Toujours plus intuitifNouvelle mÃ©thode Index().NET 9 introduit Index(), qui rend le traitement des indices plus Ã©lÃ©gant et naturel :var resultat = maListe.Index().Select(x =&gt; $\"{x.Value} Ã  l'index {x.Index}\");Dites adieu Ã  lâ€™utilisation dâ€™un index manuel !Autres nouveautÃ©s : CountBy() et AggregateBy() CountBy() : Simplifie le comptage des occurrences par clÃ©. AggregateBy() : Facilite les agrÃ©gations sur des groupes de donnÃ©es.2. UUID v7 : Identifiants uniques temporellement ordonnÃ©sAvec UUID v7, Microsoft rÃ©pond aux besoins des systÃ¨mes modernes nÃ©cessitant un tri temporel efficace.Ces identifiants : Incluent un horodatage en millisecondes. Restent conformes aux standards tout en amÃ©liorant la compatibilitÃ© avec des scÃ©narios actuels. Sâ€™utilisent facilement avec Guid.CreateVersion7().Une solution idÃ©ale pour les journaux, les bases de donnÃ©es ou toute application axÃ©e sur lâ€™ordre chronologique.3. Gestion optimisÃ©e des chaÃ®nes avec Span et SearchValuesSplit sans allocationsDÃ©sormais, vous pouvez dÃ©couper des chaÃ®nes en utilisant Span, rÃ©duisant ainsi les allocations mÃ©moire :var fruits = \"pomme,banane,fraise\";foreach (var fruit in fruits.AsSpan().Split(',')){ Console.WriteLine(fruit.ToString());}Recherches avancÃ©esGrÃ¢ce Ã  SearchValues : ContainsAny : Identifiez rapidement si une valeur fait partie de la collection. IndexOfAny : Localisez efficacement les Ã©lÃ©ments recherchÃ©s.4. Nouvelle gestion de la synchronisation avec System.Threading.Lock.NET 9 introduit un mÃ©canisme de verrouillage dÃ©diÃ©, plus lisible et fiable que les approches traditionnelles :private readonly Lock _lock = new Lock();lock (_lock){ // Section critique} LisibilitÃ© accrue : le code exprime clairement son intention. RÃ©duction des erreurs grÃ¢ce Ã  une meilleure gestion des synchronisations complexes.5. Performances : Des gains concretsMicrosoft continue dâ€™optimiser .NET, et cette version ne fait pas exception : RÃ©duction des allocations mÃ©moire avec des amÃ©liorations sur LINQ et Span. CompatibilitÃ© renforcÃ©e avec AOT (Ahead-of-Time), un atout pour les Minimal APIs et les environnements Ã  ressources limitÃ©es.6. Swagger : OÃ¹ est-il passÃ© ?Une surprise pour plusieurs : Swagger nâ€™est plus activÃ© par dÃ©faut. Si cela vous perturbe, pas de panique ! Patrick God vous guide dans lâ€™ajout de Swagger Ã  vos projets.Pour en savoir plus ğŸ‘‰ Issue GitHubConclusionAvec .NET 9, Microsoft consolide sa vision dâ€™un framework qui allie performance, lisibilitÃ© et innovation. Que vous soyez un adepte des derniÃ¨res technologies ou simplement curieux dâ€™amÃ©liorer vos pratiques, cette version offre de nombreux outils pour faire passer vos projets au niveau supÃ©rieur." }, { "title": "La performance de Mapperly", "url": "/posts/performance-mapperly/", "categories": "outil-developpement", "tags": "dotnet, source-generator, mapperly", "date": "2024-11-25 16:00:00 -0500", "snippet": "La semaine derniÃ¨re, jâ€™ai eu la chance de participer Ã  Microsoft Ignite Ã  Chicago. Cet Ã©vÃ©nement riche en annonces et en Ã©changes mâ€™a inspirÃ© plusieurs sujets que je partagerai sur mon blog dans le...", "content": "La semaine derniÃ¨re, jâ€™ai eu la chance de participer Ã  Microsoft Ignite Ã  Chicago. Cet Ã©vÃ©nement riche en annonces et en Ã©changes mâ€™a inspirÃ© plusieurs sujets que je partagerai sur mon blog dans les jours Ã  venir !Dans un article prÃ©cÃ©dent, jâ€™ai mentionnÃ© les avantages de lâ€™utilisation des Source Generators. Jâ€™avais Ã©galement prÃ©sentÃ© un exemple avec Mapperly, un outil qui utilise les Source Generators pour effectuer le mapping entre deux entitÃ©s, Ã©vitant ainsi la rÃ©flexion.Curieux de comparer les performances entre AutoMapper, un outil frÃ©quemment utilisÃ© dans mes mandats, et Mapperly, jâ€™ai dÃ©cidÃ© de rÃ©aliser quelques benchmarks !Mise en place de lâ€™environnementJâ€™ai commencÃ© par crÃ©er un projet .NET 9 et jâ€™ai ajoutÃ© la structure de base pour utiliser BenchmarkDotNet.Jâ€™ai dÃ©fini une classe Personne et un DTO PersonneDto :public class Personne{ public required string Prenom { get; set; } public required string Nom { get; set; } public required int Age { get; set; } public required string Adresse { get; set; } public required string Ville { get; set; } public required string Province { get; set; }}public class PersonneDto{ public required string Prenom { get; set; } public required string Nom { get; set; } public required string Adresse { get; set; } public required string Ville { get; set; } public required string Province { get; set; }}Pour Mapperly, jâ€™ai crÃ©Ã© le mapper suivant :[Mapper]public partial class PersonneMapper{ public partial PersonneDto PersonneToPersonneDto(Personne personne);}Configuration des benchmarksJâ€™ai configurÃ© mes benchmarks pour itÃ©rer sur 1, 100 et 1 000 fois :[MemoryDiagnoser][Orderer(SummaryOrderPolicy.FastestToSlowest)][RankColumn][HideColumns(Column.Error, Column.Median, Column.RatioSD, Column.StdDev)]public class BenchmarkMapperlyAutomapper{ private IMapper _automapper; private PersonneMapper _mapperly; private Personne _personne; [GlobalSetup] public void GlobalSetup() { _automapper = new MapperConfiguration(cfg =&gt; cfg.CreateMap&lt;Personne, PersonneDto&gt;()).CreateMapper(); _mapperly = new PersonneMapper(); _personne = new Fixture().Create&lt;Personne&gt;(); } [Params(1, 100, 1_000)] public int Iteration { get; set; } [Benchmark(Baseline = true)] public List&lt;PersonneDto&gt; AvecAutomapper() { var personnes = new List&lt;PersonneDto&gt;(); for (var i = 0; i &lt; Iteration; i++) { var personneDto = _automapper.Map&lt;PersonneDto&gt;(_personne); personnes.Add(personneDto); } return personnes; } [Benchmark] public List&lt;PersonneDto&gt; AvecMapperly() { var personnes = new List&lt;PersonneDto&gt;(); for (var i = 0; i &lt; Iteration; i++) { var personneDto = _mapperly.PersonneToPersonneDto(_personne); personnes.Add(personneDto); } return personnes; }}RÃ©sultats des benchmarksVoici les rÃ©sultats obtenus :| Method | Iteration | Mean | Ratio | Rank | Gen0 | Gen1 | Allocated | Alloc Ratio ||--------------- |---------- |-------------:|------:|-----:|-------:|-------:|----------:|------------:|| AvecMapperly | 1 | 25.61 ns | 0.32 | 1 | 0.0172 | - | 144 B | 1.00 || AvecAutomapper | 1 | 80.79 ns | 1.00 | 2 | 0.0172 | - | 144 B | 1.00 || | | | | | | | | || AvecMapperly | 100 | 1,506.71 ns | 0.22 | 1 | 0.9308 | 0.0229 | 7792 B | 1.00 || AvecAutomapper | 100 | 6,938.45 ns | 1.00 | 2 | 0.9308 | 0.0153 | 7792 B | 1.00 || | | | | | | | | || AvecMapperly | 1000 | 14,079.26 ns | 0.20 | 1 | 8.6670 | 1.7242 | 72600 B | 1.00 || AvecAutomapper | 1000 | 70,748.49 ns | 1.01 | 2 | 8.6670 | 1.7090 | 72600 B | 1.00 |ConclusionLes rÃ©sultats montrent clairement que Mapperly est beaucoup plus performant que AutoMapper. Lâ€™utilisation des Source Generators plutÃ´t que de la rÃ©flexion porte ses fruits !La grande question reste : comment lâ€™outil se comporte-t-il avec des structures plus complexes ? Restez Ã  lâ€™affut pour un prochain article !Pour en savoir plus sur Mapperly, consultez la documentation officielle." }, { "title": "L'importance d'un registre de dÃ©cisions", "url": "/posts/registre-decisions/", "categories": "", "tags": "", "date": "2024-11-06 19:00:00 -0500", "snippet": "PrÃ©ambuleJe ne pensais pas publier un article aujourdâ€™hui, mais je me suis laissÃ© inspirer ! La majoritÃ© ne le sait pas, mais jâ€™ai changÃ© de mandat depuis peu. Je me retrouve maintenant dans un min...", "content": "PrÃ©ambuleJe ne pensais pas publier un article aujourdâ€™hui, mais je me suis laissÃ© inspirer ! La majoritÃ© ne le sait pas, mais jâ€™ai changÃ© de mandat depuis peu. Je me retrouve maintenant dans un ministÃ¨re oÃ¹ les ressources et les pratiques ne sont pas toutes en place pour affronter des projets dâ€™envergure informatique. Dans ce genre de contexte, on rÃ©alise rapidement que certaines choses que lâ€™on prend pour acquises sont loin dâ€™Ãªtre la norme ! Ce nâ€™est pas par manque de volontÃ© ; plusieurs personnes dans lâ€™organisation occupent des rÃ´les multiples, remplissant parfois les tÃ¢ches dâ€™une autre ressource en plus de leur propre mission.Cette situation me rappelle une confÃ©rence de Roy Osherove, Team Leadership in the Age of Agile, oÃ¹ il explique quâ€™une Ã©quipe constamment en mode â€œsurvieâ€ peine Ã  surmonter les problÃ¨mes quotidiens. Impossible donc, pour une Ã©quipe dans cette situation, dâ€™amÃ©liorer ses processus par elle-mÃªme ; câ€™est pourquoi il mentionne souvent quâ€™une intervention externe est indispensable pour aider lâ€™Ã©quipe Ã  sortir de cette impasse.Pour donner un peu de contexte, lors de mon dernier mandat, jâ€™ai eu la chance dâ€™Ãªtre impliquÃ© activement dans un groupe dâ€™experts qui avait pour mission de dÃ©finir des orientations, de dÃ©velopper et maintenir des outils communs, et surtout, de prÃ©venir les problÃ¨mes avant mÃªme quâ€™ils nâ€™apparaissent. Nous avons instaurÃ©, au fil du temps, divers processus, documentations et outils pour allÃ©ger les tÃ¢ches des Ã©quipes de dÃ©veloppement.Câ€™est dans cette mÃªme optique que jâ€™ai voulu continuer cette pratique dans mon nouveau mandat en recommandant la mise en place dâ€™un registre de dÃ©cisions !Mais pourquoi, allez-vous me demander ? Câ€™est prÃ©cisÃ©ment ce que je veux explorer dans cet article ğŸ˜‰.Quâ€™est-ce quâ€™un registre de dÃ©cisions ?Un registre de dÃ©cisions est un document centralisÃ© (parfois mÃªme un outil) oÃ¹ sont consignÃ©es toutes les dÃ©cisions importantes prises dans le cadre dâ€™un projet, quâ€™elles soient techniques, stratÃ©giques ou organisationnelles. Chaque dÃ©cision y est dÃ©crite avec son contexte, les options envisagÃ©es, les raisons du choix final, ainsi que la date et les participants Ã  la prise de dÃ©cision.Il permet de garder une trace claire de chaque orientation prise pour diverses raisons : justifications futures, onboarding des nouveaux membres de lâ€™Ã©quipe ou, tout simplement, Ã©viter de revenir sans cesse sur des sujets dÃ©jÃ  tranchÃ©s.Ce registre nâ€™a pas besoin dâ€™Ãªtre complexe, mais il doit Ãªtre Ã  jour et facilement accessible par lâ€™Ã©quipe pour Ãªtre utile. On peut utiliser un simple document partagÃ© ou opter pour des outils de gestion plus avancÃ©s si lâ€™ampleur du projet le justifie. Dans mon prÃ©cÃ©dent mandat, par exemple, notre registre Ã©tait intÃ©grÃ© dans un wiki dans Azure DevOps.Les gainsLa mise en place dâ€™un registre de dÃ©cisions offre plusieurs avantages significatifs : Transparence et clartÃ© : Toutes les dÃ©cisions sont visibles et documentÃ©es, ce qui rÃ©duit les interprÃ©tations ou les malentendus. Gain de temps : Plus besoin de revisiter sans cesse les mÃªmes discussions ou de redÃ©battre des choix dÃ©jÃ  faits. Lâ€™Ã©quipe peut consulter le registre pour comprendre pourquoi une dÃ©cision a Ã©tÃ© prise et se concentrer sur les prochaines Ã©tapes. ResponsabilitÃ© et engagement : En documentant chaque dÃ©cision avec les noms des participants, chacun sâ€™investit davantage dans le processus, sachant que son avis est pris en compte et consignÃ©. TraÃ§abilitÃ© : Dans un contexte de rÃ©vision, de retour en arriÃ¨re ou mÃªme dâ€™audit, le registre sert de rÃ©fÃ©rence pour comprendre la logique derriÃ¨re chaque Ã©tape du projet. Support Ã  la continuitÃ© : En cas de changement de personnel, le registre facilite la transition et lâ€™onboarding des nouveaux arrivants en leur offrant un historique clair. Un exemplePrenons un exemple du monde .NET. Vous avez peut-Ãªtre remarquÃ© que certaines mÃ©thodes portent le suffixe Async.La question sâ€™est posÃ©e : est-ce systÃ©matique de lâ€™ajouter ?Voici une dÃ©cision prise pour clarifier cette pratique :2024-11-06 - Nomenclature des mÃ©thodes asynchrones Contexte : Devons-nous systÃ©matiquement ajouter le suffixe Async aux mÃ©thodes asynchrones ? DÃ©cision : Comme toutes nos mÃ©thodes sont asynchrones, le suffixe Async devient redondant, donc il nâ€™est pas recommandÃ© de lâ€™ajouter systÃ©matiquement. Nous suivons lâ€™orientation prÃ©conisÃ©e par MediatR. Cependant, si une interface doit proposer des versions synchrone et asynchrone dâ€™une mÃªme mÃ©thode, le suffixe Async est recommandÃ© pour identifier clairement la mÃ©thode asynchrone. ConsÃ©quences : Aucun impact notable dans ce cas-ci.ConclusionUn registre de dÃ©cisions est bien plus quâ€™un simple document administratif. Dans les environnements complexes et les projets dâ€™envergure, il devient un outil essentiel de transparence, de cohÃ©rence et dâ€™efficacitÃ© pour toute lâ€™Ã©quipe. Il permet non seulement de gagner du temps, mais Ã©galement de crÃ©er un espace de confiance oÃ¹ chacun peut retrouver les raisons et le contexte des choix passÃ©s, sans devoir constamment revenir en arriÃ¨re.En documentant nos dÃ©cisions, nous crÃ©ons un cadre de rÃ©fÃ©rence clair et solide pour Ã©viter les erreurs de rÃ©pÃ©tition, faciliter lâ€™intÃ©gration des nouveaux membres et renforcer la continuitÃ© dans les projets. En fin de compte, ce petit investissement de temps pour maintenir un registre peut faire toute la diffÃ©rence dans la gestion dâ€™un projet rÃ©ussi.Alors, pourquoi ne pas intÃ©grer un registre de dÃ©cisions dÃ¨s aujourdâ€™hui dans vos pratiques ?" }, { "title": "Introduction Ã  BenchmarkDotNet", "url": "/posts/introduction-benchmarkdotnet/", "categories": "outil-developpement", "tags": "dotnet", "date": "2024-10-31 19:00:00 -0400", "snippet": "BenchmarkDotNet est une bibliothÃ¨que populaire pour effectuer des benchmarks en .NET. Elle permet de mesurer prÃ©cisÃ©ment les performances de vos applications et dâ€™identifier les points Ã  optimiser....", "content": "BenchmarkDotNet est une bibliothÃ¨que populaire pour effectuer des benchmarks en .NET. Elle permet de mesurer prÃ©cisÃ©ment les performances de vos applications et dâ€™identifier les points Ã  optimiser.Voici comment commencer : Installation : Tout dâ€™abord, installez BenchmarkDotNet via NuGet dans votre projet : dotnet add package BenchmarkDotNet CrÃ©ation dâ€™un benchmark : CrÃ©ez une classe pour votre benchmark et ajoutez-y une mÃ©thode marquÃ©e avec lâ€™attribut [Benchmark]. Configuration : Vous pouvez configurer votre benchmark en utilisant des attributs comme [Params] pour spÃ©cifier diffÃ©rents paramÃ¨tres de test. Exemple dâ€™utilisationSupposons que vous vouliez mesurer les performances de deux mÃ©thodes diffÃ©rentes pour obtenir la valeur texte dâ€™un enum.Voici comment vous pourriez procÃ©der :using BenchmarkDotNet.Attributes;using BenchmarkDotNet.Order;namespace TestBenchmarkDotnet.TestEnumToString;[MemoryDiagnoser][Orderer(SummaryOrderPolicy.FastestToSlowest)][RankColumn]public class BenchmarkEnumToString{ [Params(1, 100, 1_000)] public int Iteration { get; set; } [Benchmark(Baseline = true)] public List&lt;string&gt; EnumToString() { var valeursEnum = new List&lt;string&gt;(); for (var i = 0; i &lt; Iteration; i++) { valeursEnum.Add(Joueur.ValeurEnum1.ToString()); } return valeursEnum; } [Benchmark] public List&lt;string&gt; EnumNameof() { var valeursEnum = new List&lt;string&gt;(); for (var i = 0; i &lt; Iteration; i++) { valeursEnum.Add(nameof(Joueur.ValeurEnum1)); } return valeursEnum; } private enum Joueur { ValeurEnum1, ValeurEnum2 }}// ------------- au niveau du program.csusing BenchmarkDotNet.Running;using TestBenchmarkDotnet.TestEnumToString;BenchmarkRunner.Run&lt;BenchmarkEnumToString&gt;();DÃ©marrer lâ€™exÃ©cution dâ€™un benchmarkPour exÃ©cuter un benchmark, il est essentiel dâ€™exÃ©cuter vos benchmarks en mode Release.Voici comment procÃ©der : ExÃ©cution des benchmarks : Vous pouvez exÃ©cuter vos benchmarks en ligne de commande. dotnet run -c Release ğŸ’¡ Assurez-vous dâ€™exÃ©cuter la commande oÃ¹ se trouve votre fichier .csproj. Analyse des rÃ©sultats : AprÃ¨s lâ€™exÃ©cution, BenchmarkDotNet gÃ©nÃ©rera un rapport dÃ©taillÃ© dans la console avec des mÃ©triques telles que le temps dâ€™exÃ©cution moyen, lâ€™Ã©cart-type et lâ€™allocation mÃ©moire. Voici ce que ces mÃ©triques signifient : Temps dâ€™exÃ©cution moyen (Mean) : Câ€™est la moyenne des temps dâ€™exÃ©cution mesurÃ©s pour chaque itÃ©ration du benchmark. MesurÃ© en nanosecondes (ns), microsecondes (us), ou millisecondes (ms). Ã‰cart-type (StdDev) : Il mesure la variation ou la dispersion des temps dâ€™exÃ©cution mesurÃ©s autour de la moyenne. Une valeur faible indique une grande stabilitÃ© dans les mesures. Allocation mÃ©moire (Allocated) : Câ€™est la quantitÃ© totale de mÃ©moire allouÃ©e pendant lâ€™exÃ©cution du benchmark. MesurÃ©e en octets (B), kilooctets (KB), ou mÃ©gaoctets (MB), selon lâ€™ampleur de lâ€™allocation. RÃ©sultat de lâ€™exÃ©cution| Method | Iteration | Mean | Error | StdDev | Median | Ratio | RatioSD | Rank | Gen0 | Gen1 | Allocated | Alloc Ratio ||------------- |---------- |------------:|-----------:|-----------:|------------:|------:|--------:|-----:|-------:|-------:|----------:|------------:|| EnumNameof | 1 | 13.71 ns | 0.117 ns | 0.109 ns | 13.69 ns | 0.59 | 0.02 | 1 | 0.0105 | - | 88 B | 0.79 || EnumToString | 1 | 22.44 ns | 0.496 ns | 1.002 ns | 22.32 ns | 1.00 | 0.00 | 2 | 0.0134 | - | 112 B | 1.00 || | | | | | | | | | | | | || EnumNameof | 100 | 375.49 ns | 7.477 ns | 18.621 ns | 367.17 ns | 0.35 | 0.02 | 1 | 0.2618 | 0.0010 | 2192 B | 0.48 || EnumToString | 100 | 1,065.24 ns | 21.091 ns | 44.488 ns | 1,064.77 ns | 1.00 | 0.00 | 2 | 0.5474 | 0.0019 | 4592 B | 1.00 || | | | | | | | | | | | | || EnumNameof | 1000 | 3,190.26 ns | 110.750 ns | 319.539 ns | 3,023.35 ns | 0.35 | 0.03 | 1 | 1.9836 | 0.0572 | 16600 B | 0.41 || EnumToString | 1000 | 9,346.52 ns | 186.882 ns | 360.058 ns | 9,284.09 ns | 1.00 | 0.00 | 2 | 4.8523 | 0.1373 | 40602 B | 1.00 |On constate quâ€™il est plus rapide dâ€™utiliser nameof et Ã©galement moins coÃ»teux en termes dâ€™allocation mÃ©moire.Points Ã  considÃ©rer dans les benchmarksLorsque vous analysez les rÃ©sultats de vos benchmarks avec BenchmarkDotNet, voici quelques points clÃ©s Ã  garder Ã  lâ€™esprit : MÃ©triques : Les mÃ©triques comme le temps dâ€™exÃ©cution moyen, lâ€™Ã©cart-type et lâ€™allocation mÃ©moire sont cruciales pour Ã©valuer les performances et lâ€™efficacitÃ© de votre code. Comparaison : Utilisez les rÃ©sultats pour comparer diffÃ©rentes implÃ©mentations ou configurations de votre code.ConclusionBenchmarkDotNet est un outil prÃ©cieux pour Ã©valuer les performances de vos applications .NET de maniÃ¨re prÃ©cise et reproductible. En suivant ces Ã©tapes, vous pouvez commencer Ã  utiliser efficacement BenchmarkDotNet pour optimiser vos applications." }, { "title": "Introduction aux Minimal APIs", "url": "/posts/introduction-minimal-apis/", "categories": "outil-developpement", "tags": "dotnet, aspnet-core", "date": "2024-10-10 20:00:00 -0400", "snippet": "Lâ€™utilisation des Minimal APIs est une approche simplifiÃ©e pour crÃ©er des API HTTP rapides avec ASP.NET Core. Elles permettent de crÃ©er des points de terminaison REST entiÃ¨rement fonctionnels avec ...", "content": "Lâ€™utilisation des Minimal APIs est une approche simplifiÃ©e pour crÃ©er des API HTTP rapides avec ASP.NET Core. Elles permettent de crÃ©er des points de terminaison REST entiÃ¨rement fonctionnels avec un minimum de code et de configuration. Vous pouvez ignorer la gÃ©nÃ©ration automatique classique et Ã©viter les contrÃ´leurs en dÃ©clarant directement des routes et des actions dâ€™API.Par exemple, le code suivant crÃ©e une API Ã  la racine de lâ€™application web qui retourne simplement le texte â€œHello World!â€ :var app = WebApplication.Create(args)app.MapGet(\"/\", () =&gt; \"Hello World!\");app.Run();Cela suffit pour dÃ©marrer, mais il y a bien plus Ã  dÃ©couvrir. Les API minimales offrent Ã©galement la configuration et la personnalisation nÃ©cessaires pour Ã©voluer vers plusieurs API, gÃ©rer des routes complexes, appliquer des rÃ¨gles dâ€™autorisation, et contrÃ´ler le contenu des rÃ©ponses.Comment crÃ©er un premier minimal ApiCe tutoriel dÃ©crit les principes fondamentaux liÃ©s Ã  la crÃ©ation dâ€™un Minimal API avec ASP.NET Core.ğŸ‘¨â€ğŸ’» Le code du tutoriel est disponible ici. Si vous ne le saviez pas, il est possible dâ€™ouvrir un rÃ©fÃ©rentiel GitHub directement dans une version web de Visual Studio Code en ajoutant â€œ1sâ€ aprÃ¨s â€œgithubâ€ dans lâ€™URL.Comment structurer ces endpointsQue remarquez-vous Ã  la suite de la lecture du tutoriel ? Les Minimal APIs offrent une grande flexibilitÃ© sur la maniÃ¨re de les structurer. On peut dÃ©finir des endpoints directement dans le fichier Program.cs, contrairement Ã  lâ€™utilisation classique des contrÃ´leurs. Câ€™est trÃ¨s pratique pour rÃ©aliser des prototypes ou mÃªme des projets de faible envergure.Mais que faire si notre projet devient plus complexe ? Comment gÃ©rer les essais unitaires et structurer le code pour faciliter sa lisibilitÃ© et son entretien ?Voici quelques recommandations importantes Ã  garder en tÃªte : Pour Ãªtre testable unitairement, une mÃ©thode doit Ãªtre exposÃ©e et non simplement dÃ©finie en tant quâ€™endpoint (exemple). Il est possible de dÃ©finir des mÃ©thodes statiques dans le Program.cs, mais il est recommandÃ© de dÃ©placer les points de terminaison Ã  lâ€™extÃ©rieur pour une meilleure organisation. Utilisez le type TypedResults au lieu du type gÃ©nÃ©rique Results pour un meilleur contrÃ´le des rÃ©ponses API.Le code associÃ© au tutoriel illustre ces pratiques Ã  travers la structure de la classe TodoEndpointsV2.Dâ€™autres lectures pour aller plus loinSi vous souhaitez approfondir vos connaissances sur les Minimal APIs, voici quelques ressources que je vous recommande fortement : Gestionnaires de routage Liaison de paramÃ¨tres CrÃ©ation de rÃ©ponses Filtres Tests unitaires et dâ€™intÃ©grationQuand utiliser les Minimal APIs ?Les Minimal APIs brillent dans les cas oÃ¹ la simplicitÃ©, la rapiditÃ©, et la lÃ©gÃ¨retÃ© sont des prioritÃ©s. Elles sont particuliÃ¨rement adaptÃ©es pour : Les microservices : Leur faible complexitÃ© en fait un excellent choix pour les services isolÃ©s, oÃ¹ les fonctionnalitÃ©s sont limitÃ©es et oÃ¹ les performances sont cruciales. Les prototypes ou petites applications : Si vous avez besoin de mettre en place rapidement un service sans trop de structure, les Minimal APIs offrent une solution agile. Les fonctions serverless : Leur nature lÃ©gÃ¨re permet de rÃ©duire les temps de dÃ©marrage et dâ€™amÃ©liorer les performances, particuliÃ¨rement pour des environnements comme Azure Functions ou AWS Lambda. CompatibilitÃ© avec lâ€™AOT (Ahead-of-Time) compilation : Contrairement aux contrÃ´leurs classiques, les Minimal APIs sont compatibles avec la compilation AOT, ce qui est un atout important si vous avez des exigences de performance et de dÃ©ploiement rapide. Les Web APIs classiques, reposant sur la rÃ©flexion, ne peuvent pas bÃ©nÃ©ficier de cette optimisation.Les Minimal APIs et les Web APIs classiques ont toutes deux leur utilitÃ©, en fonction des besoins du projet. Pour des applications simples ou des microservices, les Minimal APIs offrent rapiditÃ©, compatibilitÃ© AOT, et efficacitÃ©. Pour des applications plus complexes nÃ©cessitant une structure bien dÃ©finie, les Web APIs classiques restent le choix idÃ©al.ConclusionLâ€™utilisation des API minimales dans ASP.NET Core offre une maniÃ¨re moderne et efficace de crÃ©er des services web lÃ©gers et performants. GrÃ¢ce Ã  leur simplicitÃ©, elles permettent aux dÃ©veloppeurs de se concentrer sur lâ€™essentiel, en rÃ©duisant la complexitÃ© du code et en accÃ©lÃ©rant le dÃ©veloppement. Bien quâ€™elles soient idÃ©ales pour des scÃ©narios simples et des microservices, elles offrent Ã©galement la flexibilitÃ© nÃ©cessaire pour Ã©voluer vers des architectures plus complexes si besoin. En adoptant cette approche, vous bÃ©nÃ©ficiez dâ€™une solution Ã©lÃ©gante, facile Ã  maintenir, tout en profitant des puissantes fonctionnalitÃ©s dâ€™ASP.NET Core.ğŸ’¡ Pour approfondir le dÃ©bat entre Minimal APIs et Web APIs classiques, je vous recommande vivement de visionner la vidÃ©o de Nick Chapsas, oÃ¹ il partage son avis Ã©clairÃ© sur le sujet.RÃ©fÃ©rences Vue dâ€™ensemble des API minimales Tutoriel : CrÃ©er une API minimale avec ASP.NET Core Gestionnaires de routage dans les applications API minimales Liaison de paramÃ¨tres dans les applications API minimales CrÃ©er des rÃ©ponses dans les applications API minimales Filtres dans les applications dâ€™API minimales Tester les applications API minimales" }, { "title": "Les GÃ©nÃ©rateurs de source", "url": "/posts/source-generator-dotnet/", "categories": "", "tags": "dotnet, source-generator", "date": "2024-09-29 18:00:00 -0400", "snippet": "Les Source Generators (GÃ©nÃ©rateurs de source) sont des outils intÃ©grÃ©s au compilateur qui gÃ©nÃ¨rent automatiquement du code source lors de la compilation, permettant aux dÃ©veloppeurs dâ€™automatiser l...", "content": "Les Source Generators (GÃ©nÃ©rateurs de source) sont des outils intÃ©grÃ©s au compilateur qui gÃ©nÃ¨rent automatiquement du code source lors de la compilation, permettant aux dÃ©veloppeurs dâ€™automatiser la crÃ©ation de portions de code rÃ©pÃ©titives ou complexes tout en gardant le code gÃ©nÃ©rÃ© visible et modifiable. Introduits avec .NET 5, ils sont rapidement devenus un Ã©lÃ©ment clÃ© de lâ€™Ã©cosystÃ¨me .NET. Contrairement Ã  des techniques comme la rÃ©flexion, qui gÃ©nÃ¨rent du code dynamiquement Ã  lâ€™exÃ©cution, les Source Generators produisent du code statique, optimisÃ© et vÃ©rifiÃ© lors de la compilation.La rÃ©volution sur le quotidienLâ€™arrivÃ©e des Source Generators a considÃ©rablement transformÃ© la maniÃ¨re de travailler au quotidien, surtout pour les dÃ©veloppeurs confrontÃ©s Ã  du boilerplate code ou des scÃ©narios nÃ©cessitant une gÃ©nÃ©ration de code personnalisÃ©e.Voici comment ils rÃ©volutionnent le dÃ©veloppement : RÃ©duction du code rÃ©current : Les gÃ©nÃ©rateurs de code prennent en charge la gÃ©nÃ©ration de classes de mapping, DTOs, modÃ¨les de validation, ou tout autre code rÃ©pÃ©titif, permettant ainsi de se concentrer sur des tÃ¢ches Ã  plus forte valeur ajoutÃ©e. Meilleures performances : Le code Ã©tant gÃ©nÃ©rÃ© Ã  la compilation, il est plus performant que les approches reposant sur des mÃ©canismes comme la rÃ©flexion, qui sont dynamiques et coÃ»teux en termes de performance. Centralisation de la logique : Les rÃ¨gles de gÃ©nÃ©ration peuvent Ãªtre dÃ©finies une fois et appliquÃ©es de maniÃ¨re uniforme dans tout le projet, amÃ©liorant la maintenabilitÃ© et rÃ©duisant les erreurs. CompatibilitÃ© AOT : Lâ€™un des avantages cruciaux des Source Generators est quâ€™ils ne reposent pas sur la rÃ©flexion, une technique incompatible avec le mode de compilation Ahead-Of-Time (AOT). AOT est utilisÃ© pour optimiser les applications .NET destinÃ©es Ã  des environnements contraints en ressources, comme les applications mobiles et les conteneurs. En remplaÃ§ant la rÃ©flexion par du code gÃ©nÃ©rÃ© statiquement, les Source Generators rendent le code compatible avec AOT et permettent ainsi une exÃ©cution plus rapide et plus lÃ©gÃ¨re.Exemple avec MapperlyMapperly est un exemple parfait dâ€™outil tirant parti des Source Generators pour automatiser les tÃ¢ches de mapping entre objets, tout en Ã©liminant la dÃ©pendance Ã  la rÃ©flexion.Imaginons que vous ayez une classe Personne et un DTO PersonneDto, et que vous souhaitiez les mapper :public class Personne{ public string Prenom { get; set; } public string Nom { get; set; } public DateTime DateNaissance { get; set; }}public class PersonneDto{ public string Prenom { get; set; } public string Nom { get; set; }}Avec Mapperly, il vous suffit de crÃ©er un mapper sous la forme dâ€™une classe partielle et dâ€™appliquer lâ€™attribut [Mapper].[Mapper]public partial class PersonneMapper{ public partial PersonneDto MapToDto(Personne personne);}Ã€ la compilation, Mapperly gÃ©nÃ¨re automatiquement le code nÃ©cessaire pour mapper les propriÃ©tÃ©s correspondantesÂ :public partial class PersonneMapper{ public partial PersonneDto MapToDto(Personne personne) { return new PersonneDto { Prenom = personne.Prenom, Nom = personne.Nom }; }}Ici, Mapperly remplace la rÃ©flexion, souvent utilisÃ©e dans des solutions comme AutoMapper, par du code statique et optimisÃ©. Ce qui, en plus de rendre le mapping plus rapide, le rend compatible avec le mode AOT, oÃ¹ la rÃ©flexion nâ€™est pas une option possible.Outils Ã  surveiller pour exploiter pleinement les Source GeneratorsLâ€™Ã©cosystÃ¨me .NET regorge dâ€™outils qui exploitent les Source Generators pour simplifier et automatiser certaines tÃ¢ches courantes du dÃ©veloppement.Voici une sÃ©lection dâ€™outils Ã  surveiller qui pourraient transformer vos pratiques de dÃ©veloppement : Mapperly - Mapperly, comme mentionnÃ© prÃ©cÃ©demment, est un outil qui gÃ©nÃ¨re du code de mapping entre objets de maniÃ¨re statique. Contrairement Ã  AutoMapper, qui utilise la rÃ©flexion, Mapperly repose sur des Source Generators, ce qui le rend beaucoup plus performant et compatible avec des environnements AOT. TUnit - TUnit est une bibliothÃ¨que de tests unitaires qui utilise les Source Generators pour gÃ©nÃ©rer des mÃ©thodes de test Ã  la volÃ©e. Cela permet de rÃ©duire considÃ©rablement le code Ã  Ã©crire pour chaque test, en particulier pour les scÃ©narios rÃ©pÃ©titifs oÃ¹ seule la donnÃ©e testÃ©e varie. CoreWCF - CoreWCF est un port de Windows Communication Foundation (WCF) pour .NET Core. Il utilise les Source Generators pour gÃ©nÃ©rer automatiquement des contrats de service et des proxy, facilitant la migration des applications WCF existantes vers des technologies modernes comme .NET 6+. Enum.Source.Generator - Cet outil permet de gÃ©nÃ©rer du code Ã  partir des Ã©numÃ©rations dÃ©finies dans vos projets. PlutÃ´t que dâ€™Ã©crire manuellement des mÃ©thodes pour obtenir des valeurs ou des descriptions associÃ©es Ã  chaque enum, Enum.Source.Generator gÃ©nÃ¨re automatiquement ce code pour vous, rÃ©duisant ainsi les risques dâ€™erreurs et la charge de maintenance. Dunet - Dunet est une bibliothÃ¨que pour faciliter la crÃ©ation de types discriminÃ©s (discriminated unions), une fonctionnalitÃ© trÃ¨s puissante que lâ€™on retrouve dans des langages comme F#. Dunet gÃ©nÃ¨re automatiquement le code nÃ©cessaire pour gÃ©rer ces types discriminÃ©s en .NET, simplifiant grandement la gestion des cas particuliers dans les applications.ConclusionLes Source Generators reprÃ©sentent une avancÃ©e majeure, permettant aux dÃ©veloppeurs dâ€™Ã©crire moins de code rÃ©pÃ©titif et dâ€™amÃ©liorer les performances globales de leurs applications. En Ã©liminant la nÃ©cessitÃ© de la rÃ©flexion dans certains scÃ©narios critiques, comme le mapping dâ€™objets ou la gÃ©nÃ©ration de proxy, ils permettent Ã©galement une meilleure compatibilitÃ© avec les environnements AOT. Des outils comme Mapperly, TUnit, CoreWCF, Enum.Source.Generator et Dunet montrent bien le potentiel transformateur de ces outils.Restez attentif Ã  ces outils qui permettent de tirer pleinement parti des capacitÃ©s de .NET !" }, { "title": "Les avantages d'utiliser un fichier .editorconfig en .NET", "url": "/posts/avantages-editorconfig/", "categories": "outil-developpement", "tags": "dotnet", "date": "2024-08-19 19:58:00 -0400", "snippet": "IntroductionSi vous travaillez sur des projets en .NET (ou mÃªme dans dâ€™autres langages), vous avez peut-Ãªtre dÃ©jÃ  entendu parler du fichier .editorconfig. Câ€™est un fichier qui peut rendre la vie de...", "content": "IntroductionSi vous travaillez sur des projets en .NET (ou mÃªme dans dâ€™autres langages), vous avez peut-Ãªtre dÃ©jÃ  entendu parler du fichier .editorconfig. Câ€™est un fichier qui peut rendre la vie de votre Ã©quipe de dÃ©veloppement beaucoup plus facile. Il permet de dÃ©finir et de maintenir des rÃ¨gles de codage cohÃ©rentes, peu importe lâ€™Ã©diteur ou lâ€™IDE utilisÃ©.Voici pourquoi utiliser un fichier .editorconfig est une bonne idÃ©e, avec quelques exemples pratiques spÃ©cifiques Ã  .NET.Pourquoi utiliser un fichier .editorconfig? Code uniforme Constance : Un fichier .editorconfig permet de dÃ©finir des rÃ¨gles de formatage et de style que tout le monde doit suivre. Fini les disputes sur les tabulations versus les espaces! ConformitÃ©: Tout le monde respecte les mÃªmes rÃ¨gles, ce qui rÃ©duit les erreurs et rend le code plus propre. Travail dâ€™Ã©quipe simplifiÃ© Harmonisation : Que votre Ã©quipe soit Ã  QuÃ©bec, MontrÃ©al ou New York, tout le monde suit les mÃªmes conventions. Le code reste uniforme peu importe qui lâ€™Ã©crit. Moins de conflits : Avec un formatage standardisÃ©, il y a moins de conflits quand vous fusionnez des branches dans Git. Compatible avec vos outils Fonctionne partout : La plupart des Ã©diteurs de code et IDE comme Visual Studio et Visual Studio Code supportent les .editorconfig. Pas besoin de plugins supplÃ©mentaires, mais une extension peut Ãªtre nÃ©cessaire. Adaptable : Vous pouvez ajuster les rÃ¨gles de formatage selon les besoins spÃ©cifiques de votre projet ou organisation. Impliquer lâ€™Ã©quipe dans la dÃ©finition des rÃ¨glesIl est crucial que toute lâ€™Ã©quipe soit dâ€™accord sur les rÃ¨gles de codage dÃ©finies dans le fichier .editorconfig.Voici un processus simple pour y arriver : RÃ©union dâ€™Ã©quipe : Organisez une rÃ©union pour discuter des rÃ¨gles de codage Ã  adopter. Assurez-vous que tout le monde puisse exprimer ses prÃ©fÃ©rences et ses besoins. Brainstorming : Encouragez les membres de lâ€™Ã©quipe Ã  proposer des rÃ¨gles et des conventions quâ€™ils aimeraient suivre. Notez toutes les suggestions. Consensus : Discutez des diffÃ©rentes propositions et essayez de trouver un consensus. Lâ€™objectif est de choisir des rÃ¨gles que tout le monde pourra suivre facilement. Documenter les rÃ¨gles : Une fois que les rÃ¨gles sont dÃ©cidÃ©es, documentez-les dans le fichier .editorconfig et partagez-le avec toute lâ€™Ã©quipe. Revue pÃ©riodique : Planifiez des revues pÃ©riodiques des rÃ¨gles pour sâ€™assurer quâ€™elles restent pertinentes et adaptÃ©es aux besoins de lâ€™Ã©quipe.Installer lâ€™extension requise pour Visual Studio CodePour vous assurer que Visual Studio Code utilise correctement le fichier .editorconfig, vous devez installer lâ€™extension EditorConfig for VS Code. Cette extension garantit que les paramÃ¨tres de votre fichier .editorconfig sont respectÃ©s.Pour installer lâ€™extension : Ouvrez Visual Studio Code. Allez dans lâ€™onglet â€œExtensionsâ€ ou appuyez sur Ctrl+Shift+X. Recherchez â€œEditorConfigâ€. Cliquez sur â€œInstallerâ€ pour lâ€™extension â€œEditorConfig for VS Codeâ€.Exemples de configurationVoici quelques exemples de configurations pratiques pour un projet .NET. Indenter avec des espaces # Utiliser des espaces pour l'indentation, avec une largeur de 4 espaces[*.cs]indent_style = spaceindent_size = 4 Fin de ligne uniforme # Utiliser LF (Line Feed) pour les fins de ligne[*.cs]end_of_line = lf Encodage des fichiers # Utiliser UTF-8 pour tous les fichiers[*.cs]charset = utf-8 Espaces autour des opÃ©rateurs # Ajouter des espaces autour des opÃ©rateurs[*.cs]dotnet_style_operator_placement_when_wrapping = before RÃ¨gles spÃ©cifiques pour les fichiers de YAML # Utiliser des espaces pour l'indentation avec une largeur de 2 espaces pour les fichiers YAML[*.yaml]indent_style = spaceindent_size = 2 Avec cette configuration dans votre fichier .editorconfig, tous les fichiers YAML dans votre projet respecteront la convention de deux espaces par indentation. Cela permet dâ€™assurer une cohÃ©rence dans le formatage du code YAML, quel que soit lâ€™Ã©diteur utilisÃ© par votre Ã©quipe.Ressources additionnellesPour des informations plus dÃ©taillÃ©es sur les analyseurs et les bonnes pratiques en .NET, vous pouvez consulter les articles de Dave McCarter (dotnetdave) sur son blog DotNet Tips and Tricks. Dave publie rÃ©guliÃ¨rement des articles pertinents sur lâ€™utilisation des analyseurs et dâ€™autres outils pour amÃ©liorer la qualitÃ© du code et la productivitÃ© des dÃ©veloppeurs.ConclusionUtiliser un fichier .editorconfig dans vos projets .NET a plein dâ€™avantages. Cela aide Ã  garder un code propre et cohÃ©rent, facilite le travail en Ã©quipe, et Ã©vite les conflits de formatage. En plus, câ€™est super facile Ã  mettre en place et Ã§a marche avec la plupart des Ã©diteurs et IDE. Bref, si vous voulez rendre la vie de votre Ã©quipe de dÃ©veloppeurs plus simple et le code plus propre, essayez le fichier .editorconfig. Nâ€™oubliez pas de le mettre Ã  la racine de votre rÃ©fÃ©rentiel, dâ€™impliquer toute lâ€™Ã©quipe dans la dÃ©finition des rÃ¨gles et dâ€™installer lâ€™extension â€œEditorConfig for VS Codeâ€.Vous allez me remercier!" }, { "title": "DÃ©couvrez la nouvelle version de MSTest", "url": "/posts/mise-a-jour-mstest/", "categories": "outil-developpement", "tags": "dotnet, essais", "date": "2024-06-26 19:00:00 -0400", "snippet": "Microsoft vient tout juste de sortir une nouvelle version de MSTest!Quoi de neuf ? MSTest.Analyzers : Plein de corrections de bugs et dâ€™amÃ©liorations pour rendre vos analyses de code encore plus e...", "content": "Microsoft vient tout juste de sortir une nouvelle version de MSTest!Quoi de neuf ? MSTest.Analyzers : Plein de corrections de bugs et dâ€™amÃ©liorations pour rendre vos analyses de code encore plus efficaces. MSTest.Sdk : De nouvelles fonctionnalitÃ©s et des amÃ©liorations pour une meilleure performance et intÃ©gration des tests. MSTest.Runner : Vous pouvez maintenant tester vos applications WinUI avec MSTest. GÃ©nial, non ?Pour plus de dÃ©tails, consultez lâ€™article officiel ici.Les rÃ¨gles Ã  ajouterSi vous voulez tirer le meilleur parti des nouveaux analyseurs, pensez Ã  ajouter ces rÃ¨gles Ã  votre fichier editorconfig :RÃ¨gles de structures : MSTEST0004 - PublicTypeShouldBeTestClassAnalyzer MSTEST0006 - AvoidExpectedExceptionAttributeAnalyzer MSTEST0015 - TestMethodShouldNotBeIgnored MSTEST0016 - TestClassShouldHaveTestMethod MSTEST0019 - PreferTestInitializeOverConstructorAnalyzer MSTEST0021 - PreferDisposeOverTestCleanupAnalyzer MSTEST0025 - PreferAssertFailOverAlwaysFalseConditionsAnalyzerRÃ¨gles dâ€™usages : MSTEST0002 - TestClassShouldBeValidAnalyzer MSTEST0003 - TestMethodShouldBeValidAnalyzer MSTEST0005 - TestContextShouldBeValidAnalyzer MSTEST0007 - UseAttributeOnTestMethodAnalyzer MSTEST0008 - TestInitializeShouldBeValidAnalyzer MSTEST0009 - TestCleanupShouldBeValidAnalyzer MSTEST0010 - ClassInitializeShouldBeValidAnalyzer MSTEST0011 - ClassCleanupShouldBeValidAnalyzer MSTEST0012 - AssemblyInitializeShouldBeValidAnalyzer MSTEST0013 - AssemblyCleanupShouldBeValidAnalyzer MSTEST0014 - DataRowShouldBeValidAnalyzer MSTEST0017 - AssertionArgsShouldBePassedInCorrectOrder MSTEST0023 - DoNotNegateBooleanAssertionAnalyzer MSTEST0024 - DoNotStoreStaticTestContextAnalyzer MSTEST0026 - AssertionArgsShouldAvoidConditionalAccessRuleIdConfiguration du timeout pour le cleanup des classesIl est Ã©galement possible de configurer un timeout pour le cleanup des classes. Pour cela, il suffit dâ€™ajouter la configuration suivante dans votre fichier .runsettings :&lt;RunSettings&gt; &lt;MSTest&gt; &lt;ClassCleanupTimeout&gt;1000&lt;/ClassCleanupTimeout&gt; &lt;/MSTest&gt;&lt;/RunSettings&gt;Pour en savoir plus sur ces rÃ¨gles, jetez un Å“il Ã  la documentation officielle ici." }, { "title": "Annonce de la mise en ligne", "url": "/posts/annonce-mise-en-ligne/", "categories": "", "tags": "", "date": "2024-05-12 21:00:00 -0400", "snippet": "Oyez, Oyez! Je voulais vous informer que je suis actuellement trÃ¨s engagÃ© dans lâ€™achÃ¨vement de la structure de mon site web.Il me reste encore quelques ajustements Ã  faire pour bien maÃ®triser la pl...", "content": "Oyez, Oyez! Je voulais vous informer que je suis actuellement trÃ¨s engagÃ© dans lâ€™achÃ¨vement de la structure de mon site web.Il me reste encore quelques ajustements Ã  faire pour bien maÃ®triser la plateforme que jâ€™utilise, mais cela avance plus rapidement que prÃ©vu.En attendant sa mise en ligne, je vous encourage vivement Ã  me faire parvenir vos suggestions!Restez informÃ©s en vous abonnant au flux RSS!" } ]
